{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started 1. Installation You only need two things: Python 3 and your favourite Python IDE to get started. Then simply install via pip. 1 pip install photonai 2. Setup New Analysis Start by importing some utilities and creating a new Hyperpipe instance, naming the analysis and specifying where to save all outputs. 1 2 3 4 5 6 from sklearn.model_selection import ShuffleSplit , KFold from sklearn.datasets import load_breast_cancer from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import IntegerRange , FloatRange pipe = Hyperpipe ( 'basic_pipe' , project_folder = './' ) 3. Define training, optimization and testing parameters Select parameters to customize the training, hyperparameter optimization and testing procedure. Particularly, you can choose the hyperparameter optimization strategy, set parameters, choose performance metrics and choose the performance metric to minimize or maximize, respectively. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pipe = Hyperpipe ( 'basic_pipe' , project_folder = './' , # choose hyperparameter optimization strategy optimizer = 'random_grid_search' , # PHOTONAI automatically calculates your preferred metrics metrics = [ 'accuracy' , 'balanced_accuracy' , 'f1_score' ], # this metrics selects the best hyperparameter configuration # in this case mean squared error is minimized best_config_metric = 'f1_score' , # select cross validation strategies outer_cv = ShuffleSplit ( n_splits = 3 , test_size = 0.2 ), inner_cv = KFold ( n_splits = 10 )) 4. Build custom pipeline Select and arrange normalization, dimensionality reduction, feature selection, data augmentation, over- or undersampling algorithms in simple or parallel data streams. You can integrate custom algorithms or choose from our wide range of pre-registered algorithms from established toolboxes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 pipe += PipelineElement ( 'StandardScaler' ) pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : FloatRange ( 0.5 , 0.8 , step = 0.1 )}) pipe += PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : [ 'RandomUnderSampler' , 'RandomOverSampler' , 'SMOTE' ]}) or_element = Switch ( 'EstimatorSwitch' ) or_element += PipelineElement ( 'RandomForestClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 30 )}) or_element += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 10 ), 'kernel' : [ 'linear' , 'rbf' ]}) pipe += or_element 5. Load Data and Train Load your data and start the (nested-) cross-validated hyperparameter optimization, training and evaluation procedure. You will see an extensive output to monitor the hyperparameter optimization progress, see the results and track the best performances so far. 1 2 X , y = load_breast_cancer ( return_X_y = True ) pipe . fit ( X , y )","title":"Basic Usage"},{"location":"algorithms/algorithms_index/","text":"Algorithms PHOTONAI offers easy access to established machine learning algorithms. The algorithms can be imported by adding a PipelineElement with a specific name, such as \"SVC\" for importing the SupportVectorClassifier from scikit-learn , as shown in the following examples. You can set all parameters of the imported class as usual: e.g. add gamma='auto' to the PipelineElement to set the support vector machine's gamma parameter to 'auto'. In addition, you can specify each parameter as a hyperparameter and define a value range or value list to find the optimal value, such as 'kernel': ['linear', 'rbf'] . To build a custom pipeline, have a look at PHOTONAIs pre-registered processing- and learning algorithms . You can access algorithms for all purposes from several open-source packages. In addition, PHOTONAI offers several utility classes as well, such as linear statistical feature selection or sample pairing algorithms. In addition you can specify hyperparameters as well as their value range in order to be optimized by the hyperparameter optimization strategy. Currently, PHOTONAI offers Grid-Search , Random Search and two frameworks for bayesian optimization. PCA 1 2 3 4 5 6 from photonai.base import PipelineElement PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) # to test if disabling the PipelineElement improves performance, # simply add the test_disabled=True parameter SVC 1 2 3 4 PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'poly' ]), 'C' : FloatRange ( 0.5 , 2 )}, gamma = 'auto' ) Keras Neural Net 1 2 3 4 5 6 7 8 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 , 3 ]]), 'dropout_rate' : Categorical ([[ 0.5 , 0.2 , 0.1 ], 0.1 ])}, activations = 'relu' , epochs = 5 , batch_size = 32 )","title":"Access pre-registered algorithms"},{"location":"algorithms/algorithms_index/#algorithms","text":"PHOTONAI offers easy access to established machine learning algorithms. The algorithms can be imported by adding a PipelineElement with a specific name, such as \"SVC\" for importing the SupportVectorClassifier from scikit-learn , as shown in the following examples. You can set all parameters of the imported class as usual: e.g. add gamma='auto' to the PipelineElement to set the support vector machine's gamma parameter to 'auto'. In addition, you can specify each parameter as a hyperparameter and define a value range or value list to find the optimal value, such as 'kernel': ['linear', 'rbf'] . To build a custom pipeline, have a look at PHOTONAIs pre-registered processing- and learning algorithms . You can access algorithms for all purposes from several open-source packages. In addition, PHOTONAI offers several utility classes as well, such as linear statistical feature selection or sample pairing algorithms. In addition you can specify hyperparameters as well as their value range in order to be optimized by the hyperparameter optimization strategy. Currently, PHOTONAI offers Grid-Search , Random Search and two frameworks for bayesian optimization.","title":"Algorithms"},{"location":"algorithms/algorithms_index/#pca","text":"1 2 3 4 5 6 from photonai.base import PipelineElement PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) # to test if disabling the PipelineElement improves performance, # simply add the test_disabled=True parameter","title":"PCA"},{"location":"algorithms/algorithms_index/#svc","text":"1 2 3 4 PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'poly' ]), 'C' : FloatRange ( 0.5 , 2 )}, gamma = 'auto' )","title":"SVC"},{"location":"algorithms/algorithms_index/#keras-neural-net","text":"1 2 3 4 5 6 7 8 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 , 3 ]]), 'dropout_rate' : Categorical ([[ 0.5 , 0.2 , 0.1 ], 0.1 ])}, activations = 'relu' , epochs = 5 , batch_size = 32 )","title":"Keras Neural Net"},{"location":"algorithms/estimators/","text":"Estimator All Classification Regression Linear Estimators Name Class Package ARDRegression sklearn.linear_model.ARDRegression scikit-learn BayesianRidge sklearn.linear_model.BayesianRidge scikit-learn ElasticNet sklearn.linear_model.ElasticNet scikit-learn HuberRegressor sklearn.linear_model.HuberRegressor scikit-learn Lars sklearn.linear_model.Lars scikit-learn Lasso sklearn.linear_model.Lasso scikit-learn LassoLars sklearn.linear_model.LassoLars scikit-learn LinearRegression sklearn.linear_model.LinearRegression scikit-learn LogisticRegression sklearn.linear_model.LogisticRegression scikit-learn PassiveAggressiveClassifier sklearn.linear_model.PassiveAggressiveClassifier scikit-learn Perceptron sklearn.linear_model.Perceptron scikit-learn RANSACRegressor sklearn.linear_model.RANSACRegressor scikit-learn Ridge sklearn.linear_model.Ridge scikit-learn RidgeClassifier sklearn.linear_model.RidgeClassifier scikit-learn SGDClassifier sklearn.linear_model.SGDClassifier scikit-learn SGDRegressor sklearn.linear_model.SGDRegressor scikit-learn TheilSenRegressor sklearn.linear_model.TheilSenRegressor scikit-learn Tree-based Name Class Package ExtraTreesClassifier sklearn.ensemble.ExtraTreesClassifier scikit-learn ExtraTreesRegressor sklearn.ensemble.ExtraTreesRegressor scikit-learn DecisionTreeClassifier sklearn.tree.DecisionTreeClassifier scikit-learn DecisionTreeRegressor sklearn.tree.DecisionTreeRegressor scikit-learn RandomForestClassifier sklearn.ensemble.RandomForestClassifier scikit-learn RandomForestRegressor sklearn.ensemble.RandomForestRegressor scikit-learn Supported Vector Machines Name Class Package LinearSVC sklearn.svm.LinearSVC scikit-learn LinearSVR sklearn.svm.LinearSVR scikit-learn NuSVC sklearn.svm.NuSVC scikit-learn NuSVR sklearn.svm.NuSVR scikit-learn OneClassSVM sklearn.svm.OneClassSVM scikit-learn PhotonOneClassSVM photonai.modelwrapper.PhotonOneClassSVM.PhotonOneClassSVM scikit-learn / PHOTONAI SVC sklearn.svm.SVC scikit-learn SVR sklearn.svm.SVR scikit-learn Neural Networks Name Class Package BernoulliRBM sklearn.neural_network.BernoulliRBM scikit-learn KerasDnnClassifier photonai.modelwrapper.keras_dnn_classifier.KerasDnnClassifier keras / PHOTONAI KerasDnnRegressor photonai.modelwrapper.keras_dnn_regressor.KerasDnnRegressor keras / PHOTONAI MLPClassifier sklearn.neural_network.MLPClassifier scikit-learn MLPRegressor sklearn.neural_network.MLPRegressor scikit-learn PhotonMLPClassifier photonai.modelwrapper.PhotonMLPClassifier.PhotonMLPClassifier scikit-learn / PHOTONAI Ensemble Name Class Package AdaBoostClassifier sklearn.ensemble.AdaBoostClassifier scikit-learn AdaBoostRegressor sklearn.ensemble.AdaBoostRegressor scikit-learn BaggingClassifier sklearn.ensemble.BaggingClassifier scikit-learn BaggingRegressor sklearn.ensemble.BaggingRegressor scikit-learn GradientBoostingClassifier sklearn.ensemble.GradientBoostingClassifier scikit-learn GradientBoostingRegressor sklearn.ensemble.GradientBoostingRegressor scikit-learn Neighour-Based Name Class Package KNeighborsClassifier sklearn.neighbors.KNeighborsClassifier scikit-learn KNeighborsRegressor sklearn.neighbors.KNeighborsRegressor scikit-learn NearestCentroid sklearn.neighbors.NearestCentroid scikit-learn RadiusNeighborsClassifier sklearn.neighbors.RadiusNeighborsClassifier scikit-learn RadiusNeighborsRegressor sklearn.neighbors.RadiusNeighborsRegressor scikit-learn Probabilistic Name Class Package BayesianGaussianMixture sklearn.mixture.BayesianGaussianMixture scikit-learn BernoulliNB sklearn.naive_bayes.BernoulliNB scikit-learn GaussianNB sklearn.naive_bayes.GaussianNB scikit-learn MultinomialNB sklearn.naive_bayes.MultinomialNB scikit-learn GaussianMixture sklearn.mixture.GaussianMixture scikit-learn GaussianProcessClassifier sklearn.gaussian_process.GaussianProcessClassifier scikit-learn GaussianProcessRegressor sklearn.gaussian_process.GaussianProcessRegressor scikit-learn Other Name Class Package DummyClassifier sklearn.dummy.DummyClassifier scikit-learn DummyRegressor sklearn.dummy.DummyRegressor scikit-learn KernelRidge sklearn.kernel_ridge.KernelRidge scikit-learn PhotonVotingClassifier photonai.modelwrapper.Voting.PhotonVotingClassifier PHOTONAI PhotonVotingRegressor photonai.modelwrapper.Voting.PhotonVotingRegressor PHOTONAI function myFunction(filter) { var input, table, tr, td, i, txtValue; if (filter == 'class') { document.getElementById(\"button_all\").classList.remove('md-button--primary'); document.getElementById(\"button_class\").classList.add('md-button--primary'); document.getElementById(\"button_reg\").classList.remove('md-button--primary'); } else if (filter == 'reg') { document.getElementById(\"button_all\").classList.remove('md-button--primary'); document.getElementById(\"button_class\").classList.remove('md-button--primary'); document.getElementById(\"button_reg\").classList.add('md-button--primary'); } else { document.getElementById(\"button_all\").classList.add('md-button--primary'); document.getElementById(\"button_class\").classList.remove('md-button--primary'); document.getElementById(\"button_reg\").classList.remove('md-button--primary'); } alltables = document.querySelectorAll(\"table[data-name=filterTable]\"); alltables.forEach(function(table){ tr = table.getElementsByTagName(\"tr\"); // Loop through all table rows, and hide those who don't match the search query for (i = 0; i < tr.length; i++) { txtValue = tr[i].getAttribute('ml-type'); if (txtValue) { if (txtValue.indexOf(filter) > -1) { tr[i].style.display = \"\"; } else { tr[i].style.display = \"none\"; } } } }); }","title":"Estimators"},{"location":"algorithms/hpos/","text":"Hyperparameter Optimization PHOTONAI offers easy access to several established hyperparameter optimization strategies. Grid Search An exhaustive searching through a manually specified subset of the hyperparameter space. The grid is defined by a finite list for each hyperparameter. 1 2 pipe = Hyperpipe ( \"...\" , optimizer = 'grid_search' ) Random Grid Search Random sampling of a manually specified subset of the hyperparameter space. The grid is defined by a finite list for each hyperparameter. Then, a specified number of random configurations from this grid is tested 1 2 3 4 pipe = Hyperpipe ( \"...\" , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 30 , 'limit_in_minutes' : 10 }) Random Search A grid-free selection of configurations based on the hyperparameter space. In the case of numerical parameters, decisions are made only on the basis of the interval limits. The creation of configurations is limited by time or a maximum number of runs. 1 2 3 4 pipe = Hyperpipe ( \"...\" , optimizer = 'random_search' , optimizer_params = { 'n_configurations' : 30 , 'limit_in_minutes' : 20 }) Scikit-Optimize Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details available here . A detailed parameter documentation here. 1 2 3 4 5 6 7 pipe = Hyperpipe ( \"...\" , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 55 , 'n_initial_points' : 15 , 'initial_point_generator' : \"sobol\" , 'acq_func' : 'LCB' , 'acq_func_kwargs' : { 'kappa' : 1.96 }}) Nevergrad Nevergrad is a gradient-free optimization platform. Thus, this package is suitable for optimizing over the hyperparamter space. As a great advantage, evolutionary algorithms are implemented here in addition to Bayesian techniques. Nevergrad usage and implementation details available here . 1 2 3 4 5 6 import nevergrad as ng # list of all available nevergrad optimizer print ( list ( ng . optimizers . registry . values ())) my_pipe = Hyperpipe ( \"...\" , optimizer = 'nevergrad' , optimizer_params = { 'facade' : 'NGO' , 'n_configurations' : 30 }) Smac SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details available here . 1 2 3 4 5 6 my_pipe = Hyperpipe ( \"...\" , optimizer = 'smac' , optimizer_params = { \"facade\" : \"SMAC4BO\" , \"wallclock_limit\" : 60.0 * 10 , # seconds \"ta_run_limit\" : 100 } # limit of configurations ) Switch Optimizer This optimizer is special, as it uses the strategies above to optimizes the same dataflow for different learning algorithms in a switch (\"OR\") element at the end of the pipeline. For example you can use bayesian optimization for each learning algorithm and select that each of the algorithms gets 25 configurations to be tested. This is different to a global optimization, in which, after an initial exploration phase, computational resources are dedicated to the best performing learning algorithm only. By equally distributing computational ressources to each learning algorithms, better comparability is achieved in-between the algorithms. This can according to the use case be desirable. 1 2 3 pipe = Hyperpipe ( \"...\" , optimizer = \"switch\" , optimizer_params = { 'name' : 'sk_opt' , 'n_configurations' : 25 })","title":"Hyperparameter Optimizers"},{"location":"algorithms/registry/","text":"Registry The PHOTONAI Registry class lets you register your class with a key, so that you can access it conveniently in your PHOTONAI Hyperpipe . setup via the PipelineElement class . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import os from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , PhotonRegistry from photonai.optimization import IntegerRange # REGISTER ELEMENT base_folder = os . path . dirname ( os . path . abspath ( __file__ )) custom_elements_folder = os . path . join ( base_folder , '../advanced/custom_elements' ) registry = PhotonRegistry ( custom_elements_folder = custom_elements_folder ) registry . register ( photon_name = 'MyCustomEstimator' , class_str = 'custom_estimator.CustomEstimator' , element_type = 'Estimator' ) registry . register ( photon_name = 'MyCustomTransformer' , class_str = 'custom_transformer.CustomTransformer' , element_type = 'Transformer' ) registry . activate () # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'custom_estimator_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 2 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # SHOW WHAT IS POSSIBLE IN THE CONSOLE registry . list_available_elements () # NOW FIND OUT MORE ABOUT A SPECIFIC ELEMENT registry . info ( 'MyCustomEstimator' ) registry . info ( 'MyCustomTransformer' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) my_pipe += PipelineElement ( 'MyCustomEstimator' ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Registry"},{"location":"algorithms/transformers/","text":"Transformer Decomposition Name Class Package CCA sklearn.cross_decomposition.CCA scikit-learn DictionaryLearning sklearn.decomposition.DictionaryLearning scikit-learn dict_learning sklearn.decomposition.dict_learning scikit-learn dict_learning_online sklearn.decomposition.dict_learning_online scikit-learn FactorAnalysis sklearn.decomposition.FactorAnalysis scikit-learn FastICA sklearn.decomposition.FastICA scikit-learn IncrementalPCA sklearn.decomposition.IncrementalPCA scikit-learn KernelPCA sklearn.decomposition.KernelPCA scikit-learn LatentDirichletAllocation sklearn.decomposition.LatentDirichletAllocation scikit-learn MiniBatchDictionaryLearning sklearn.decomposition.MiniBatchDictionaryLearning scikit-learn MiniBatchSparsePCA sklearn.decomposition.MiniBatchSparsePCA scikit-learn NMF sklearn.decomposition.NMF scikit-learn PCA sklearn.decomposition.PCA scikit-learn PLSCanonical sklearn.cross_decomposition.PLSCanonical scikit-learn PLSRegression sklearn.cross_decomposition.PLSRegression scikit-learn PLSSVD sklearn.cross_decomposition.PLSSVD scikit-learn SparsePCA sklearn.decomposition.SparsePCA scikit-learn SparseCoder sklearn.decomposition.SparseCoder scikit-learn TruncatedSVD sklearn.decomposition.TruncatedSVD scikit-learn sparse_encode sklearn.decomposition.sparse_encode scikit-learn Feature Selection Name Class Package FClassifSelectPercentile photonai.modelwrapper.FeatureSelection.FClassifSelectPercentile PHOTONAI FRegressionFilterPValue photonai.modelwrapper.FeatureSelection.FRegressionFilterPValue PHOTONAI FRegressionSelectPercentile photonai.modelwrapper.FeatureSelection.FRegressionSelectPercentile PHOTONAI GenericUnivariateSelect sklearn.feature_selection.GenericUnivariateSelect scikit-learn LassoFeatureSelection photonai.modelwrapper.FeatureSelection.LassoFeatureSelection PHOTONAI RFE sklearn.feature_selection.RFE scikit-learn RFECV sklearn.feature_selection.RFECV scikit-learn SelectPercentile sklearn.feature_selection.SelectPercentile scikit-learn SelectKBest sklearn.feature_selection.SelectKBest scikit-learn SelectFpr sklearn.feature_selection.SelectFpr scikit-learn SelectFdr sklearn.feature_selection.SelectFdr scikit-learn SelectFromModel sklearn.feature_selection.SelectFromModel scikit-learn SelectFwe sklearn.feature_selection.SelectFwe scikit-learn VarianceThreshold sklearn.feature_selection.VarianceThreshold scikit-learn Preprocessing Name Class Package Binarizer sklearn.preprocessing.Binarizer scikit-learn FeatureEncoder photonai.modelwrapper.OrdinalEncoder.FeatureEncoder PHOTON FunctionTransformer sklearn.preprocessing.FunctionTransformer scikit-learn KernelCenterer sklearn.preprocessing.KernelCenterer scikit-learn LabelEncoder sklearn.preprocessing.LabelEncoder scikit-learn MaxAbsScaler sklearn.preprocessing.MaxAbsScaler scikit-learn MinMaxScaler sklearn.preprocessing.MinMaxScaler scikit-learn Normalizer sklearn.preprocessing.Normalizer scikit-learn PolynomialFeatures sklearn.preprocessing.PolynomialFeatures scikit-learn QuantileTransformer sklearn.preprocessing.QuantileTransformer scikit-learn RobustScaler sklearn.preprocessing.RobustScaler scikit-learn SimpleImputer sklearn.impute.SimpleImputer scikit-learn StandardScaler sklearn.preprocessing.StandardScaler scikit-learn SourceSplitter photonai.modelwrapper.source_splitter.SourceSplitter PHOTONAI Other Name Class Package ConfounderRemoval photonai.modelwrapper.ConfounderRemoval.ConfounderRemoval PHOTONAI ImbalancedDataTransformer photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer imbalanced-learn / PHOTONAI SamplePairingClassification photonai.modelwrapper.SamplePairing.SamplePairingClassification PHOTONAI SamplePairingRegression photonai.modelwrapper.SamplePairing.SamplePairingRegression PHOTONAI","title":"Transformers"},{"location":"api/architecture/","text":"Package Structure The photonai source code is divided in the following folders base : Here reside photonai's core elements such as the Hyperpipe, the pipeline, the pipeline element and all other photonai pipeline specialities. helper : not much to say here modelwrapper : All algorithms shipped with PHOTONAI and wrappers for accessing non-scikit-learn conform algorithms are stored here. optimization : Everything around Hyperparameter Optimization. photonlogger : Special logging logic to make everything as informative and pretty as possible. Also to avoid naming conflicts with loggers from other packages. processing : Here reside all classes that do the actual computing","title":"Overview"},{"location":"api/architecture/#package-structure","text":"The photonai source code is divided in the following folders base : Here reside photonai's core elements such as the Hyperpipe, the pipeline, the pipeline element and all other photonai pipeline specialities. helper : not much to say here modelwrapper : All algorithms shipped with PHOTONAI and wrappers for accessing non-scikit-learn conform algorithms are stored here. optimization : Everything around Hyperparameter Optimization. photonlogger : Special logging logic to make everything as informative and pretty as possible. Also to avoid naming conflicts with loggers from other packages. processing : Here reside all classes that do the actual computing","title":"Package Structure"},{"location":"api/custom_estimator/","text":"Custom Estimator You can combine your own learning algorithm, bet it a neural net or anything else, by simply adhering to the scikit-learn interface as shown below. Then register your class with the Register module and you're done! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np from sklearn.base import BaseEstimator , ClassifierMixin class CustomEstimator ( BaseEstimator , ClassifierMixin ): def __init__ ( self , param1 = 0 , param2 = None ): # it is important that you name your params the same in the constructor # stub as well as in your class variables! self . param1 = param1 self . param2 = param2 def fit ( self , X , y = None , ** kwargs ): \"\"\" Adjust the underlying model or method to the data. Returns ------- IMPORTANT: must return self! \"\"\" return self def predict ( self , X ): \"\"\" Use the learned model to make predictions. \"\"\" return np . random . randint ( 0 , 2 , X . shape [ 0 ])","title":"Custom Estimator"},{"location":"api/custom_transformer/","text":"Custom Transformer You can add your own method, be it preprocessing, feature selection or dimensionality reduction, by simply adhering to the scikit-learn interface as shown below. Then register your class with the Register module and you're good to go. You can then combine it with any optimizer and metric and design your custom pipeline layout. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # we use BaseEstimator as to prepare the transformer for hyperparameter optimization # we inherit the get_params and set_params methods from sklearn.base import BaseEstimator class CustomTransformer ( BaseEstimator ): def __init__ ( self , param1 = 0 , param2 = None ): # it is important that you name your params the same in the constructor # stub as well as in your class variables! self . param1 = param1 self . param2 = param2 def fit ( self , data , targets = None , ** kwargs ): \"\"\" Adjust the underlying model or method to the data. Returns ------- IMPORTANT: must return self! \"\"\" return self def transform ( self , data , targets = None , ** kwargs ): \"\"\" Apply the method's logic to the data. \"\"\" return data","title":"Custom Transformer"},{"location":"api/hyperpipe/","text":"Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. __init__ ( self , mongodb_connect_url = None , save_output = True , overwrite_results = False , generate_best_model = True , user_id = '' , wizard_object_id = '' , wizard_project_name = '' , project_folder = '' ) special Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"Hyperpipe"},{"location":"api/hyperpipe/#photonai.base.hyperpipe.OutputSettings","text":"Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances.","title":"photonai.base.hyperpipe.OutputSettings"},{"location":"api/hyperpipe/#photonai.base.hyperpipe.OutputSettings.__init__","text":"Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"__init__()"},{"location":"api/base/branch/","text":"Documentation for Branch A substream of pipeline elements that is encapsulated, e.g. for parallelization. Examples: 1 2 3 4 5 6 7 8 from photonai.base import Branch from photonai.optimization import IntegerRange tree_qua_branch = Branch ( 'tree_branch' ) tree_qua_branch += PipelineElement ( 'QuantileTransformer' , n_quantiles = 100 ) tree_qua_branch += PipelineElement ( 'DecisionTreeClassifier' , { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) __iadd__ ( self , pipe_element ) special Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The PipelineElement to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" super ( Branch , self ) . __iadd__ ( pipe_element ) self . _prepare_pipeline () return self __init__ ( self , name , elements = None ) special Initialize the object. Parameters: Name Type Description Default name str Name of the encapsulated item and/or summary of the encapsulated element`s functions. required elements List[photonai.base.photon_elements.PipelineElement] List of PipelineElements added one after another to the Branch. None Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None ): \"\"\" Initialize the object. Parameters: name: Name of the encapsulated item and/or summary of the encapsulated element`s functions. elements: List of PipelineElements added one after another to the Branch. \"\"\" super () . __init__ ( name , {}, test_disabled = False , disabled = False , base_element = True ) # in case any of the children needs y or covariates we need to request them self . needs_y = True self . needs_covariates = True self . elements = [] self . has_hyperparameters = True self . skip_caching = True self . identifier = \"BRANCH:\" # needed for caching on individual level self . fix_fold_id = False self . do_not_delete_cache_folder = False # add elements if elements : for element in elements : self . add ( element ) add ( self , pipe_element ) Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The PipelineElement to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) fit ( self , X , y = None , ** kwargs ) Calls the fit function on all underlying base elements. Parameters: Name Type Description Default X ndarray The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function on all underlying base elements. Parameters: X: The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" self . base_element = Branch . sanity_check_pipeline ( self . base_element ) return super () . fit ( X , y , ** kwargs ) predict ( self , X , ** kwargs ) Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_elements predict method. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict method. Returns: Prediction values. \"\"\" return super () . predict ( X , ** kwargs ) transform ( self , X , y = None , ** kwargs ) Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements predict/transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) Transformed/Predicted data. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements predict/transform. Returns: Transformed/Predicted data. \"\"\" if self . _estimator_type == 'classifier' or self . _estimator_type == 'regressor' : return super () . predict ( X ), y , kwargs return super () . transform ( X , y , ** kwargs )","title":"Branch"},{"location":"api/base/branch/#documentation-for-branch","text":"","title":"Documentation for Branch"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch","text":"A substream of pipeline elements that is encapsulated, e.g. for parallelization. Examples: 1 2 3 4 5 6 7 8 from photonai.base import Branch from photonai.optimization import IntegerRange tree_qua_branch = Branch ( 'tree_branch' ) tree_qua_branch += PipelineElement ( 'QuantileTransformer' , n_quantiles = 100 ) tree_qua_branch += PipelineElement ( 'DecisionTreeClassifier' , { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' )","title":"photonai.base.photon_elements.Branch"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.__iadd__","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The PipelineElement to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" super ( Branch , self ) . __iadd__ ( pipe_element ) self . _prepare_pipeline () return self","title":"__iadd__()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.__init__","text":"Initialize the object. Parameters: Name Type Description Default name str Name of the encapsulated item and/or summary of the encapsulated element`s functions. required elements List[photonai.base.photon_elements.PipelineElement] List of PipelineElements added one after another to the Branch. None Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None ): \"\"\" Initialize the object. Parameters: name: Name of the encapsulated item and/or summary of the encapsulated element`s functions. elements: List of PipelineElements added one after another to the Branch. \"\"\" super () . __init__ ( name , {}, test_disabled = False , disabled = False , base_element = True ) # in case any of the children needs y or covariates we need to request them self . needs_y = True self . needs_covariates = True self . elements = [] self . has_hyperparameters = True self . skip_caching = True self . identifier = \"BRANCH:\" # needed for caching on individual level self . fix_fold_id = False self . do_not_delete_cache_folder = False # add elements if elements : for element in elements : self . add ( element )","title":"__init__()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.add","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The PipelineElement to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The PipelineElement to add, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element )","title":"add()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.fit","text":"Calls the fit function on all underlying base elements. Parameters: Name Type Description Default X ndarray The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function on all underlying base elements. Parameters: X: The array-like input with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" self . base_element = Branch . sanity_check_pipeline ( self . base_element ) return super () . fit ( X , y , ** kwargs )","title":"fit()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.predict","text":"Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_elements predict method. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict method. Returns: Prediction values. \"\"\" return super () . predict ( X , ** kwargs )","title":"predict()"},{"location":"api/base/branch/#photonai.base.photon_elements.Branch.transform","text":"Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements predict/transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) Transformed/Predicted data. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls the transform function on all underlying base elements. If _estimator_type is in ['classifier', 'regressor'], predict is called instead. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements predict/transform. Returns: Transformed/Predicted data. \"\"\" if self . _estimator_type == 'classifier' or self . _estimator_type == 'regressor' : return super () . predict ( X ), y , kwargs return super () . transform ( X , y , ** kwargs )","title":"transform()"},{"location":"api/base/hyperpipe/","text":"Documentation for Hyperpipe The PHOTONAI Hyperpipe class creates a custom machine learning pipeline. In addition it defines the relevant analysis\u2019 parameters such as the cross-validation scheme, the hyperparameter optimization strategy, and the performance metrics of interest. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods or algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI Hyperpipe automatizes the nested training, test and hyperparameter optimization procedures. The Hyperpipe monitors: the nested-cross-validated training and test procedure, communicates with the hyperparameter optimization strategy, streams information between the pipeline elements, logs all results obtained and evaluates the performance, guides the hyperparameter optimization process by a so-called best config metric which is used to select the best performing hyperparameter configuration. Attributes: Name Type Description optimum_pipe PhotonPipeline An sklearn pipeline object that is fitted to the training data according to the best hyperparameter configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds. We find the best config by comparing the test error across outer folds. The hyperparameter config of the best fold is used as the optimal model and is then trained on the complete set. best_config dict Dictionary containing the hyperparameters of the best configuration. Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value. results MDBHyperpipe Object containing all information about the for the performed hyperparameter search. Holds the training and test metrics for all outer folds, inner folds and configurations, as well as additional information. elements list Contains `all PipelineElement or Hyperpipe objects that are added to the pipeline. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.model_selection import ShuffleSplit , KFold from sklearn.datasets import load_breast_cancer hyperpipe = Hyperpipe ( 'myPipe' , optimizer = 'random_grid_search' , optimizer_params = { 'limit_in_minutes' : 5 }, outer_cv = ShuffleSplit ( test_size = 0.2 , n_splits = 3 ), inner_cv = KFold ( n_splits = 10 , shuffle = True ), metrics = [ 'accuracy' , 'precision' , 'recall' , \"f1_score\" ], best_config_metric = 'accuracy' , eval_final_performance = True , verbosity = 0 ) hyperpipe += PipelineElement ( \"SVC\" , hyperparameters = { \"C\" : FloatRange ( 1 , 100 )}) X , y = load_breast_cancer ( return_X_y = True ) hyperpipe . fit ( X , y ) __init__ ( self , name , inner_cv = None , outer_cv = None , optimizer = 'grid_search' , optimizer_params = None , metrics = None , best_config_metric = None , eval_final_performance = None , use_test_set = True , test_size = 0.2 , project_folder = '' , calculate_metrics_per_fold = True , calculate_metrics_across_folds = False , random_seed = None , verbosity = 0 , learning_curves = False , learning_curves_cut = None , output_settings = None , performance_constraints = None , permutation_id = None , cache_folder = None , nr_of_processes = 1 , allow_multidim_targets = False ) special Initialize the object. Parameters: Name Type Description Default name Optional[str] Name of hyperpipe instance. required inner_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to test hyperparameter configurations, generates the validation set. None outer_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to use for the hyperparameter search itself, generates the test set. None optimizer str Hyperparameter optimization algorithm. In case a string literal is given: \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. \"sk_opt\": Scikit-Optimize based on theories of Baysian optimization. \"random_search\": randomly chooses hyperparameter from grid-free domain. \"smac\": SMAC based on theories of Baysian optimization. \"nevergrad\": Nevergrad based on theories of evolutionary learning. In case an object is given: expects the object to have the following methods: ask : returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding model_name__parameter_name: parameter_value prepare : takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space tell : gets a tested config and the respective performance in order to calculate a smart next configuration to process 'grid_search' metrics Optional[List[Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str]]] Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own Metrics for classification : accuracy : sklearn.metrics.accuracy_score matthews_corrcoef : sklearn.metrics.matthews_corrcoef confusion_matrix : sklearn.metrics.confusion_matrix, f1_score : sklearn.metrics.f1_score hamming_loss : sklearn.metrics.hamming_loss log_loss : sklearn.metrics.log_loss precision : sklearn.metrics.precision_score recall : sklearn.metrics.recall_score Metrics for regression : mean_squared_error : sklearn.metrics.mean_squared_error mean_absolute_error : sklearn.metrics.mean_absolute_error explained_variance : sklearn.metrics.explained_variance_score r2 : sklearn.metrics.r2_score Other metrics pearson_correlation : photon_core.framework.Metrics.pearson_correlation variance_explained : photon_core.framework.Metrics.variance_explained_score categorical_accuracy : photon_core.framework.Metrics.categorical_accuracy_score None best_config_metric Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str] The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. None eval_final_performance bool DEPRECATED! Use \"use_test_set\" instead! None use_test_set bool If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. True project_folder str The output folder in which all files generated by the PHOTONAI project are saved to. '' test_size float The amount of the data that should be left out if no outer_cv is given and eval_final_perfomance is set to True. 0.2 calculate_metrics_per_fold bool If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. True calculate_metrics_across_folds bool If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. False random_seed int Random Seed. None verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 0 learning_curves bool Enables larning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. False learning_curves_cut FloatRange The tested relativ cuts for data size. None performance_constraints list Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. None permutation_id str String identifier for permutation tests. None cache_folder str Folder path for multi-processing. None nr_of_processes int Determined the amount of simultaneous calculation of outer_folds. 1 allow_multidim_targets bool Allows multidimensional targets. False Source code in photonai/base/hyperpipe.py def __init__ ( self , name : Optional [ str ], inner_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits ] = None , outer_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits , None ] = None , optimizer : str = 'grid_search' , optimizer_params : dict = None , metrics : Optional [ List [ Union [ Scorer . Metric_Type , str ]]] = None , best_config_metric : Optional [ Union [ Scorer . Metric_Type , str ]] = None , eval_final_performance : bool = None , use_test_set : bool = True , test_size : float = 0.2 , project_folder : str = '' , calculate_metrics_per_fold : bool = True , calculate_metrics_across_folds : bool = False , random_seed : int = None , verbosity : int = 0 , learning_curves : bool = False , learning_curves_cut : FloatRange = None , output_settings : OutputSettings = None , performance_constraints : list = None , permutation_id : str = None , cache_folder : str = None , nr_of_processes : int = 1 , allow_multidim_targets : bool = False ): \"\"\" Initialize the object. Parameters: name: Name of hyperpipe instance. inner_cv: Cross validation strategy to test hyperparameter configurations, generates the validation set. outer_cv: Cross validation strategy to use for the hyperparameter search itself, generates the test set. optimizer: Hyperparameter optimization algorithm. - In case a string literal is given: - \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. - \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. - \"sk_opt\": Scikit-Optimize based on theories of Baysian optimization. - \"random_search\": randomly chooses hyperparameter from grid-free domain. - \"smac\": SMAC based on theories of Baysian optimization. - \"nevergrad\": Nevergrad based on theories of evolutionary learning. - In case an object is given: expects the object to have the following methods: - `ask`: returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value` - `prepare`: takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space - `tell`: gets a tested config and the respective performance in order to calculate a smart next configuration to process metrics: Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own - Metrics for `classification`: - `accuracy`: sklearn.metrics.accuracy_score - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef - `confusion_matrix`: sklearn.metrics.confusion_matrix, - `f1_score`: sklearn.metrics.f1_score - `hamming_loss`: sklearn.metrics.hamming_loss - `log_loss`: sklearn.metrics.log_loss - `precision`: sklearn.metrics.precision_score - `recall`: sklearn.metrics.recall_score - Metrics for `regression`: - `mean_squared_error`: sklearn.metrics.mean_squared_error - `mean_absolute_error`: sklearn.metrics.mean_absolute_error - `explained_variance`: sklearn.metrics.explained_variance_score - `r2`: sklearn.metrics.r2_score - Other metrics - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation - `variance_explained`: photon_core.framework.Metrics.variance_explained_score - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score best_config_metric: The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. eval_final_performance [bool, default=True]: DEPRECATED! Use \"use_test_set\" instead! use_test_set [bool, default=True]: If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. project_folder: The output folder in which all files generated by the PHOTONAI project are saved to. test_size: The amount of the data that should be left out if no outer_cv is given and eval_final_perfomance is set to True. calculate_metrics_per_fold: If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. calculate_metrics_across_folds: If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. random_seed: Random Seed. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. learning_curves: Enables larning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. learning_curves_cut: The tested relativ cuts for data size. performance_constraints: Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. permutation_id: String identifier for permutation tests. cache_folder: Folder path for multi-processing. nr_of_processes: Determined the amount of simultaneous calculation of outer_folds. allow_multidim_targets: Allows multidimensional targets. \"\"\" self . name = re . sub ( r '\\W+' , '' , name ) if eval_final_performance is not None : depr_warning = \"Hyperpipe parameter eval_final_performance is deprecated. It's called use_test_set now.\" use_test_set = eval_final_performance logger . warning ( depr_warning ) raise DeprecationWarning ( depr_warning ) # ====================== Cross Validation =========================== # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False if not calculate_metrics_across_folds and not calculate_metrics_per_fold : raise NotImplementedError ( \"Apparently, you've set calculate_metrics_across_folds=False and \" \"calculate_metrics_per_fold=False. In this case PHOTONAI does not calculate \" \"any metrics which doesn't make any sense. Set at least one to True.\" ) if inner_cv is None : msg = \"PHOTONAI requires an inner_cv split. Please enable inner cross-validation. \" \\ \"As exmaple: Hyperpipe(...inner_cv = KFold(n_splits = 3), ...). \" \\ \"Ensure you import the cross_validation object first.\" logger . error ( msg ) raise AttributeError ( msg ) # use default cut 'FloatRange(0, 1, 'range', 0.2)' if learning_curves = True but learning_curves_cut is None if learning_curves and learning_curves_cut is None : learning_curves_cut = FloatRange ( 0 , 1 , 'range' , 0.2 ) elif not learning_curves and learning_curves_cut is not None : learning_curves_cut = None self . cross_validation = Hyperpipe . CrossValidation ( inner_cv = inner_cv , outer_cv = outer_cv , use_test_set = use_test_set , test_size = test_size , calculate_metrics_per_fold = calculate_metrics_per_fold , calculate_metrics_across_folds = calculate_metrics_across_folds , learning_curves = learning_curves , learning_curves_cut = learning_curves_cut ) # ====================== Data =========================== self . data = Hyperpipe . Data () # ====================== Output Folder and Log File Management =========================== if output_settings : self . output_settings = output_settings else : self . output_settings = OutputSettings () if project_folder == '' : self . project_folder = os . getcwd () else : self . project_folder = project_folder self . output_settings . set_project_folder ( self . project_folder ) # update output options to add pipe name and timestamp to results folder self . _verbosity = 0 self . verbosity = verbosity self . output_settings . set_log_file () # ====================== Result Logging =========================== self . results_handler = None self . results = None self . best_config = None # ====================== Pipeline =========================== self . elements = [] self . _pipe = None self . optimum_pipe = None self . preprocessing = None # ====================== Performance Optimization =========================== if optimizer_params is None : optimizer_params = {} self . optimization = Optimization ( metrics = metrics , best_config_metric = best_config_metric , optimizer_input = optimizer , optimizer_params = optimizer_params , performance_constraints = performance_constraints ) # self.optimization.sanity_check_metrics() # ====================== Caching and Parallelization =========================== self . nr_of_processes = nr_of_processes if cache_folder : self . cache_folder = os . path . join ( cache_folder , self . name ) else : self . cache_folder = None # ====================== Internals =========================== self . permutation_id = permutation_id self . allow_multidim_targets = allow_multidim_targets self . is_final_fit = False # ====================== Random Seed =========================== self . random_state = random_seed if random_seed is not None : import random random . seed ( random_seed ) add ( self , pipe_element ) Add an element to the machine learning pipeline. Returns self. Parameters: Name Type Description Default pipe_element PipelineElement The object to add to the machine learning pipeline, being either a transformer or an estimator. required Source code in photonai/base/hyperpipe.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline. Returns self. Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element ) copy_me ( self ) Helper function to copy an entire Hyperpipe Returns: Type Description Hyperpipe Source code in photonai/base/hyperpipe.py def copy_me ( self ): \"\"\" Helper function to copy an entire Hyperpipe Returns: Hyperpipe \"\"\" signature = inspect . getfullargspec ( OutputSettings . __init__ )[ 0 ] settings = OutputSettings () for attr in signature : if hasattr ( self . output_settings , attr ): setattr ( settings , attr , getattr ( self . output_settings , attr )) self . output_settings . initialize_log_file () # create new Hyperpipe instance pipe_copy = Hyperpipe ( name = self . name , inner_cv = deepcopy ( self . cross_validation . inner_cv ), outer_cv = deepcopy ( self . cross_validation . outer_cv ), best_config_metric = self . optimization . best_config_metric , metrics = self . optimization . metrics , optimizer = self . optimization . optimizer_input_str , optimizer_params = self . optimization . optimizer_params , project_folder = self . project_folder , output_settings = settings ) signature = inspect . getfullargspec ( self . __init__ )[ 0 ] for attr in signature : if hasattr ( self , attr ) and attr != 'output_settings' : setattr ( pipe_copy , attr , getattr ( self , attr )) if hasattr ( self , 'preprocessing' ) and self . preprocessing : preprocessing = Preprocessing () for element in self . preprocessing . elements : preprocessing += element . copy_me () pipe_copy += preprocessing if hasattr ( self , 'elements' ): for element in self . elements : pipe_copy += element . copy_me () return pipe_copy fit ( self , data , targets , ** kwargs ) Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) requests new configurations from the hyperparameter search strategy, the optimizer, initializes the testing of a specific configuration, communicates the result to the optimizer, repeats 2-4 until optimizer delivers no more configurations to test finally searches for the best config in all tested configs, trains the pipeline with the best config and evaluates the performance on the test set Parameters: Name Type Description Default data ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required targets ndarray The truth array-like values with shape=[N], where N is the number of samples. required **kwargs Keyword arguments, passed to Outer_Fold_Manager.fit. {} Returns: Type Description Fitted Hyperpipe. Source code in photonai/base/hyperpipe.py def fit ( self , data : np . ndarray , targets : np . ndarray , ** kwargs ): \"\"\" Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: 1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) 2. requests new configurations from the hyperparameter search strategy, the optimizer, 3. initializes the testing of a specific configuration, 4. communicates the result to the optimizer, 5. repeats 2-4 until optimizer delivers no more configurations to test 6. finally searches for the best config in all tested configs, 7. trains the pipeline with the best config and evaluates the performance on the test set Parameters: data: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. targets: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to Outer_Fold_Manager.fit. Returns: Fitted Hyperpipe. \"\"\" # switch to result output folder start = datetime . datetime . now () self . output_settings . update_settings ( self . name , start . strftime ( \"%Y-%m- %d _%H-%M-%S\" )) logger . photon_system_log ( '=' * 101 ) logger . photon_system_log ( 'PHOTONAI ANALYSIS: ' + self . name ) logger . photon_system_log ( '=' * 101 ) logger . info ( \"Preparing data and PHOTONAI objects for analysis...\" ) # loop over outer cross validation if self . nr_of_processes > 1 : hyperpipe_client = Client ( threads_per_worker = 1 , n_workers = self . nr_of_processes , processes = False ) try : # check data self . data . input_data_sanity_checks ( data , targets , ** kwargs ) # create photon pipeline self . _prepare_pipeline () # initialize the progress monitors self . _prepare_result_logging ( start ) # apply preprocessing self . preprocess_data () if not self . is_final_fit : # Outer Folds outer_folds = FoldInfo . generate_folds ( self . cross_validation . outer_cv , self . data . X , self . data . y , self . data . kwargs , self . cross_validation . use_test_set , self . cross_validation . test_size ) self . cross_validation . outer_folds = { f . fold_id : f for f in outer_folds } delayed_jobs = [] # Run Dummy Estimator dummy_estimator = self . _prepare_dummy_estimator () if self . cache_folder is not None : logger . info ( \"Removing cache files...\" ) CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) # loop over outer cross validation for i , outer_f in enumerate ( outer_folds ): # 1. generate OuterFolds Object outer_fold = MDBOuterFold ( fold_nr = outer_f . fold_nr ) outer_fold_computer = OuterFoldManager ( self . _pipe , self . optimization , outer_f . fold_id , self . cross_validation , cache_folder = self . cache_folder , cache_updater = self . recursive_cache_folder_propagation , dummy_estimator = dummy_estimator , result_obj = outer_fold ) # 2. monitor outputs self . results . outer_folds . append ( outer_fold ) if self . nr_of_processes > 1 : result = dask . delayed ( Hyperpipe . fit_outer_folds )( outer_fold_computer , self . data . X , self . data . y , self . data . kwargs , self . cache_folder ) delayed_jobs . append ( result ) else : try : # 3. fit outer_fold_computer . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # 4. save outer fold results self . results_handler . save () finally : # 5. clear cache CacheManager . clear_cache_files ( self . cache_folder ) if self . nr_of_processes > 1 : dask . compute ( * delayed_jobs ) self . results_handler . save () # evaluate hyperparameter optimization results for best config self . _finalize_optimization () # clear complete cache ? CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) ############################################################################################### else : self . preprocess_data () self . _pipe . fit ( self . data . X , self . data . y , ** kwargs ) except Exception as e : logger . error ( e ) logger . error ( traceback . format_exc ()) traceback . print_exc () raise e finally : if self . nr_of_processes > 1 : hyperpipe_client . close () return self get_permutation_feature_importances ( self , X_val , y_val , ** kwargs ) Since PHOTONAI is built on top of the scikit-learn interface, it is possible to use direct functions from their package. Here the example of the feature importance via permutations . Parameters: Name Type Description Default X_val ndarray The array-like data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required y_val ndarray The array-like true targets. required **kwargs Keyword arguments, passed to sklearn.permutation_importance. {} Returns: Type Description Dictionary-like object, with the following attributes importances_mean, importances_std, importances. Source code in photonai/base/hyperpipe.py def get_permutation_feature_importances ( self , X_val : np . ndarray , y_val : np . ndarray , ** kwargs ): \"\"\" Since PHOTONAI is built on top of the scikit-learn interface, it is possible to use direct functions from their package. Here the example of the [feature importance via permutations]( https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html). Parameters: X_val: The array-like data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. y_val: The array-like true targets. **kwargs: Keyword arguments, passed to sklearn.permutation_importance. Returns: Dictionary-like object, with the following attributes: importances_mean, importances_std, importances. \"\"\" return permutation_importance ( self . optimum_pipe , X_val , y_val , ** kwargs ) inverse_transform_pipeline ( self , hyperparameters , data , targets , data_to_inverse ) Inverse transform data for a pipeline with specific hyperparameter configuration. Copy Sklearn Pipeline, Set Parameters Fit Pipeline to data and targets Inverse transform data with that pipeline Parameters: Name Type Description Default hyperparameters dict The concrete configuration settings for the pipeline elements. required data ndarray The training data to which the pipeline is fitted. required targets ndarray The truth values for training. required data_to_inverse ndarray The data that should be inversed after training. required Returns: Type Description ndarray Inverse data as array. Source code in photonai/base/hyperpipe.py def inverse_transform_pipeline ( self , hyperparameters : dict , data : np . ndarray , targets : np . ndarray , data_to_inverse : np . ndarray ) -> np . ndarray : \"\"\" Inverse transform data for a pipeline with specific hyperparameter configuration. 1. Copy Sklearn Pipeline, 2. Set Parameters 3. Fit Pipeline to data and targets 4. Inverse transform data with that pipeline Parameters: hyperparameters: The concrete configuration settings for the pipeline elements. data: The training data to which the pipeline is fitted. targets: The truth values for training. data_to_inverse: The data that should be inversed after training. Returns: Inverse data as array. \"\"\" copied_pipe = self . pipe . copy_me () copied_pipe . set_params ( ** hyperparameters ) copied_pipe . fit ( data , targets ) return copied_pipe . inverse_transform ( data_to_inverse ) load_optimum_pipe ( file , password = None ) staticmethod Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: Name Type Description Default file str File path specifying .photon file to load trained pipeline from zipped file. required password str Passcode for read file. None Returns: Type Description PhotonPipeline Returns pipeline with all trained PipelineElements. Source code in photonai/base/hyperpipe.py @staticmethod def load_optimum_pipe ( file : str , password : str = None ) -> PhotonPipeline : \"\"\" Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: file: File path specifying .photon file to load trained pipeline from zipped file. password: Passcode for read file. Returns: Returns pipeline with all trained PipelineElements. \"\"\" return PhotonModelPersistor . load_optimum_pipe ( file , password ) predict ( self , data , ** kwargs ) Use the optimum pipe to predict the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required **kwargs Keyword arguments, passed to optimum_pipe.predict. {} Returns: Type Description ndarray Predicted targets calculated on input data with trained model. Source code in photonai/base/hyperpipe.py def predict ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Predicted targets calculated on input data with trained model. \"\"\" # Todo: if local_search = true then use optimized pipe here? if self . _pipe : return self . optimum_pipe . predict ( data , ** kwargs ) predict_proba ( self , data , ** kwargs ) Use the optimum pipe to predict the probabilities from the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required **kwargs Keyword arguments, passed to optimum_pipe.predict_proba. {} Returns: Type Description ndarray Probabilities calculated from input data on fitted model. Source code in photonai/base/hyperpipe.py def predict_proba ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the probabilities from the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict_proba. Returns: Probabilities calculated from input data on fitted model. \"\"\" if self . _pipe : return self . optimum_pipe . predict_proba ( data , ** kwargs )","title":"Hyperpipe"},{"location":"api/base/hyperpipe/#documentation-for-hyperpipe","text":"","title":"Documentation for Hyperpipe"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe","text":"The PHOTONAI Hyperpipe class creates a custom machine learning pipeline. In addition it defines the relevant analysis\u2019 parameters such as the cross-validation scheme, the hyperparameter optimization strategy, and the performance metrics of interest. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing and combining data-processing methods or algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI Hyperpipe automatizes the nested training, test and hyperparameter optimization procedures. The Hyperpipe monitors: the nested-cross-validated training and test procedure, communicates with the hyperparameter optimization strategy, streams information between the pipeline elements, logs all results obtained and evaluates the performance, guides the hyperparameter optimization process by a so-called best config metric which is used to select the best performing hyperparameter configuration. Attributes: Name Type Description optimum_pipe PhotonPipeline An sklearn pipeline object that is fitted to the training data according to the best hyperparameter configuration found. Currently, we don't create an ensemble of all best hyperparameter configs over all folds. We find the best config by comparing the test error across outer folds. The hyperparameter config of the best fold is used as the optimal model and is then trained on the complete set. best_config dict Dictionary containing the hyperparameters of the best configuration. Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value. results MDBHyperpipe Object containing all information about the for the performed hyperparameter search. Holds the training and test metrics for all outer folds, inner folds and configurations, as well as additional information. elements list Contains `all PipelineElement or Hyperpipe objects that are added to the pipeline. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.model_selection import ShuffleSplit , KFold from sklearn.datasets import load_breast_cancer hyperpipe = Hyperpipe ( 'myPipe' , optimizer = 'random_grid_search' , optimizer_params = { 'limit_in_minutes' : 5 }, outer_cv = ShuffleSplit ( test_size = 0.2 , n_splits = 3 ), inner_cv = KFold ( n_splits = 10 , shuffle = True ), metrics = [ 'accuracy' , 'precision' , 'recall' , \"f1_score\" ], best_config_metric = 'accuracy' , eval_final_performance = True , verbosity = 0 ) hyperpipe += PipelineElement ( \"SVC\" , hyperparameters = { \"C\" : FloatRange ( 1 , 100 )}) X , y = load_breast_cancer ( return_X_y = True ) hyperpipe . fit ( X , y )","title":"photonai.base.hyperpipe.Hyperpipe"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.__init__","text":"Initialize the object. Parameters: Name Type Description Default name Optional[str] Name of hyperpipe instance. required inner_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to test hyperparameter configurations, generates the validation set. None outer_cv Union[sklearn.model_selection._split.BaseCrossValidator, sklearn.model_selection._split.BaseShuffleSplit, sklearn.model_selection._split._RepeatedSplits] Cross validation strategy to use for the hyperparameter search itself, generates the test set. None optimizer str Hyperparameter optimization algorithm. In case a string literal is given: \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. \"sk_opt\": Scikit-Optimize based on theories of Baysian optimization. \"random_search\": randomly chooses hyperparameter from grid-free domain. \"smac\": SMAC based on theories of Baysian optimization. \"nevergrad\": Nevergrad based on theories of evolutionary learning. In case an object is given: expects the object to have the following methods: ask : returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding model_name__parameter_name: parameter_value prepare : takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space tell : gets a tested config and the respective performance in order to calculate a smart next configuration to process 'grid_search' metrics Optional[List[Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str]]] Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own Metrics for classification : accuracy : sklearn.metrics.accuracy_score matthews_corrcoef : sklearn.metrics.matthews_corrcoef confusion_matrix : sklearn.metrics.confusion_matrix, f1_score : sklearn.metrics.f1_score hamming_loss : sklearn.metrics.hamming_loss log_loss : sklearn.metrics.log_loss precision : sklearn.metrics.precision_score recall : sklearn.metrics.recall_score Metrics for regression : mean_squared_error : sklearn.metrics.mean_squared_error mean_absolute_error : sklearn.metrics.mean_absolute_error explained_variance : sklearn.metrics.explained_variance_score r2 : sklearn.metrics.r2_score Other metrics pearson_correlation : photon_core.framework.Metrics.pearson_correlation variance_explained : photon_core.framework.Metrics.variance_explained_score categorical_accuracy : photon_core.framework.Metrics.categorical_accuracy_score None best_config_metric Union[Callable, keras.metrics.Metric, Type[keras.metrics.Metric], str] The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. None eval_final_performance bool DEPRECATED! Use \"use_test_set\" instead! None use_test_set bool If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. True project_folder str The output folder in which all files generated by the PHOTONAI project are saved to. '' test_size float The amount of the data that should be left out if no outer_cv is given and eval_final_perfomance is set to True. 0.2 calculate_metrics_per_fold bool If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. True calculate_metrics_across_folds bool If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. False random_seed int Random Seed. None verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 0 learning_curves bool Enables larning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. False learning_curves_cut FloatRange The tested relativ cuts for data size. None performance_constraints list Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. None permutation_id str String identifier for permutation tests. None cache_folder str Folder path for multi-processing. None nr_of_processes int Determined the amount of simultaneous calculation of outer_folds. 1 allow_multidim_targets bool Allows multidimensional targets. False Source code in photonai/base/hyperpipe.py def __init__ ( self , name : Optional [ str ], inner_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits ] = None , outer_cv : Union [ BaseCrossValidator , BaseShuffleSplit , _RepeatedSplits , None ] = None , optimizer : str = 'grid_search' , optimizer_params : dict = None , metrics : Optional [ List [ Union [ Scorer . Metric_Type , str ]]] = None , best_config_metric : Optional [ Union [ Scorer . Metric_Type , str ]] = None , eval_final_performance : bool = None , use_test_set : bool = True , test_size : float = 0.2 , project_folder : str = '' , calculate_metrics_per_fold : bool = True , calculate_metrics_across_folds : bool = False , random_seed : int = None , verbosity : int = 0 , learning_curves : bool = False , learning_curves_cut : FloatRange = None , output_settings : OutputSettings = None , performance_constraints : list = None , permutation_id : str = None , cache_folder : str = None , nr_of_processes : int = 1 , allow_multidim_targets : bool = False ): \"\"\" Initialize the object. Parameters: name: Name of hyperpipe instance. inner_cv: Cross validation strategy to test hyperparameter configurations, generates the validation set. outer_cv: Cross validation strategy to use for the hyperparameter search itself, generates the test set. optimizer: Hyperparameter optimization algorithm. - In case a string literal is given: - \"grid_search\": Optimizer that iteratively tests all possible hyperparameter combinations. - \"random_grid_search\": A variation of the grid search optimization that randomly picks hyperparameter combinations from all possible hyperparameter combinations. - \"sk_opt\": Scikit-Optimize based on theories of Baysian optimization. - \"random_search\": randomly chooses hyperparameter from grid-free domain. - \"smac\": SMAC based on theories of Baysian optimization. - \"nevergrad\": Nevergrad based on theories of evolutionary learning. - In case an object is given: expects the object to have the following methods: - `ask`: returns a hyperparameter configuration in form of an dictionary containing key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value` - `prepare`: takes a list of pipeline elements and their particular hyperparameters to prepare the hyperparameter space - `tell`: gets a tested config and the respective performance in order to calculate a smart next configuration to process metrics: Metrics that should be calculated for both training, validation and test set Use the preimported metrics from sklearn and photonai, or register your own - Metrics for `classification`: - `accuracy`: sklearn.metrics.accuracy_score - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef - `confusion_matrix`: sklearn.metrics.confusion_matrix, - `f1_score`: sklearn.metrics.f1_score - `hamming_loss`: sklearn.metrics.hamming_loss - `log_loss`: sklearn.metrics.log_loss - `precision`: sklearn.metrics.precision_score - `recall`: sklearn.metrics.recall_score - Metrics for `regression`: - `mean_squared_error`: sklearn.metrics.mean_squared_error - `mean_absolute_error`: sklearn.metrics.mean_absolute_error - `explained_variance`: sklearn.metrics.explained_variance_score - `r2`: sklearn.metrics.r2_score - Other metrics - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation - `variance_explained`: photon_core.framework.Metrics.variance_explained_score - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score best_config_metric: The metric that should be maximized or minimized in order to choose the best hyperparameter configuration. eval_final_performance [bool, default=True]: DEPRECATED! Use \"use_test_set\" instead! use_test_set [bool, default=True]: If the metrics should be calculated for the test set, otherwise the test set is seperated but not used. project_folder: The output folder in which all files generated by the PHOTONAI project are saved to. test_size: The amount of the data that should be left out if no outer_cv is given and eval_final_perfomance is set to True. calculate_metrics_per_fold: If True, the metrics are calculated for each inner_fold. If False, calculate_metrics_across_folds must be True. calculate_metrics_across_folds: If True, the metrics are calculated across all inner_fold. If False, calculate_metrics_per_fold must be True. random_seed: Random Seed. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. learning_curves: Enables larning curve procedure. Evaluate learning process over different sizes of input. Depends on learning_curves_cut. learning_curves_cut: The tested relativ cuts for data size. performance_constraints: Objects that indicate whether a configuration should be tested further. For example, the inner fold of a config does not perform better than the dummy performance. permutation_id: String identifier for permutation tests. cache_folder: Folder path for multi-processing. nr_of_processes: Determined the amount of simultaneous calculation of outer_folds. allow_multidim_targets: Allows multidimensional targets. \"\"\" self . name = re . sub ( r '\\W+' , '' , name ) if eval_final_performance is not None : depr_warning = \"Hyperpipe parameter eval_final_performance is deprecated. It's called use_test_set now.\" use_test_set = eval_final_performance logger . warning ( depr_warning ) raise DeprecationWarning ( depr_warning ) # ====================== Cross Validation =========================== # check if both calculate_metrics_per_folds and calculate_metrics_across_folds is False if not calculate_metrics_across_folds and not calculate_metrics_per_fold : raise NotImplementedError ( \"Apparently, you've set calculate_metrics_across_folds=False and \" \"calculate_metrics_per_fold=False. In this case PHOTONAI does not calculate \" \"any metrics which doesn't make any sense. Set at least one to True.\" ) if inner_cv is None : msg = \"PHOTONAI requires an inner_cv split. Please enable inner cross-validation. \" \\ \"As exmaple: Hyperpipe(...inner_cv = KFold(n_splits = 3), ...). \" \\ \"Ensure you import the cross_validation object first.\" logger . error ( msg ) raise AttributeError ( msg ) # use default cut 'FloatRange(0, 1, 'range', 0.2)' if learning_curves = True but learning_curves_cut is None if learning_curves and learning_curves_cut is None : learning_curves_cut = FloatRange ( 0 , 1 , 'range' , 0.2 ) elif not learning_curves and learning_curves_cut is not None : learning_curves_cut = None self . cross_validation = Hyperpipe . CrossValidation ( inner_cv = inner_cv , outer_cv = outer_cv , use_test_set = use_test_set , test_size = test_size , calculate_metrics_per_fold = calculate_metrics_per_fold , calculate_metrics_across_folds = calculate_metrics_across_folds , learning_curves = learning_curves , learning_curves_cut = learning_curves_cut ) # ====================== Data =========================== self . data = Hyperpipe . Data () # ====================== Output Folder and Log File Management =========================== if output_settings : self . output_settings = output_settings else : self . output_settings = OutputSettings () if project_folder == '' : self . project_folder = os . getcwd () else : self . project_folder = project_folder self . output_settings . set_project_folder ( self . project_folder ) # update output options to add pipe name and timestamp to results folder self . _verbosity = 0 self . verbosity = verbosity self . output_settings . set_log_file () # ====================== Result Logging =========================== self . results_handler = None self . results = None self . best_config = None # ====================== Pipeline =========================== self . elements = [] self . _pipe = None self . optimum_pipe = None self . preprocessing = None # ====================== Performance Optimization =========================== if optimizer_params is None : optimizer_params = {} self . optimization = Optimization ( metrics = metrics , best_config_metric = best_config_metric , optimizer_input = optimizer , optimizer_params = optimizer_params , performance_constraints = performance_constraints ) # self.optimization.sanity_check_metrics() # ====================== Caching and Parallelization =========================== self . nr_of_processes = nr_of_processes if cache_folder : self . cache_folder = os . path . join ( cache_folder , self . name ) else : self . cache_folder = None # ====================== Internals =========================== self . permutation_id = permutation_id self . allow_multidim_targets = allow_multidim_targets self . is_final_fit = False # ====================== Random Seed =========================== self . random_state = random_seed if random_seed is not None : import random random . seed ( random_seed )","title":"__init__()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.add","text":"Add an element to the machine learning pipeline. Returns self. Parameters: Name Type Description Default pipe_element PipelineElement The object to add to the machine learning pipeline, being either a transformer or an estimator. required Source code in photonai/base/hyperpipe.py def add ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the machine learning pipeline. Returns self. Parameters: pipe_element: The object to add to the machine learning pipeline, being either a transformer or an estimator. \"\"\" self . __iadd__ ( pipe_element )","title":"add()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.copy_me","text":"Helper function to copy an entire Hyperpipe Returns: Type Description Hyperpipe Source code in photonai/base/hyperpipe.py def copy_me ( self ): \"\"\" Helper function to copy an entire Hyperpipe Returns: Hyperpipe \"\"\" signature = inspect . getfullargspec ( OutputSettings . __init__ )[ 0 ] settings = OutputSettings () for attr in signature : if hasattr ( self . output_settings , attr ): setattr ( settings , attr , getattr ( self . output_settings , attr )) self . output_settings . initialize_log_file () # create new Hyperpipe instance pipe_copy = Hyperpipe ( name = self . name , inner_cv = deepcopy ( self . cross_validation . inner_cv ), outer_cv = deepcopy ( self . cross_validation . outer_cv ), best_config_metric = self . optimization . best_config_metric , metrics = self . optimization . metrics , optimizer = self . optimization . optimizer_input_str , optimizer_params = self . optimization . optimizer_params , project_folder = self . project_folder , output_settings = settings ) signature = inspect . getfullargspec ( self . __init__ )[ 0 ] for attr in signature : if hasattr ( self , attr ) and attr != 'output_settings' : setattr ( pipe_copy , attr , getattr ( self , attr )) if hasattr ( self , 'preprocessing' ) and self . preprocessing : preprocessing = Preprocessing () for element in self . preprocessing . elements : preprocessing += element . copy_me () pipe_copy += preprocessing if hasattr ( self , 'elements' ): for element in self . elements : pipe_copy += element . copy_me () return pipe_copy","title":"copy_me()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.fit","text":"Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) requests new configurations from the hyperparameter search strategy, the optimizer, initializes the testing of a specific configuration, communicates the result to the optimizer, repeats 2-4 until optimizer delivers no more configurations to test finally searches for the best config in all tested configs, trains the pipeline with the best config and evaluates the performance on the test set Parameters: Name Type Description Default data ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required targets ndarray The truth array-like values with shape=[N], where N is the number of samples. required **kwargs Keyword arguments, passed to Outer_Fold_Manager.fit. {} Returns: Type Description Fitted Hyperpipe. Source code in photonai/base/hyperpipe.py def fit ( self , data : np . ndarray , targets : np . ndarray , ** kwargs ): \"\"\" Starts the hyperparameter search and/or fits the pipeline to the data and targets. Manages the nested cross validated hyperparameter search: 1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2) 2. requests new configurations from the hyperparameter search strategy, the optimizer, 3. initializes the testing of a specific configuration, 4. communicates the result to the optimizer, 5. repeats 2-4 until optimizer delivers no more configurations to test 6. finally searches for the best config in all tested configs, 7. trains the pipeline with the best config and evaluates the performance on the test set Parameters: data: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. targets: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to Outer_Fold_Manager.fit. Returns: Fitted Hyperpipe. \"\"\" # switch to result output folder start = datetime . datetime . now () self . output_settings . update_settings ( self . name , start . strftime ( \"%Y-%m- %d _%H-%M-%S\" )) logger . photon_system_log ( '=' * 101 ) logger . photon_system_log ( 'PHOTONAI ANALYSIS: ' + self . name ) logger . photon_system_log ( '=' * 101 ) logger . info ( \"Preparing data and PHOTONAI objects for analysis...\" ) # loop over outer cross validation if self . nr_of_processes > 1 : hyperpipe_client = Client ( threads_per_worker = 1 , n_workers = self . nr_of_processes , processes = False ) try : # check data self . data . input_data_sanity_checks ( data , targets , ** kwargs ) # create photon pipeline self . _prepare_pipeline () # initialize the progress monitors self . _prepare_result_logging ( start ) # apply preprocessing self . preprocess_data () if not self . is_final_fit : # Outer Folds outer_folds = FoldInfo . generate_folds ( self . cross_validation . outer_cv , self . data . X , self . data . y , self . data . kwargs , self . cross_validation . use_test_set , self . cross_validation . test_size ) self . cross_validation . outer_folds = { f . fold_id : f for f in outer_folds } delayed_jobs = [] # Run Dummy Estimator dummy_estimator = self . _prepare_dummy_estimator () if self . cache_folder is not None : logger . info ( \"Removing cache files...\" ) CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) # loop over outer cross validation for i , outer_f in enumerate ( outer_folds ): # 1. generate OuterFolds Object outer_fold = MDBOuterFold ( fold_nr = outer_f . fold_nr ) outer_fold_computer = OuterFoldManager ( self . _pipe , self . optimization , outer_f . fold_id , self . cross_validation , cache_folder = self . cache_folder , cache_updater = self . recursive_cache_folder_propagation , dummy_estimator = dummy_estimator , result_obj = outer_fold ) # 2. monitor outputs self . results . outer_folds . append ( outer_fold ) if self . nr_of_processes > 1 : result = dask . delayed ( Hyperpipe . fit_outer_folds )( outer_fold_computer , self . data . X , self . data . y , self . data . kwargs , self . cache_folder ) delayed_jobs . append ( result ) else : try : # 3. fit outer_fold_computer . fit ( self . data . X , self . data . y , ** self . data . kwargs ) # 4. save outer fold results self . results_handler . save () finally : # 5. clear cache CacheManager . clear_cache_files ( self . cache_folder ) if self . nr_of_processes > 1 : dask . compute ( * delayed_jobs ) self . results_handler . save () # evaluate hyperparameter optimization results for best config self . _finalize_optimization () # clear complete cache ? CacheManager . clear_cache_files ( self . cache_folder , force_all = True ) ############################################################################################### else : self . preprocess_data () self . _pipe . fit ( self . data . X , self . data . y , ** kwargs ) except Exception as e : logger . error ( e ) logger . error ( traceback . format_exc ()) traceback . print_exc () raise e finally : if self . nr_of_processes > 1 : hyperpipe_client . close () return self","title":"fit()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.get_permutation_feature_importances","text":"Since PHOTONAI is built on top of the scikit-learn interface, it is possible to use direct functions from their package. Here the example of the feature importance via permutations . Parameters: Name Type Description Default X_val ndarray The array-like data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required y_val ndarray The array-like true targets. required **kwargs Keyword arguments, passed to sklearn.permutation_importance. {} Returns: Type Description Dictionary-like object, with the following attributes importances_mean, importances_std, importances. Source code in photonai/base/hyperpipe.py def get_permutation_feature_importances ( self , X_val : np . ndarray , y_val : np . ndarray , ** kwargs ): \"\"\" Since PHOTONAI is built on top of the scikit-learn interface, it is possible to use direct functions from their package. Here the example of the [feature importance via permutations]( https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html). Parameters: X_val: The array-like data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. y_val: The array-like true targets. **kwargs: Keyword arguments, passed to sklearn.permutation_importance. Returns: Dictionary-like object, with the following attributes: importances_mean, importances_std, importances. \"\"\" return permutation_importance ( self . optimum_pipe , X_val , y_val , ** kwargs )","title":"get_permutation_feature_importances()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.inverse_transform_pipeline","text":"Inverse transform data for a pipeline with specific hyperparameter configuration. Copy Sklearn Pipeline, Set Parameters Fit Pipeline to data and targets Inverse transform data with that pipeline Parameters: Name Type Description Default hyperparameters dict The concrete configuration settings for the pipeline elements. required data ndarray The training data to which the pipeline is fitted. required targets ndarray The truth values for training. required data_to_inverse ndarray The data that should be inversed after training. required Returns: Type Description ndarray Inverse data as array. Source code in photonai/base/hyperpipe.py def inverse_transform_pipeline ( self , hyperparameters : dict , data : np . ndarray , targets : np . ndarray , data_to_inverse : np . ndarray ) -> np . ndarray : \"\"\" Inverse transform data for a pipeline with specific hyperparameter configuration. 1. Copy Sklearn Pipeline, 2. Set Parameters 3. Fit Pipeline to data and targets 4. Inverse transform data with that pipeline Parameters: hyperparameters: The concrete configuration settings for the pipeline elements. data: The training data to which the pipeline is fitted. targets: The truth values for training. data_to_inverse: The data that should be inversed after training. Returns: Inverse data as array. \"\"\" copied_pipe = self . pipe . copy_me () copied_pipe . set_params ( ** hyperparameters ) copied_pipe . fit ( data , targets ) return copied_pipe . inverse_transform ( data_to_inverse )","title":"inverse_transform_pipeline()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.load_optimum_pipe","text":"Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: Name Type Description Default file str File path specifying .photon file to load trained pipeline from zipped file. required password str Passcode for read file. None Returns: Type Description PhotonPipeline Returns pipeline with all trained PipelineElements. Source code in photonai/base/hyperpipe.py @staticmethod def load_optimum_pipe ( file : str , password : str = None ) -> PhotonPipeline : \"\"\" Load optimum pipe from file. As staticmethod, instantiation is thus not required. Called backend: PhotonModelPersistor.load_optimum_pipe. Parameters: file: File path specifying .photon file to load trained pipeline from zipped file. password: Passcode for read file. Returns: Returns pipeline with all trained PipelineElements. \"\"\" return PhotonModelPersistor . load_optimum_pipe ( file , password )","title":"load_optimum_pipe()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.predict","text":"Use the optimum pipe to predict the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required **kwargs Keyword arguments, passed to optimum_pipe.predict. {} Returns: Type Description ndarray Predicted targets calculated on input data with trained model. Source code in photonai/base/hyperpipe.py def predict ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict. Returns: Predicted targets calculated on input data with trained model. \"\"\" # Todo: if local_search = true then use optimized pipe here? if self . _pipe : return self . optimum_pipe . predict ( data , ** kwargs )","title":"predict()"},{"location":"api/base/hyperpipe/#photonai.base.hyperpipe.Hyperpipe.predict_proba","text":"Use the optimum pipe to predict the probabilities from the input data. Parameters: Name Type Description Default data ndarray The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. required **kwargs Keyword arguments, passed to optimum_pipe.predict_proba. {} Returns: Type Description ndarray Probabilities calculated from input data on fitted model. Source code in photonai/base/hyperpipe.py def predict_proba ( self , data : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Use the optimum pipe to predict the probabilities from the input data. Parameters: data: The array-like prediction data with shape=[M, D], where M is the number of samples and D is the number of features. D must correspond to the number of trained dimensions of the fit method. **kwargs: Keyword arguments, passed to optimum_pipe.predict_proba. Returns: Probabilities calculated from input data on fitted model. \"\"\" if self . _pipe : return self . optimum_pipe . predict_proba ( data , ** kwargs )","title":"predict_proba()"},{"location":"api/base/output_settings/","text":"Documentation for OutputSettings Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances. __init__ ( self , mongodb_connect_url = None , save_output = True , overwrite_results = False , generate_best_model = True , user_id = '' , wizard_object_id = '' , wizard_project_name = '' , project_folder = '' ) special Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"OutputSettings"},{"location":"api/base/output_settings/#documentation-for-outputsettings","text":"","title":"Documentation for OutputSettings"},{"location":"api/base/output_settings/#photonai.base.hyperpipe.OutputSettings","text":"Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB or a simple son-file. You can also choose whether to save predictions and/or feature importances.","title":"photonai.base.hyperpipe.OutputSettings"},{"location":"api/base/output_settings/#photonai.base.hyperpipe.OutputSettings.__init__","text":"Initialize the object. Parameters: Name Type Description Default mongodb_connect_url str Valid mongodb connection url that specifies a database for storing the results. None save_output bool Controls the general saving of the results. True overwrite_results bool Allows overwriting the results folder if it already exists. False generate_best_model bool Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. True user_id str The user name of the according PHOTONAI Wizard login. '' wizard_object_id str The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. '' wizard_project_name str How the project is titled in the PHOTONAI Wizard. '' project_folder str Deprecated Parameter - transferred to Hyperpipe. '' Source code in photonai/base/hyperpipe.py def __init__ ( self , mongodb_connect_url : str = None , save_output : bool = True , overwrite_results : bool = False , generate_best_model : bool = True , user_id : str = '' , wizard_object_id : str = '' , wizard_project_name : str = '' , project_folder : str = '' ): \"\"\" Initialize the object. Parameters: mongodb_connect_url: Valid mongodb connection url that specifies a database for storing the results. save_output: Controls the general saving of the results. overwrite_results: Allows overwriting the results folder if it already exists. generate_best_model: Determines whether an optimum_pipe should be created and fitted. If False, no dependent files are created. user_id: The user name of the according PHOTONAI Wizard login. wizard_object_id: The object id to map the designed pipeline in the PHOTONAI Wizard to the results in the PHOTONAI CORE Database. wizard_project_name: How the project is titled in the PHOTONAI Wizard. project_folder: Deprecated Parameter - transferred to Hyperpipe. \"\"\" if project_folder : msg = \"Deprecated: The parameter 'project_folder' was moved to the Hyperpipe. \" \\ \"Please use Hyperpipe(..., project_folder='').\" logger . error ( msg ) raise DeprecationWarning ( msg ) self . mongodb_connect_url = mongodb_connect_url self . overwrite_results = overwrite_results self . user_id = user_id self . wizard_object_id = wizard_object_id self . wizard_project_name = wizard_project_name self . generate_best_model = generate_best_model self . save_output = save_output self . save_predictions_from_best_config_inner_folds = None self . verbosity = 0 self . results_folder = '' self . project_folder = '' self . log_file = '' self . logging_file_handler = None","title":"__init__()"},{"location":"api/base/pipeline_element/","text":"Documentation for PipelineElement PHOTONAI wrapper class for any transformer or estimator in the pipeline. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing, combining data-processing methods and algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI PipelineElement implements several helpful features: Saves the hyperparameters that should be tested and creates a grid of all hyperparameter configurations. Enables fast and rapid instantiation of pipeline elements per string identifier, e.g 'svc' creates an sklearn.svm.SVC object. Attaches a \"disable\" switch to every element in the pipeline in order to test a complete disable. __iadd__ ( self , pipe_element ) special Add an element to the intern list of elements. Parameters: Name Type Description Default pipe_element PipelineElement The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element ): \"\"\" Add an element to the intern list of elements. Parameters: pipe_element (PipelineElement): The object to add, being either a transformer or an estimator. \"\"\" PipelineElement . sanity_check_element_type_for_building_photon_pipes ( pipe_element , type ( self )) # check if that exact instance has been added before already_added_objects = len ([ i for i in self . elements if i is pipe_element ]) if already_added_objects > 0 : error_msg = \"Cannot add the same instance twice to \" + self . name + \" - \" + str ( type ( self )) logger . error ( error_msg ) raise ValueError ( error_msg ) # check for doubled names: already_existing_element_with_that_name = len ([ i for i in self . elements if i . name == pipe_element . name ]) if already_existing_element_with_that_name > 0 : error_msg = \"Already added a pipeline element with the name \" + pipe_element . name + \" to \" + self . name logger . warning ( error_msg ) warnings . warn ( error_msg ) # check for other items that have been renamed nr_of_existing_elements_with_that_name = len ([ i for i in self . elements if i . name . startswith ( pipe_element . name )]) new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) while len ([ i for i in self . elements if i . name == new_name ]) > 0 : nr_of_existing_elements_with_that_name += 1 new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) msg = \"Renaming \" + pipe_element . name + \" in \" + self . name + \" to \" + new_name + \" in \" + self . name logger . warning ( msg ) warnings . warn ( msg ) pipe_element . name = new_name self . elements . append ( pipe_element ) return self __init__ ( self , name , hyperparameters = None , test_disabled = False , disabled = False , base_element = None , batch_size = 0 , ** kwargs ) special Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. None test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False base_element BaseEstimator The underlying BaseEstimator. If not given the instantiation per string identifier takes place. None batch_size int Size of the division on which is calculated separately. 0 **kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , hyperparameters : dict = None , test_disabled : bool = False , disabled : bool = False , base_element : BaseEstimator = None , batch_size : int = 0 , ** kwargs ) -> None : \"\"\" Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. base_element: The underlying BaseEstimator. If not given the instantiation per string identifier takes place. batch_size: Size of the division on which is calculated separately. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. \"\"\" if hyperparameters is None : hyperparameters = {} if base_element is None : # Registering Pipeline Elements if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : registry = PhotonRegistry if name not in PhotonRegistry . ELEMENT_DICTIONARY : # try to reload PhotonRegistry . ELEMENT_DICTIONARY = PhotonRegistry () . get_package_info () if name in PhotonRegistry . ELEMENT_DICTIONARY : try : desired_class_info = PhotonRegistry . ELEMENT_DICTIONARY [ name ] desired_class_home = desired_class_info [ 0 ] desired_class_name = desired_class_info [ 1 ] imported_module = importlib . import_module ( desired_class_home ) desired_class = getattr ( imported_module , desired_class_name ) self . base_element = desired_class ( ** kwargs ) except AttributeError as ae : logger . error ( 'ValueError: Could not find according class:' + str ( PhotonRegistry . ELEMENT_DICTIONARY [ name ])) raise ValueError ( 'Could not find according class:' , PhotonRegistry . ELEMENT_DICTIONARY [ name ]) else : # if even after reload the element does not appear, it is not supported logger . error ( 'Element not supported right now:' + name ) raise NameError ( 'Element not supported right now:' , name ) else : self . base_element = base_element self . is_transformer = hasattr ( self . base_element , \"transform\" ) self . reduce_dimension = False # boolean - set on transform method self . is_estimator = hasattr ( self . base_element , \"predict\" ) self . _name = name self . initial_name = str ( name ) self . kwargs = kwargs self . current_config = None self . batch_size = batch_size self . test_disabled = test_disabled self . initial_hyperparameters = dict ( hyperparameters ) self . _sklearn_disabled = self . name + '__disabled' self . _hyperparameters = hyperparameters if len ( hyperparameters ) > 0 : key_0 = next ( iter ( hyperparameters )) if self . name not in key_0 : self . hyperparameters = hyperparameters else : self . hyperparameters = hyperparameters # self.initalize_hyperparameters = hyperparameters # check if hyperparameters are already in sklearn style # check if hyperparameters are members of the class if self . is_transformer or self . is_estimator : self . _check_hyperparameters ( BaseEstimator ) self . disabled = disabled # check if self.base element needs y for fitting and transforming if hasattr ( self . base_element , 'needs_y' ): self . needs_y = self . base_element . needs_y else : self . needs_y = False # or if it maybe needs covariates for fitting and transforming if hasattr ( self . base_element , 'needs_covariates' ): self . needs_covariates = self . base_element . needs_covariates else : self . needs_covariates = False self . _random_state = False create ( name , base_element , hyperparameters , test_disabled = False , disabled = False , ** kwargs ) classmethod Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required base_element BaseEstimator The underlying transformer or estimator class. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. required test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False **kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Examples: 1 2 3 4 5 6 7 8 9 10 11 12 class RD ( BaseEstimator , TransformerMixin ): def fit ( self , X , y , ** kwargs ): pass def fit_transform ( self , X , y = None , ** fit_params ): return self . transform ( X ) def transform ( self , X ): return X [:, : 3 ] trans = PipelineElement . create ( 'MyTransformer' , base_element = RD (), hyperparameters = {}) Source code in photonai/base/photon_elements.py @classmethod def create ( cls , name : str , base_element : BaseEstimator , hyperparameters : dict , test_disabled : bool = False , disabled : bool = False , ** kwargs ): \"\"\" Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: name: A string literal encoding the class to be instantiated. base_element: The underlying transformer or estimator class. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. Example: ``` python class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) ``` \"\"\" if isinstance ( base_element , type ): raise ValueError ( \"Base element should be an instance but is a class.\" ) return PipelineElement ( name , hyperparameters , test_disabled , disabled , base_element = base_element , ** kwargs ) fit ( self , X , y = None , ** kwargs ) Calls the fit function of the base element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Fitted self. \"\"\" if not self . disabled : obj = self . base_element arg_list = inspect . signature ( obj . fit ) if len ( arg_list . parameters ) > 2 : vals = arg_list . parameters . values () kwargs_param = list ( vals )[ - 1 ] if kwargs_param . kind == kwargs_param . VAR_KEYWORD : obj . fit ( X , y , ** kwargs ) return self obj . fit ( X , y ) return self inverse_transform ( self , X , y = None , ** kwargs ) Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) (X, y, kwargs) in back-transformed version. Source code in photonai/base/photon_elements.py def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Raises: NotImplementedError: Thrown when there is a dimensional reduction but no inverse is defined. Returns: (X, y, kwargs) in back-transformed version. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) elif self . is_transformer and self . reduce_dimension : msg = \" {} has no inverse_transform, but element reduce dimesions.\" . format ( self . name ) logger . error ( msg ) raise NotImplementedError ( msg ) return X , y , kwargs predict ( self , X , ** kwargs ) Calls the predict function of the underlying base_element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description ndarray Predictions values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function of the underlying base_element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Predictions values. \"\"\" if self . batch_size == 0 : return self . __predict ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict , X , ** kwargs ) score ( self , X_test , y_test ) Calls the score function on the base element. Parameters: Name Type Description Default X_test ndarray Input test data to score on. required y_test ndarray Input true targets to score on. required Returns: Type Description float A goodness of fit measure or a likelihood of unseen data. Source code in photonai/base/photon_elements.py def score ( self , X_test : np . ndarray , y_test : np . ndarray ) -> float : \"\"\" Calls the score function on the base element. Parameters: X_test: Input test data to score on. y_test: Input true targets to score on. Returns: A goodness of fit measure or a likelihood of unseen data. \"\"\" return self . base_element . score ( X_test , y_test ) transform ( self , X , y = None , ** kwargs ) Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) (X, y) in transformed version and original kwargs. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y) in transformed version and original kwargs. \"\"\" if self . batch_size == 0 : Xt , yt , kwargs = self . __transform ( X , y , ** kwargs ) else : Xt , yt , kwargs = self . __batch_transform ( X , y , ** kwargs ) if all ( hasattr ( data , \"shape\" ) for data in [ X , Xt ]) and all ( len ( data . shape ) > 1 for data in [ X , Xt ]): self . reduce_dimension = ( Xt . shape [ 1 ] < X . shape [ 1 ]) return Xt , yt , kwargs","title":"PipelineElement"},{"location":"api/base/pipeline_element/#documentation-for-pipelineelement","text":"","title":"Documentation for PipelineElement"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement","text":"PHOTONAI wrapper class for any transformer or estimator in the pipeline. So called PHOTONAI PipelineElements can be added to the Hyperpipe, each of them being a data-processing method or a learning algorithm. By choosing, combining data-processing methods and algorithms, and arranging them with the PHOTONAI classes, simple and complex pipeline architectures can be designed rapidly. The PHOTONAI PipelineElement implements several helpful features: Saves the hyperparameters that should be tested and creates a grid of all hyperparameter configurations. Enables fast and rapid instantiation of pipeline elements per string identifier, e.g 'svc' creates an sklearn.svm.SVC object. Attaches a \"disable\" switch to every element in the pipeline in order to test a complete disable.","title":"photonai.base.photon_elements.PipelineElement"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.__iadd__","text":"Add an element to the intern list of elements. Parameters: Name Type Description Default pipe_element PipelineElement The object to add, being either a transformer or an estimator. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element ): \"\"\" Add an element to the intern list of elements. Parameters: pipe_element (PipelineElement): The object to add, being either a transformer or an estimator. \"\"\" PipelineElement . sanity_check_element_type_for_building_photon_pipes ( pipe_element , type ( self )) # check if that exact instance has been added before already_added_objects = len ([ i for i in self . elements if i is pipe_element ]) if already_added_objects > 0 : error_msg = \"Cannot add the same instance twice to \" + self . name + \" - \" + str ( type ( self )) logger . error ( error_msg ) raise ValueError ( error_msg ) # check for doubled names: already_existing_element_with_that_name = len ([ i for i in self . elements if i . name == pipe_element . name ]) if already_existing_element_with_that_name > 0 : error_msg = \"Already added a pipeline element with the name \" + pipe_element . name + \" to \" + self . name logger . warning ( error_msg ) warnings . warn ( error_msg ) # check for other items that have been renamed nr_of_existing_elements_with_that_name = len ([ i for i in self . elements if i . name . startswith ( pipe_element . name )]) new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) while len ([ i for i in self . elements if i . name == new_name ]) > 0 : nr_of_existing_elements_with_that_name += 1 new_name = pipe_element . name + str ( nr_of_existing_elements_with_that_name + 1 ) msg = \"Renaming \" + pipe_element . name + \" in \" + self . name + \" to \" + new_name + \" in \" + self . name logger . warning ( msg ) warnings . warn ( msg ) pipe_element . name = new_name self . elements . append ( pipe_element ) return self","title":"__iadd__()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.__init__","text":"Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. None test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False base_element BaseEstimator The underlying BaseEstimator. If not given the instantiation per string identifier takes place. None batch_size int Size of the division on which is calculated separately. 0 **kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , hyperparameters : dict = None , test_disabled : bool = False , disabled : bool = False , base_element : BaseEstimator = None , batch_size : int = 0 , ** kwargs ) -> None : \"\"\" Takes a string literal and transforms it into an object of the associated class (see PhotonCore.JSON). Parameters: name: A string literal encoding the class to be instantiated. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. base_element: The underlying BaseEstimator. If not given the instantiation per string identifier takes place. batch_size: Size of the division on which is calculated separately. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. \"\"\" if hyperparameters is None : hyperparameters = {} if base_element is None : # Registering Pipeline Elements if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 : registry = PhotonRegistry if name not in PhotonRegistry . ELEMENT_DICTIONARY : # try to reload PhotonRegistry . ELEMENT_DICTIONARY = PhotonRegistry () . get_package_info () if name in PhotonRegistry . ELEMENT_DICTIONARY : try : desired_class_info = PhotonRegistry . ELEMENT_DICTIONARY [ name ] desired_class_home = desired_class_info [ 0 ] desired_class_name = desired_class_info [ 1 ] imported_module = importlib . import_module ( desired_class_home ) desired_class = getattr ( imported_module , desired_class_name ) self . base_element = desired_class ( ** kwargs ) except AttributeError as ae : logger . error ( 'ValueError: Could not find according class:' + str ( PhotonRegistry . ELEMENT_DICTIONARY [ name ])) raise ValueError ( 'Could not find according class:' , PhotonRegistry . ELEMENT_DICTIONARY [ name ]) else : # if even after reload the element does not appear, it is not supported logger . error ( 'Element not supported right now:' + name ) raise NameError ( 'Element not supported right now:' , name ) else : self . base_element = base_element self . is_transformer = hasattr ( self . base_element , \"transform\" ) self . reduce_dimension = False # boolean - set on transform method self . is_estimator = hasattr ( self . base_element , \"predict\" ) self . _name = name self . initial_name = str ( name ) self . kwargs = kwargs self . current_config = None self . batch_size = batch_size self . test_disabled = test_disabled self . initial_hyperparameters = dict ( hyperparameters ) self . _sklearn_disabled = self . name + '__disabled' self . _hyperparameters = hyperparameters if len ( hyperparameters ) > 0 : key_0 = next ( iter ( hyperparameters )) if self . name not in key_0 : self . hyperparameters = hyperparameters else : self . hyperparameters = hyperparameters # self.initalize_hyperparameters = hyperparameters # check if hyperparameters are already in sklearn style # check if hyperparameters are members of the class if self . is_transformer or self . is_estimator : self . _check_hyperparameters ( BaseEstimator ) self . disabled = disabled # check if self.base element needs y for fitting and transforming if hasattr ( self . base_element , 'needs_y' ): self . needs_y = self . base_element . needs_y else : self . needs_y = False # or if it maybe needs covariates for fitting and transforming if hasattr ( self . base_element , 'needs_covariates' ): self . needs_covariates = self . base_element . needs_covariates else : self . needs_covariates = False self . _random_state = False","title":"__init__()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.create","text":"Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: Name Type Description Default name str A string literal encoding the class to be instantiated. required base_element BaseEstimator The underlying transformer or estimator class. required hyperparameters dict Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. required test_disabled bool If the hyperparameter search should evaluate a complete disabling of the element. False disabled bool If true, the element is currently disabled and does nothing except return the data it received. False **kwargs Any parameters that should be passed to the object to be instantiated, default parameters. {} Examples: 1 2 3 4 5 6 7 8 9 10 11 12 class RD ( BaseEstimator , TransformerMixin ): def fit ( self , X , y , ** kwargs ): pass def fit_transform ( self , X , y = None , ** fit_params ): return self . transform ( X ) def transform ( self , X ): return X [:, : 3 ] trans = PipelineElement . create ( 'MyTransformer' , base_element = RD (), hyperparameters = {}) Source code in photonai/base/photon_elements.py @classmethod def create ( cls , name : str , base_element : BaseEstimator , hyperparameters : dict , test_disabled : bool = False , disabled : bool = False , ** kwargs ): \"\"\" Takes an instantiated object and encapsulates it into the PHOTONAI structure. Add the disabled function and attaches information about the hyperparameters that should be tested. Parameters: name: A string literal encoding the class to be instantiated. base_element: The underlying transformer or estimator class. hyperparameters: Which values/value range should be tested for the hyperparameter. In form of Dict: parameter_name -> HyperparameterElement. test_disabled: If the hyperparameter search should evaluate a complete disabling of the element. disabled: If true, the element is currently disabled and does nothing except return the data it received. **kwargs: Any parameters that should be passed to the object to be instantiated, default parameters. Example: ``` python class RD(BaseEstimator, TransformerMixin): def fit(self, X, y, **kwargs): pass def fit_transform(self, X, y=None, **fit_params): return self.transform(X) def transform(self, X): return X[:, :3] trans = PipelineElement.create('MyTransformer', base_element=RD(), hyperparameters={}) ``` \"\"\" if isinstance ( base_element , type ): raise ValueError ( \"Base element should be an instance but is a class.\" ) return PipelineElement ( name , hyperparameters , test_disabled , disabled , base_element = base_element , ** kwargs )","title":"create()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.fit","text":"Calls the fit function of the base element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls the fit function of the base element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Fitted self. \"\"\" if not self . disabled : obj = self . base_element arg_list = inspect . signature ( obj . fit ) if len ( arg_list . parameters ) > 2 : vals = arg_list . parameters . values () kwargs_param = list ( vals )[ - 1 ] if kwargs_param . kind == kwargs_param . VAR_KEYWORD : obj . fit ( X , y , ** kwargs ) return self obj . fit ( X , y ) return self","title":"fit()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.inverse_transform","text":"Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) (X, y, kwargs) in back-transformed version. Source code in photonai/base/photon_elements.py def inverse_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls inverse_transform on the base element. When the dimension is preserved: transformers without inverse returns original input. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Raises: NotImplementedError: Thrown when there is a dimensional reduction but no inverse is defined. Returns: (X, y, kwargs) in back-transformed version. \"\"\" if hasattr ( self . base_element , 'inverse_transform' ): # todo: check this X , y , kwargs = self . adjusted_delegate_call ( self . base_element . inverse_transform , X , y , ** kwargs ) elif self . is_transformer and self . reduce_dimension : msg = \" {} has no inverse_transform, but element reduce dimesions.\" . format ( self . name ) logger . error ( msg ) raise NotImplementedError ( msg ) return X , y , kwargs","title":"inverse_transform()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.predict","text":"Calls the predict function of the underlying base_element. Parameters: Name Type Description Default X ndarray The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_element.predict. {} Returns: Type Description ndarray Predictions values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function of the underlying base_element. Parameters: X: The array-like training and test data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_element.predict. Returns: Predictions values. \"\"\" if self . batch_size == 0 : return self . __predict ( X , ** kwargs ) else : return self . __batch_predict ( self . __predict , X , ** kwargs )","title":"predict()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.score","text":"Calls the score function on the base element. Parameters: Name Type Description Default X_test ndarray Input test data to score on. required y_test ndarray Input true targets to score on. required Returns: Type Description float A goodness of fit measure or a likelihood of unseen data. Source code in photonai/base/photon_elements.py def score ( self , X_test : np . ndarray , y_test : np . ndarray ) -> float : \"\"\" Calls the score function on the base element. Parameters: X_test: Input test data to score on. y_test: Input true targets to score on. Returns: A goodness of fit measure or a likelihood of unseen data. \"\"\" return self . base_element . score ( X_test , y_test )","title":"score()"},{"location":"api/base/pipeline_element/#photonai.base.photon_elements.PipelineElement.transform","text":"Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_element.transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) (X, y) in transformed version and original kwargs. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on the base element. In case there is no transform method, calls predict. This is used if we are using an estimator as a preprocessing step. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_element.transform. Returns: (X, y) in transformed version and original kwargs. \"\"\" if self . batch_size == 0 : Xt , yt , kwargs = self . __transform ( X , y , ** kwargs ) else : Xt , yt , kwargs = self . __batch_transform ( X , y , ** kwargs ) if all ( hasattr ( data , \"shape\" ) for data in [ X , Xt ]) and all ( len ( data . shape ) > 1 for data in [ X , Xt ]): self . reduce_dimension = ( Xt . shape [ 1 ] < X . shape [ 1 ]) return Xt , yt , kwargs","title":"transform()"},{"location":"api/base/preprocessing/","text":"Documentation for Preprocessing Special kind of Branch. If a Preprocessing pipe is added to a PHOTONAI Hyperpipe, all transformers are applied to the data ONCE BEFORE cross validation starts in order to prepare the data. Every added element should be a transformer PipelineElement. Examples: 1 2 3 pre_proc = Preprocessing () pre_proc += PipelineElement ( 'OneHotEncoder' , sparse = False ) my_pipe += pre_proc Some transformations should be performed bundled at the beginning. Here at the example of the OneHotEncoder. Due to the cross-validation split, some cateogries can no longer occur in any subsets. Therefore, a trained OneHotEncoding could fail on other subsets. By using the Preprocessing object, this effect can no longer appear. __iadd__ ( self , pipe_element ) special Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The transformer object to add. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The transformer object to add. \"\"\" if hasattr ( pipe_element , \"transform\" ): super ( Preprocessing , self ) . __iadd__ ( pipe_element ) if len ( pipe_element . hyperparameters ) > 0 : raise ValueError ( \"A preprocessing transformer must not have any hyperparameter \" \"because it is not part of the optimization and cross validation procedure\" ) else : raise ValueError ( \"Pipeline Element must have transform function\" ) return self __init__ ( self ) special Initialize the object. Source code in photonai/base/photon_elements.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super () . __init__ ( 'Preprocessing' ) self . has_hyperparameters = False self . needs_y = True self . needs_covariates = True self . _name = 'Preprocessing' self . is_transformer = True self . is_estimator = False","title":"Preprocessing"},{"location":"api/base/preprocessing/#documentation-for-preprocessing","text":"","title":"Documentation for Preprocessing"},{"location":"api/base/preprocessing/#photonai.base.photon_elements.Preprocessing","text":"Special kind of Branch. If a Preprocessing pipe is added to a PHOTONAI Hyperpipe, all transformers are applied to the data ONCE BEFORE cross validation starts in order to prepare the data. Every added element should be a transformer PipelineElement. Examples: 1 2 3 pre_proc = Preprocessing () pre_proc += PipelineElement ( 'OneHotEncoder' , sparse = False ) my_pipe += pre_proc Some transformations should be performed bundled at the beginning. Here at the example of the OneHotEncoder. Due to the cross-validation split, some cateogries can no longer occur in any subsets. Therefore, a trained OneHotEncoding could fail on other subsets. By using the Preprocessing object, this effect can no longer appear.","title":"photonai.base.photon_elements.Preprocessing"},{"location":"api/base/preprocessing/#photonai.base.photon_elements.Preprocessing.__iadd__","text":"Add an element to the sub pipeline. Parameters: Name Type Description Default pipe_element PipelineElement The transformer object to add. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipe_element : PipelineElement ): \"\"\" Add an element to the sub pipeline. Parameters: pipe_element: The transformer object to add. \"\"\" if hasattr ( pipe_element , \"transform\" ): super ( Preprocessing , self ) . __iadd__ ( pipe_element ) if len ( pipe_element . hyperparameters ) > 0 : raise ValueError ( \"A preprocessing transformer must not have any hyperparameter \" \"because it is not part of the optimization and cross validation procedure\" ) else : raise ValueError ( \"Pipeline Element must have transform function\" ) return self","title":"__iadd__()"},{"location":"api/base/preprocessing/#photonai.base.photon_elements.Preprocessing.__init__","text":"Initialize the object. Source code in photonai/base/photon_elements.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super () . __init__ ( 'Preprocessing' ) self . has_hyperparameters = False self . needs_y = True self . needs_covariates = True self . _name = 'Preprocessing' self . is_transformer = True self . is_estimator = False","title":"__init__()"},{"location":"api/base/registry/","text":"Documentation for PhotonRegistry Helper class to manage the PHOTONAI Element Register. Use it to add and remove items into the register. You can also retrieve information about items and its hyperparameters. Every item in the register is encoded by a string literal that points to a python class and its namespace. You can access the python class via the string literal. The class PhotonElement imports and instantiates the class for you. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import os from photonai.base import PhotonRegistry # REGISTER ELEMENT saved in folder custom_elements_folder base_folder = os . path . dirname ( os . path . abspath ( __file__ )) custom_elements_folder = os . path . join ( base_folder , 'custom_elements' ) registry = PhotonRegistry ( custom_elements_folder = custom_elements_folder ) registry . register ( photon_name = 'MyCustomEstimator' , class_str = 'custom_estimator.CustomEstimator' , element_type = 'Estimator' ) registry . activate () registry . info ( 'MyCustomEstimator' ) # get informations of other available elements registry . info ( 'SVC' ) __init__ ( self , custom_elements_folder = None ) special Initialize the object. Parameters: Name Type Description Default custom_elements_folder str Path to folder with custom element in it. None Source code in photonai/base/registry/registry.py def __init__ ( self , custom_elements_folder : str = None ): \"\"\" Initialize the object. Parameters: custom_elements_folder: Path to folder with custom element in it. \"\"\" self . current_folder = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) self . module_path = os . path . join ( self . current_folder , \"modules\" ) if not os . path . isdir ( self . module_path ): os . mkdir ( self . module_path ) # update list with available sub_elements self . _list_available_modules () PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = custom_elements_folder self . _load_custom_folder ( custom_elements_folder ) if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 or \\ PhotonRegistry . ELEMENT_DICTIONARY == PhotonRegistry . CUSTOM_ELEMENTS : PhotonRegistry . ELEMENT_DICTIONARY . update ( self . get_package_info ()) delete ( self , photon_name ) Delete Element from JSON file. Parameters: Name Type Description Default photon_name str The string literal encoding the class. required Source code in photonai/base/registry/registry.py def delete ( self , photon_name : str ): \"\"\" Delete Element from JSON file. Parameters: photon_name: The string literal encoding the class. \"\"\" if photon_name in PhotonRegistry . CUSTOM_ELEMENTS : del PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Removing the PipelineElement named \" {0} \" from CustomElements.json.' . format ( photon_name )) else : logger . info ( 'Cannot remove \" {0} \" from CustomElements.json. Element has not been registered before.' . format ( photon_name )) get_package_info ( self , photon_package = [ 'PhotonCore' ]) Collect all registered elements from JSON file. Parameters: Name Type Description Default photon_package list The names of the PHOTONAI submodules for which the elements should be retrieved. ['PhotonCore'] Returns: Type Description dict Dict of registered elements. Source code in photonai/base/registry/registry.py def get_package_info ( self , photon_package : list = PHOTON_REGISTRIES ) -> dict : \"\"\" Collect all registered elements from JSON file. Parameters: photon_package: The names of the PHOTONAI submodules for which the elements should be retrieved. Returns: Dict of registered elements. \"\"\" class_info = dict () for package in photon_package : content = self . _load_json ( package ) for idx , key in enumerate ( content ): class_path , class_name = os . path . splitext ( content [ key ][ 0 ]) if idx == 0 and package not in [ \"PhotonCore\" , \"CustomElements\" ]: # try to import something from module. # if that fails. drop this shit. try : imported_module = importlib . import_module ( class_path ) desired_class = getattr ( imported_module , class_name [ 1 :]) custom_element = desired_class () except ( AttributeError , ModuleNotFoundError ) as e : logger . error ( e ) logger . error ( \"Could not import from package {} . Deleting json.\" . format ( package )) self . delete_module ( package ) class_info [ key ] = class_path , class_name [ 1 :] return class_info info ( self , photon_name ) Show information for object that is encoded by this name. Parameters: Name Type Description Default photon_name str The string literal which accesses the class. required Source code in photonai/base/registry/registry.py def info ( self , photon_name : str ): \"\"\" Show information for object that is encoded by this name. Parameters: photon_name: The string literal which accesses the class. \"\"\" content = self . get_package_info () # load existing json if photon_name in content : element_namespace , element_name = content [ photon_name ] print ( \"----------------------------------\" ) print ( \"Name: \" + element_name ) print ( \"Namespace: \" + element_namespace ) print ( \"----------------------------------\" ) try : imported_module = __import__ ( element_namespace , globals (), locals (), element_name , 0 ) desired_class = getattr ( imported_module , element_name ) base_element = desired_class () print ( \"Possible Hyperparameters as derived from constructor:\" ) class_args = inspect . signature ( base_element . __init__ ) for item , more_info in class_args . parameters . items (): print ( \" {:<35} {:<75} \" . format ( item , str ( more_info ))) print ( \"----------------------------------\" ) except Exception as e : logger . error ( e ) logger . error ( \"Could not instantiate class \" + element_namespace + \".\" + element_name ) else : logger . error ( \"Could not find element \" + photon_name ) list_available_elements ( self , photon_package = [ 'PhotonCore' ]) Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: Name Type Description Default photon_package list The names of the PHOTON submodules for which the elements should be retrieved. ['PhotonCore'] Source code in photonai/base/registry/registry.py def list_available_elements ( self , photon_package : list = PHOTON_REGISTRIES ): \"\"\" Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: photon_package: The names of the PHOTON submodules for which the elements should be retrieved. \"\"\" if isinstance ( photon_package , str ): photon_package = [ photon_package ] for package in photon_package : content = self . _load_json ( package ) if len ( content ) > 0 : print ( ' \\n ' + package ) for k , v in sorted ( content . items ()): class_info , package_type = v print ( \" {:<35} {:<75} {:<5} \" . format ( k , class_info , package_type )) register ( self , photon_name , class_str , element_type ) Save element information to the JSON file. Parameters: Name Type Description Default photon_name str The string literal with which you want to access the class. required class_str str The namespace of the class, like in the import statement. required element_type str Can be 'Estimator' or 'Transformer' required Source code in photonai/base/registry/registry.py def register ( self , photon_name : str , class_str : str , element_type : str ): \"\"\" Save element information to the JSON file. Parameters: photon_name: The string literal with which you want to access the class. class_str: The namespace of the class, like in the import statement. element_type: Can be 'Estimator' or 'Transformer' \"\"\" # check if folder exists if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To register an element, specify a custom elements folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER)\" ) if not element_type == \"Estimator\" and not element_type == \"Transformer\" : raise ValueError ( \"Variable element_type must be 'Estimator' or 'Transformer'\" ) duplicate = self . _check_duplicate ( photon_name = photon_name , class_str = class_str , content = PhotonRegistry . CUSTOM_ELEMENTS ) if not duplicate : python_file = os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , class_str . split ( '.' )[ 0 ] + '.py' ) if not os . path . isfile ( python_file ): raise FileNotFoundError ( \"Couldn't find python file {} in your custom elements folder. \" \"Please copy your file into this folder first!\" . format ( python_file )) # add new element PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] = class_str , element_type # write back to file self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Adding PipelineElement ' + class_str + ' to CustomElements.json as \"' + photon_name + '\".' ) # activate custom elements self . activate () # check custom element logger . info ( \"Running tests on custom element...\" ) return self . _run_tests ( photon_name , element_type ) else : logger . error ( 'Could not register element!' )","title":"Registry"},{"location":"api/base/registry/#documentation-for-photonregistry","text":"","title":"Documentation for PhotonRegistry"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry","text":"Helper class to manage the PHOTONAI Element Register. Use it to add and remove items into the register. You can also retrieve information about items and its hyperparameters. Every item in the register is encoded by a string literal that points to a python class and its namespace. You can access the python class via the string literal. The class PhotonElement imports and instantiates the class for you. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import os from photonai.base import PhotonRegistry # REGISTER ELEMENT saved in folder custom_elements_folder base_folder = os . path . dirname ( os . path . abspath ( __file__ )) custom_elements_folder = os . path . join ( base_folder , 'custom_elements' ) registry = PhotonRegistry ( custom_elements_folder = custom_elements_folder ) registry . register ( photon_name = 'MyCustomEstimator' , class_str = 'custom_estimator.CustomEstimator' , element_type = 'Estimator' ) registry . activate () registry . info ( 'MyCustomEstimator' ) # get informations of other available elements registry . info ( 'SVC' )","title":"photonai.base.registry.registry.PhotonRegistry"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.__init__","text":"Initialize the object. Parameters: Name Type Description Default custom_elements_folder str Path to folder with custom element in it. None Source code in photonai/base/registry/registry.py def __init__ ( self , custom_elements_folder : str = None ): \"\"\" Initialize the object. Parameters: custom_elements_folder: Path to folder with custom element in it. \"\"\" self . current_folder = os . path . dirname ( os . path . abspath ( inspect . getfile ( inspect . currentframe ()))) self . module_path = os . path . join ( self . current_folder , \"modules\" ) if not os . path . isdir ( self . module_path ): os . mkdir ( self . module_path ) # update list with available sub_elements self . _list_available_modules () PhotonRegistry . CUSTOM_ELEMENTS_FOLDER = custom_elements_folder self . _load_custom_folder ( custom_elements_folder ) if len ( PhotonRegistry . ELEMENT_DICTIONARY ) == 0 or \\ PhotonRegistry . ELEMENT_DICTIONARY == PhotonRegistry . CUSTOM_ELEMENTS : PhotonRegistry . ELEMENT_DICTIONARY . update ( self . get_package_info ())","title":"__init__()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.delete","text":"Delete Element from JSON file. Parameters: Name Type Description Default photon_name str The string literal encoding the class. required Source code in photonai/base/registry/registry.py def delete ( self , photon_name : str ): \"\"\" Delete Element from JSON file. Parameters: photon_name: The string literal encoding the class. \"\"\" if photon_name in PhotonRegistry . CUSTOM_ELEMENTS : del PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Removing the PipelineElement named \" {0} \" from CustomElements.json.' . format ( photon_name )) else : logger . info ( 'Cannot remove \" {0} \" from CustomElements.json. Element has not been registered before.' . format ( photon_name ))","title":"delete()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.get_package_info","text":"Collect all registered elements from JSON file. Parameters: Name Type Description Default photon_package list The names of the PHOTONAI submodules for which the elements should be retrieved. ['PhotonCore'] Returns: Type Description dict Dict of registered elements. Source code in photonai/base/registry/registry.py def get_package_info ( self , photon_package : list = PHOTON_REGISTRIES ) -> dict : \"\"\" Collect all registered elements from JSON file. Parameters: photon_package: The names of the PHOTONAI submodules for which the elements should be retrieved. Returns: Dict of registered elements. \"\"\" class_info = dict () for package in photon_package : content = self . _load_json ( package ) for idx , key in enumerate ( content ): class_path , class_name = os . path . splitext ( content [ key ][ 0 ]) if idx == 0 and package not in [ \"PhotonCore\" , \"CustomElements\" ]: # try to import something from module. # if that fails. drop this shit. try : imported_module = importlib . import_module ( class_path ) desired_class = getattr ( imported_module , class_name [ 1 :]) custom_element = desired_class () except ( AttributeError , ModuleNotFoundError ) as e : logger . error ( e ) logger . error ( \"Could not import from package {} . Deleting json.\" . format ( package )) self . delete_module ( package ) class_info [ key ] = class_path , class_name [ 1 :] return class_info","title":"get_package_info()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.info","text":"Show information for object that is encoded by this name. Parameters: Name Type Description Default photon_name str The string literal which accesses the class. required Source code in photonai/base/registry/registry.py def info ( self , photon_name : str ): \"\"\" Show information for object that is encoded by this name. Parameters: photon_name: The string literal which accesses the class. \"\"\" content = self . get_package_info () # load existing json if photon_name in content : element_namespace , element_name = content [ photon_name ] print ( \"----------------------------------\" ) print ( \"Name: \" + element_name ) print ( \"Namespace: \" + element_namespace ) print ( \"----------------------------------\" ) try : imported_module = __import__ ( element_namespace , globals (), locals (), element_name , 0 ) desired_class = getattr ( imported_module , element_name ) base_element = desired_class () print ( \"Possible Hyperparameters as derived from constructor:\" ) class_args = inspect . signature ( base_element . __init__ ) for item , more_info in class_args . parameters . items (): print ( \" {:<35} {:<75} \" . format ( item , str ( more_info ))) print ( \"----------------------------------\" ) except Exception as e : logger . error ( e ) logger . error ( \"Could not instantiate class \" + element_namespace + \".\" + element_name ) else : logger . error ( \"Could not find element \" + photon_name )","title":"info()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.list_available_elements","text":"Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: Name Type Description Default photon_package list The names of the PHOTON submodules for which the elements should be retrieved. ['PhotonCore'] Source code in photonai/base/registry/registry.py def list_available_elements ( self , photon_package : list = PHOTON_REGISTRIES ): \"\"\" Print info about all items that are registered for the PHOTONAI submodule to the console. Parameters: photon_package: The names of the PHOTON submodules for which the elements should be retrieved. \"\"\" if isinstance ( photon_package , str ): photon_package = [ photon_package ] for package in photon_package : content = self . _load_json ( package ) if len ( content ) > 0 : print ( ' \\n ' + package ) for k , v in sorted ( content . items ()): class_info , package_type = v print ( \" {:<35} {:<75} {:<5} \" . format ( k , class_info , package_type ))","title":"list_available_elements()"},{"location":"api/base/registry/#photonai.base.registry.registry.PhotonRegistry.register","text":"Save element information to the JSON file. Parameters: Name Type Description Default photon_name str The string literal with which you want to access the class. required class_str str The namespace of the class, like in the import statement. required element_type str Can be 'Estimator' or 'Transformer' required Source code in photonai/base/registry/registry.py def register ( self , photon_name : str , class_str : str , element_type : str ): \"\"\" Save element information to the JSON file. Parameters: photon_name: The string literal with which you want to access the class. class_str: The namespace of the class, like in the import statement. element_type: Can be 'Estimator' or 'Transformer' \"\"\" # check if folder exists if not PhotonRegistry . CUSTOM_ELEMENTS_FOLDER : raise ValueError ( \"To register an element, specify a custom elements folder when instantiating the registry \" \"module. Example: registry = PhotonRegistry('/MY/CUSTOM/ELEMENTS/FOLDER)\" ) if not element_type == \"Estimator\" and not element_type == \"Transformer\" : raise ValueError ( \"Variable element_type must be 'Estimator' or 'Transformer'\" ) duplicate = self . _check_duplicate ( photon_name = photon_name , class_str = class_str , content = PhotonRegistry . CUSTOM_ELEMENTS ) if not duplicate : python_file = os . path . join ( PhotonRegistry . CUSTOM_ELEMENTS_FOLDER , class_str . split ( '.' )[ 0 ] + '.py' ) if not os . path . isfile ( python_file ): raise FileNotFoundError ( \"Couldn't find python file {} in your custom elements folder. \" \"Please copy your file into this folder first!\" . format ( python_file )) # add new element PhotonRegistry . CUSTOM_ELEMENTS [ photon_name ] = class_str , element_type # write back to file self . _write_to_json ( PhotonRegistry . CUSTOM_ELEMENTS ) logger . info ( 'Adding PipelineElement ' + class_str + ' to CustomElements.json as \"' + photon_name + '\".' ) # activate custom elements self . activate () # check custom element logger . info ( \"Running tests on custom element...\" ) return self . _run_tests ( photon_name , element_type ) else : logger . error ( 'Could not register element!' )","title":"register()"},{"location":"api/base/stack/","text":"Documentation for Stack Creates a vertical stacking/parallelization of pipeline items. The object acts as a single PipelineElement and encapsulates several vertically stacked other PipelineElements, each child receiving the same input data. The data is iteratively distributed to all children, the results are collected and horizontally concatenated. Examples: 1 2 3 4 tree = PipelineElement ( 'DecisionTreeClassifier' ) svc = PipelineElement ( 'LinearSVC' ) my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True ) __iadd__ ( self , item ) special Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . check_if_needs_y ( item ) super ( Stack , self ) . __iadd__ ( item ) # for each configuration tmp_dict = dict ( item . hyperparameters ) for key , element in tmp_dict . items (): self . _hyperparameters [ self . name + '__' + key ] = tmp_dict [ key ] return self __init__ ( self , name , elements = None , use_probabilities = False ) special Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: Name Type Description Default name str Give the pipeline element a name. required elements List[photonai.base.photon_elements.PipelineElement] List of pipeline elements that should run in parallel. None use_probabilities bool For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. False Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , use_probabilities : bool = False ): \"\"\" Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: name: Give the pipeline element a name. elements: List of pipeline elements that should run in parallel. use_probabilities: For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. \"\"\" super ( Stack , self ) . __init__ ( name , hyperparameters = {}, test_disabled = False , disabled = False , base_element = True ) self . _hyperparameters = {} self . elements = list () if elements is not None : for item_to_stack in elements : self . __iadd__ ( item_to_stack ) # todo: Stack should not be allowed to change y, only covariates self . needs_y = False self . needs_covariates = True self . identifier = \"STACK:\" self . use_probabilities = use_probabilities add ( self , item ) Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def add ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . __iadd__ ( item ) fit ( self , X , y = None , ** kwargs ) Calls fit iteratively on every child. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls fit iteratively on every child. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" for element in self . elements : # Todo: parallellize fitting element . fit ( X , y , ** kwargs ) return self predict ( self , X , ** kwargs ) Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_elements predict. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict. Returns: Prediction values. \"\"\" if not self . use_probabilities : return self . _predict ( X , ** kwargs ) else : return self . predict_proba ( X , ** kwargs ) predict_proba ( self , X , y = None , ** kwargs ) Predict probabilities for every pipe element and stack them together. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, not used yet. {} Returns: Type Description ndarray Probability values. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities for every pipe element and stack them together. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, not used yet. Returns: Probability values. \"\"\" predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict_proba ( X ) if element_transform is None : element_transform = element . predict ( X ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data transform ( self , X , y = None , ** kwargs ) Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: Name Type Description Default X ndarray The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) Prediction values. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: X: The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements transform. Returns: Prediction values. \"\"\" transformed_data = np . array ([]) for element in self . elements : # if it is a hyperpipe with a final estimator, we want to use predict: element_transform , _ , _ = element . transform ( X , y , ** kwargs ) transformed_data = PhotonDataHelper . stack_data_horizontally ( transformed_data , element_transform ) return transformed_data , y , kwargs","title":"Stack"},{"location":"api/base/stack/#documentation-for-stack","text":"","title":"Documentation for Stack"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack","text":"Creates a vertical stacking/parallelization of pipeline items. The object acts as a single PipelineElement and encapsulates several vertically stacked other PipelineElements, each child receiving the same input data. The data is iteratively distributed to all children, the results are collected and horizontally concatenated. Examples: 1 2 3 4 tree = PipelineElement ( 'DecisionTreeClassifier' ) svc = PipelineElement ( 'LinearSVC' ) my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True )","title":"photonai.base.photon_elements.Stack"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.__iadd__","text":"Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . check_if_needs_y ( item ) super ( Stack , self ) . __iadd__ ( item ) # for each configuration tmp_dict = dict ( item . hyperparameters ) for key , element in tmp_dict . items (): self . _hyperparameters [ self . name + '__' + key ] = tmp_dict [ key ] return self","title":"__iadd__()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.__init__","text":"Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: Name Type Description Default name str Give the pipeline element a name. required elements List[photonai.base.photon_elements.PipelineElement] List of pipeline elements that should run in parallel. None use_probabilities bool For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. False Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , use_probabilities : bool = False ): \"\"\" Creates a new Stack element. Collects all possible hyperparameter combinations of the children. Parameters: name: Give the pipeline element a name. elements: List of pipeline elements that should run in parallel. use_probabilities: For a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators. In case only some implement predict_proba, predict is called for the remaining estimators. \"\"\" super ( Stack , self ) . __init__ ( name , hyperparameters = {}, test_disabled = False , disabled = False , base_element = True ) self . _hyperparameters = {} self . elements = list () if elements is not None : for item_to_stack in elements : self . __iadd__ ( item_to_stack ) # todo: Stack should not be allowed to change y, only covariates self . needs_y = False self . needs_covariates = True self . identifier = \"STACK:\" self . use_probabilities = use_probabilities","title":"__init__()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.add","text":"Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: Name Type Description Default item PipelineElement The Element that should be stacked and will run in a vertical parallelization in the original pipe. required Source code in photonai/base/photon_elements.py def add ( self , item : PipelineElement ): \"\"\" Add a new element to the stack. Generate sklearn hyperparameter names in order to set the item's hyperparameters in the optimization process. Parameters: item: The Element that should be stacked and will run in a vertical parallelization in the original pipe. \"\"\" self . __iadd__ ( item )","title":"add()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.fit","text":"Calls fit iteratively on every child. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements fit. {} Returns: Type Description Fitted self. Source code in photonai/base/photon_elements.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Calls fit iteratively on every child. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements fit. Returns: Fitted self. \"\"\" for element in self . elements : # Todo: parallellize fitting element . fit ( X , y , ** kwargs ) return self","title":"fit()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.predict","text":"Calls the predict function on underlying base elements. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, passed to base_elements predict. {} Returns: Type Description ndarray Prediction values. Source code in photonai/base/photon_elements.py def predict ( self , X : np . ndarray , ** kwargs ) -> np . ndarray : \"\"\" Calls the predict function on underlying base elements. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, passed to base_elements predict. Returns: Prediction values. \"\"\" if not self . use_probabilities : return self . _predict ( X , ** kwargs ) else : return self . predict_proba ( X , ** kwargs )","title":"predict()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.predict_proba","text":"Predict probabilities for every pipe element and stack them together. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, not used yet. {} Returns: Type Description ndarray Probability values. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> np . ndarray : \"\"\" Predict probabilities for every pipe element and stack them together. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, not used yet. Returns: Probability values. \"\"\" predicted_data = np . array ([]) for element in self . elements : element_transform = element . predict_proba ( X ) if element_transform is None : element_transform = element . predict ( X ) predicted_data = PhotonDataHelper . stack_data_horizontally ( predicted_data , element_transform ) return predicted_data","title":"predict_proba()"},{"location":"api/base/stack/#photonai.base.photon_elements.Stack.transform","text":"Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: Name Type Description Default X ndarray The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. required y ndarray The truth array-like values with shape=[N], where N is the number of samples. None **kwargs Keyword arguments, passed to base_elements transform. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, <class 'dict'>) Prediction values. Source code in photonai/base/photon_elements.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray , dict ): \"\"\" Calls transform on every child. If the encapsulated child is a hyperpipe, also calls predict on the last element in the pipeline. Parameters: X: The array-liketraining with shape=[N, D] and test data, where N is the number of samples and D is the number of features. y: The truth array-like values with shape=[N], where N is the number of samples. **kwargs: Keyword arguments, passed to base_elements transform. Returns: Prediction values. \"\"\" transformed_data = np . array ([]) for element in self . elements : # if it is a hyperpipe with a final estimator, we want to use predict: element_transform , _ , _ = element . transform ( X , y , ** kwargs ) transformed_data = PhotonDataHelper . stack_data_horizontally ( transformed_data , element_transform ) return transformed_data , y , kwargs","title":"transform()"},{"location":"api/base/switch/","text":"Documentation for Switch This class encapsulates several PipelineElements that belong at the same step of the pipeline, competing for being the best choice. If for example you want to find out if Preprocessing A or Preprocessing B is better at this position in the pipe. Or you want to test if a random forest outperforms the good old SVM. ATTENTION: This class is a construct that may be convenient but is not suitable for any complex optimizations. Currently optimization works for grid_search, random search and smac and the specializes switch optimizer. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 from photonai.base import PipelineElement , Switch from photonai.optimization import IntegerRange # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ]) __iadd__ ( self , pipeline_element ) special Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" super ( Switch , self ) . __iadd__ ( pipeline_element ) self . elements_dict [ pipeline_element . name ] = pipeline_element self . generate_private_config_grid () return self __init__ ( self , name , elements = None , estimator_name = '' ) special Creates a new Switch object and generated the hyperparameter combination grid. Parameters: Name Type Description Default name str How the element is called in the pipeline. required elements List[photonai.base.photon_elements.PipelineElement] The competing pipeline elements. None estimator_name str - '' Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , estimator_name : str = '' ): \"\"\" Creates a new Switch object and generated the hyperparameter combination grid. Parameters: name: How the element is called in the pipeline. elements: The competing pipeline elements. estimator_name: - \"\"\" self . _name = name self . initial_name = self . _name self . sklearn_name = self . name + \"__current_element\" self . _hyperparameters = {} self . _current_element = ( 1 , 1 ) self . pipeline_element_configurations = [] self . base_element = None self . disabled = False self . test_disabled = False self . batch_size = 0 self . estimator_name = estimator_name self . needs_y = True self . needs_covariates = True # we assume we test models against each other, but only guessing self . is_estimator = True self . is_transformer = True self . identifier = \"SWITCH:\" self . _random_state = False self . elements_dict = {} if elements : self . elements = elements self . generate_private_config_grid () for p_element in elements : self . elements_dict [ p_element . name ] = p_element else : self . elements = [] add ( self , pipeline_element ) Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def add ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" self . __iadd__ ( pipeline_element ) predict_proba ( self , X , ** kwargs ) Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, not in use yet. {} Returns: Type Description Optional[numpy.ndarray] Probabilities. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , ** kwargs ) -> Union [ np . ndarray , None ]: \"\"\" Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, not in use yet. Returns: Probabilities. \"\"\" if not self . disabled : if hasattr ( self . base_element . base_element , 'predict_proba' ): return self . base_element . predict_proba ( X ) else : return None return X","title":"Switch"},{"location":"api/base/switch/#documentation-for-switch","text":"","title":"Documentation for Switch"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch","text":"This class encapsulates several PipelineElements that belong at the same step of the pipeline, competing for being the best choice. If for example you want to find out if Preprocessing A or Preprocessing B is better at this position in the pipe. Or you want to test if a random forest outperforms the good old SVM. ATTENTION: This class is a construct that may be convenient but is not suitable for any complex optimizations. Currently optimization works for grid_search, random search and smac and the specializes switch optimizer. Examples: 1 2 3 4 5 6 7 8 9 10 11 12 from photonai.base import PipelineElement , Switch from photonai.optimization import IntegerRange # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ])","title":"photonai.base.photon_elements.Switch"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.__iadd__","text":"Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def __iadd__ ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" super ( Switch , self ) . __iadd__ ( pipeline_element ) self . elements_dict [ pipeline_element . name ] = pipeline_element self . generate_private_config_grid () return self","title":"__iadd__()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.__init__","text":"Creates a new Switch object and generated the hyperparameter combination grid. Parameters: Name Type Description Default name str How the element is called in the pipeline. required elements List[photonai.base.photon_elements.PipelineElement] The competing pipeline elements. None estimator_name str - '' Source code in photonai/base/photon_elements.py def __init__ ( self , name : str , elements : List [ PipelineElement ] = None , estimator_name : str = '' ): \"\"\" Creates a new Switch object and generated the hyperparameter combination grid. Parameters: name: How the element is called in the pipeline. elements: The competing pipeline elements. estimator_name: - \"\"\" self . _name = name self . initial_name = self . _name self . sklearn_name = self . name + \"__current_element\" self . _hyperparameters = {} self . _current_element = ( 1 , 1 ) self . pipeline_element_configurations = [] self . base_element = None self . disabled = False self . test_disabled = False self . batch_size = 0 self . estimator_name = estimator_name self . needs_y = True self . needs_covariates = True # we assume we test models against each other, but only guessing self . is_estimator = True self . is_transformer = True self . identifier = \"SWITCH:\" self . _random_state = False self . elements_dict = {} if elements : self . elements = elements self . generate_private_config_grid () for p_element in elements : self . elements_dict [ p_element . name ] = p_element else : self . elements = []","title":"__init__()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.add","text":"Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: Name Type Description Default pipeline_element PipelineElement Item that should be tested against other competing elements at that position in the pipeline. required Source code in photonai/base/photon_elements.py def add ( self , pipeline_element : PipelineElement ): \"\"\" Add a new estimator or transformer object to the switch container. All items change their positions during testing. Parameters: pipeline_element: Item that should be tested against other competing elements at that position in the pipeline. \"\"\" self . __iadd__ ( pipeline_element )","title":"add()"},{"location":"api/base/switch/#photonai.base.photon_elements.Switch.predict_proba","text":"Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: Name Type Description Default X ndarray The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. required **kwargs Keyword arguments, not in use yet. {} Returns: Type Description Optional[numpy.ndarray] Probabilities. Source code in photonai/base/photon_elements.py def predict_proba ( self , X : np . ndarray , ** kwargs ) -> Union [ np . ndarray , None ]: \"\"\" Predict probabilities. Base element needs predict_proba() function, otherwise return None. Parameters: X: The array-like data with shape=[N, D], where N is the number of samples and D is the number of features. **kwargs: Keyword arguments, not in use yet. Returns: Probabilities. \"\"\" if not self . disabled : if hasattr ( self . base_element . base_element , 'predict_proba' ): return self . base_element . predict_proba ( X ) else : return None return X","title":"predict_proba()"},{"location":"api/modelwrapper/base_model_wrapper/","text":"Documentation for BaseModelWrapper The PHOTONAI interface for implementing custom pipeline elements. PHOTONAI works on top of the scikit-learn object API, see documentation . Your class should overwrite the following definitions: fit(data) : learn or adjust to the data. If it is an estimator, which means it has the ability to learn, it should implement predict(data) : using the learned model to generate prediction, should inherit sklearn.base.BaseEstimator ( see here ), inherits get_params and set_params . If it is an transformer, which means it preprocesses or prepares the data, it should implement transform(data) : applying the logic to the data to transform it, should inherit from sklearn.base.TransformerMixin ( see here ), inherits fit_transform as a concatenation of both fit and transform, should inherit sklearn.base.BaseEstimator ( see here ) inherits get_params and set_params . Prepare for hyperparameter optimization PHOTONAI expects a definition for all parameters you want to optimize in the hyperparameter search in the constructor stub , and to be addressable with the same name as class variable . In this way you can define any parameter and it is automatically prepared for the hyperparameter search process. See the scikit-learn object API documentation for more in depth information about the interface. fit ( self , data , targets = None ) Adjust the underlying model or method to the data. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required targets ndarray The input targets of shape [n_samples, 1]. None Returns: Type Description IMPORTANT, must return self! Source code in photonai/modelwrapper/base_model_wrapper.py def fit ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Adjust the underlying model or method to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Returns: IMPORTANT, must return self! \"\"\" get_params ( self , deep = True ) Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: Name Type Description Default deep bool If True, will return the parameters for this estimator and contained subobjects that are estimators. True Returns: Type Description dict Parameter names mapped to their values. Source code in photonai/modelwrapper/base_model_wrapper.py def get_params ( self , deep : bool = True ) -> dict : \"\"\" Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: deep: If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: Parameter names mapped to their values. \"\"\" return super ( BaseModelWrapper , self ) . get_params ( deep = deep ) predict ( self , data ) Use the learned model to make predictions. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required Source code in photonai/modelwrapper/base_model_wrapper.py def predict ( self , data : np . ndarray ): \"\"\" Use the learned model to make predictions. Parameters: data: The input samples of shape [n_samples, n_original_features]. \"\"\" set_params ( self , ** kwargs ) Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: Name Type Description Default **kwargs Estimator parameters. {} Source code in photonai/modelwrapper/base_model_wrapper.py def set_params ( self , ** kwargs ): \"\"\" Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: **kwargs: Estimator parameters. \"\"\" super ( BaseModelWrapper , self ) . set_params ( ** kwargs ) transform ( self , data , targets = None ) Apply the method's logic to the data. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required targets ndarray The input targets of shape [n_samples, 1]. Not necessary. None Source code in photonai/modelwrapper/base_model_wrapper.py def transform ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Apply the method's logic to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Not necessary. \"\"\"","title":"BaseModelWrapper"},{"location":"api/modelwrapper/base_model_wrapper/#documentation-for-basemodelwrapper","text":"","title":"Documentation for BaseModelWrapper"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper","text":"The PHOTONAI interface for implementing custom pipeline elements. PHOTONAI works on top of the scikit-learn object API, see documentation . Your class should overwrite the following definitions: fit(data) : learn or adjust to the data. If it is an estimator, which means it has the ability to learn, it should implement predict(data) : using the learned model to generate prediction, should inherit sklearn.base.BaseEstimator ( see here ), inherits get_params and set_params . If it is an transformer, which means it preprocesses or prepares the data, it should implement transform(data) : applying the logic to the data to transform it, should inherit from sklearn.base.TransformerMixin ( see here ), inherits fit_transform as a concatenation of both fit and transform, should inherit sklearn.base.BaseEstimator ( see here ) inherits get_params and set_params . Prepare for hyperparameter optimization PHOTONAI expects a definition for all parameters you want to optimize in the hyperparameter search in the constructor stub , and to be addressable with the same name as class variable . In this way you can define any parameter and it is automatically prepared for the hyperparameter search process. See the scikit-learn object API documentation for more in depth information about the interface.","title":"photonai.modelwrapper.base_model_wrapper.BaseModelWrapper"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.fit","text":"Adjust the underlying model or method to the data. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required targets ndarray The input targets of shape [n_samples, 1]. None Returns: Type Description IMPORTANT, must return self! Source code in photonai/modelwrapper/base_model_wrapper.py def fit ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Adjust the underlying model or method to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Returns: IMPORTANT, must return self! \"\"\"","title":"fit()"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.get_params","text":"Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: Name Type Description Default deep bool If True, will return the parameters for this estimator and contained subobjects that are estimators. True Returns: Type Description dict Parameter names mapped to their values. Source code in photonai/modelwrapper/base_model_wrapper.py def get_params ( self , deep : bool = True ) -> dict : \"\"\" Get the models parameters. Automatically implemented when inheriting from sklearn.base.BaseEstimator Parameters: deep: If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: Parameter names mapped to their values. \"\"\" return super ( BaseModelWrapper , self ) . get_params ( deep = deep )","title":"get_params()"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.predict","text":"Use the learned model to make predictions. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required Source code in photonai/modelwrapper/base_model_wrapper.py def predict ( self , data : np . ndarray ): \"\"\" Use the learned model to make predictions. Parameters: data: The input samples of shape [n_samples, n_original_features]. \"\"\"","title":"predict()"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.set_params","text":"Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: Name Type Description Default **kwargs Estimator parameters. {} Source code in photonai/modelwrapper/base_model_wrapper.py def set_params ( self , ** kwargs ): \"\"\" Takes the given dictionary, with the keys being the variable name, and sets the object's parameters to the given values. Automatically implemented when inheriting from sklearn.base.BaseEstimator. Parameters: **kwargs: Estimator parameters. \"\"\" super ( BaseModelWrapper , self ) . set_params ( ** kwargs )","title":"set_params()"},{"location":"api/modelwrapper/base_model_wrapper/#photonai.modelwrapper.base_model_wrapper.BaseModelWrapper.transform","text":"Apply the method's logic to the data. Parameters: Name Type Description Default data ndarray The input samples of shape [n_samples, n_original_features]. required targets ndarray The input targets of shape [n_samples, 1]. Not necessary. None Source code in photonai/modelwrapper/base_model_wrapper.py def transform ( self , data : np . ndarray , targets : np . ndarray = None ): \"\"\" Apply the method's logic to the data. Parameters: data: The input samples of shape [n_samples, n_original_features]. targets: The input targets of shape [n_samples, 1]. Not necessary. \"\"\"","title":"transform()"},{"location":"api/modelwrapper/imblearn/","text":"Documentation for ImbalancedDataTransformer Applies the chosen strategy to the data in order to balance the input data. Instantiates the strategy filter object according to the name given as string literal. Underlying architecture: Imbalanced-Learning. More information on their documentation . Examples: 1 2 3 4 5 6 7 from photonai.optimization import Categorical tested_methods = Categorical ([ 'RandomOverSampler' , 'SMOTEENN' , 'SVMSMOTE' , 'BorderlineSMOTE' , 'SMOTE' , 'ClusterCentroids' ]) PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : tested_methods }, test_disabled = True ) __init__ ( self , method_name = 'RandomUnderSampler' , ** kwargs ) special Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: Name Type Description Default method_name str Imbalanced learning strategy. Possible values with an oversampling strategy are: ADASYN, BorderlineSMOTE, KMeansSMOTE, RandomOverSampler, SMOTE, SMOTENC, SVMSMOTE, an undersampling strategy are: ClusterCentroids, RandomUnderSampler, NearMiss, InstanceHardnessThreshold, CondensedNearestNeighbour, EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN, NeighbourhoodCleaningRule, OneSidedSelection, a combined strategy are: SMOTEENN, SMOTETomek. 'RandomUnderSampler' **kwargs Any parameters to pass to the imbalance strategy object. {} Source code in photonai/modelwrapper/imbalanced_data_transformer.py def __init__ ( self , method_name : str = 'RandomUnderSampler' , ** kwargs ): \"\"\" Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: method_name: Imbalanced learning strategy. Possible values with - an oversampling strategy are: - ADASYN, - BorderlineSMOTE, - KMeansSMOTE, - RandomOverSampler, - SMOTE, - SMOTENC, - SVMSMOTE, - an undersampling strategy are: - ClusterCentroids, - RandomUnderSampler, - NearMiss, - InstanceHardnessThreshold, - CondensedNearestNeighbour, - EditedNearestNeighbours, - RepeatedEditedNearestNeighbours, - AllKNN, - NeighbourhoodCleaningRule, - OneSidedSelection, - a combined strategy are: - SMOTEENN, - SMOTETomek. **kwargs: Any parameters to pass to the imbalance strategy object. \"\"\" if not __found__ : raise ModuleNotFoundError ( \"Module imblearn not found or not installed as expected. \" \"Please install the requirements.txt in PHOTON main folder.\" ) self . method_name = method_name self . needs_y = True imbalance_type = '' for group , possible_strategies in ImbalancedDataTransformer . IMBALANCED_DICT . items (): if self . method_name in possible_strategies : imbalance_type = group if imbalance_type == \"oversampling\" : home = over_sampling elif imbalance_type == \"undersampling\" : home = under_sampling elif imbalance_type == \"combine\" or imbalance_type == \"combination\" : home = combine else : msg = \"Imbalance Type not found. Can be oversampling, undersampling or combine. \" \\ \"Oversampling: method_name one of {} . Undersampling: method_name one of {} .\" \\ \"Combine: method_name one of {} .\" . format ( str ( self . IMBALANCED_DICT [ \"oversampling\" ]), str ( self . IMBALANCED_DICT [ \"undersampling\" ]), str ( self . IMBALANCED_DICT [ \"combine\" ])) logger . error ( msg ) raise ValueError ( msg ) desired_class = getattr ( home , method_name ) self . method = desired_class ( ** kwargs ) fit_transform ( self , X , y = None , ** kwargs ) Call of the underlying imblearn.fit_resample(X, y). Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Transformed data. Source code in photonai/modelwrapper/imbalanced_data_transformer.py def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying imblearn.fit_resample(X, y). Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . method . fit_resample ( X , y ) transform ( self , X , y = None , ** kwargs ) Forwarding to the self.fit_transform method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Transformed data. Source code in photonai/modelwrapper/imbalanced_data_transformer.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Forwarding to the self.fit_transform method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . fit_transform ( X , y )","title":"ImbalancedDataTransformer"},{"location":"api/modelwrapper/imblearn/#documentation-for-imbalanceddatatransformer","text":"","title":"Documentation for ImbalancedDataTransformer"},{"location":"api/modelwrapper/imblearn/#photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer","text":"Applies the chosen strategy to the data in order to balance the input data. Instantiates the strategy filter object according to the name given as string literal. Underlying architecture: Imbalanced-Learning. More information on their documentation . Examples: 1 2 3 4 5 6 7 from photonai.optimization import Categorical tested_methods = Categorical ([ 'RandomOverSampler' , 'SMOTEENN' , 'SVMSMOTE' , 'BorderlineSMOTE' , 'SMOTE' , 'ClusterCentroids' ]) PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : tested_methods }, test_disabled = True )","title":"photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer"},{"location":"api/modelwrapper/imblearn/#photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer.__init__","text":"Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: Name Type Description Default method_name str Imbalanced learning strategy. Possible values with an oversampling strategy are: ADASYN, BorderlineSMOTE, KMeansSMOTE, RandomOverSampler, SMOTE, SMOTENC, SVMSMOTE, an undersampling strategy are: ClusterCentroids, RandomUnderSampler, NearMiss, InstanceHardnessThreshold, CondensedNearestNeighbour, EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN, NeighbourhoodCleaningRule, OneSidedSelection, a combined strategy are: SMOTEENN, SMOTETomek. 'RandomUnderSampler' **kwargs Any parameters to pass to the imbalance strategy object. {} Source code in photonai/modelwrapper/imbalanced_data_transformer.py def __init__ ( self , method_name : str = 'RandomUnderSampler' , ** kwargs ): \"\"\" Instantiates an object that transforms the data into balanced groups according to the given method. Parameters: method_name: Imbalanced learning strategy. Possible values with - an oversampling strategy are: - ADASYN, - BorderlineSMOTE, - KMeansSMOTE, - RandomOverSampler, - SMOTE, - SMOTENC, - SVMSMOTE, - an undersampling strategy are: - ClusterCentroids, - RandomUnderSampler, - NearMiss, - InstanceHardnessThreshold, - CondensedNearestNeighbour, - EditedNearestNeighbours, - RepeatedEditedNearestNeighbours, - AllKNN, - NeighbourhoodCleaningRule, - OneSidedSelection, - a combined strategy are: - SMOTEENN, - SMOTETomek. **kwargs: Any parameters to pass to the imbalance strategy object. \"\"\" if not __found__ : raise ModuleNotFoundError ( \"Module imblearn not found or not installed as expected. \" \"Please install the requirements.txt in PHOTON main folder.\" ) self . method_name = method_name self . needs_y = True imbalance_type = '' for group , possible_strategies in ImbalancedDataTransformer . IMBALANCED_DICT . items (): if self . method_name in possible_strategies : imbalance_type = group if imbalance_type == \"oversampling\" : home = over_sampling elif imbalance_type == \"undersampling\" : home = under_sampling elif imbalance_type == \"combine\" or imbalance_type == \"combination\" : home = combine else : msg = \"Imbalance Type not found. Can be oversampling, undersampling or combine. \" \\ \"Oversampling: method_name one of {} . Undersampling: method_name one of {} .\" \\ \"Combine: method_name one of {} .\" . format ( str ( self . IMBALANCED_DICT [ \"oversampling\" ]), str ( self . IMBALANCED_DICT [ \"undersampling\" ]), str ( self . IMBALANCED_DICT [ \"combine\" ])) logger . error ( msg ) raise ValueError ( msg ) desired_class = getattr ( home , method_name ) self . method = desired_class ( ** kwargs )","title":"__init__()"},{"location":"api/modelwrapper/imblearn/#photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer.fit_transform","text":"Call of the underlying imblearn.fit_resample(X, y). Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Transformed data. Source code in photonai/modelwrapper/imbalanced_data_transformer.py def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying imblearn.fit_resample(X, y). Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . method . fit_resample ( X , y )","title":"fit_transform()"},{"location":"api/modelwrapper/imblearn/#photonai.modelwrapper.imbalanced_data_transformer.ImbalancedDataTransformer.transform","text":"Forwarding to the self.fit_transform method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Transformed data. Source code in photonai/modelwrapper/imbalanced_data_transformer.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Forwarding to the self.fit_transform method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Transformed data. \"\"\" return self . fit_transform ( X , y )","title":"transform()"},{"location":"api/modelwrapper/label_encoder/","text":"Documentation for LabelEncoder Suitable version of the scikit-learn LabelEncoder for PHOTONAI. Since the pipeline process streams the underlying samples to every transformer, this class is required. __init__ ( self ) special Initialize the object. Source code in photonai/modelwrapper/label_encoder.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( LabelEncoder , self ) . __init__ () self . needs_y = True fit ( self , X , y = None , ** kwargs ) Call of the underlying sklearn.fit(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Source code in photonai/modelwrapper/label_encoder.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Call of the underlying sklearn.fit(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. \"\"\" super ( LabelEncoder , self ) . fit ( y ) return self fit_transform ( self , X , y = None , ** kwargs ) Call of the underlying sklearn.fit_transform(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Original X and encoded y. Source code in photonai/modelwrapper/label_encoder.py def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.fit_transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" return super ( LabelEncoder , self ) . fit_transform ( y ) transform ( self , X , y = None , ** kwargs ) Call of the underlying sklearn.transform(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Original X and encoded y. Source code in photonai/modelwrapper/label_encoder.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" yt = super ( LabelEncoder , self ) . transform ( y ) return X , yt","title":"LabelEncoder"},{"location":"api/modelwrapper/label_encoder/#documentation-for-labelencoder","text":"","title":"Documentation for LabelEncoder"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder","text":"Suitable version of the scikit-learn LabelEncoder for PHOTONAI. Since the pipeline process streams the underlying samples to every transformer, this class is required.","title":"photonai.modelwrapper.label_encoder.LabelEncoder"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder.__init__","text":"Initialize the object. Source code in photonai/modelwrapper/label_encoder.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( LabelEncoder , self ) . __init__ () self . needs_y = True","title":"__init__()"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder.fit","text":"Call of the underlying sklearn.fit(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Source code in photonai/modelwrapper/label_encoder.py def fit ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ): \"\"\" Call of the underlying sklearn.fit(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. \"\"\" super ( LabelEncoder , self ) . fit ( y ) return self","title":"fit()"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder.fit_transform","text":"Call of the underlying sklearn.fit_transform(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Original X and encoded y. Source code in photonai/modelwrapper/label_encoder.py def fit_transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.fit_transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" return super ( LabelEncoder , self ) . fit_transform ( y )","title":"fit_transform()"},{"location":"api/modelwrapper/label_encoder/#photonai.modelwrapper.label_encoder.LabelEncoder.transform","text":"Call of the underlying sklearn.transform(y) method. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_features]. required y ndarray The input targets of shape [n_samples, 1]. None **kwargs Ignored input. {} Returns: Type Description (<class 'numpy.ndarray'>, <class 'numpy.ndarray'>) Original X and encoded y. Source code in photonai/modelwrapper/label_encoder.py def transform ( self , X : np . ndarray , y : np . ndarray = None , ** kwargs ) -> ( np . ndarray , np . ndarray ): \"\"\" Call of the underlying sklearn.transform(y) method. Parameters: X: The input samples of shape [n_samples, n_features]. y: The input targets of shape [n_samples, 1]. **kwargs: Ignored input. Returns: Original X and encoded y. \"\"\" yt = super ( LabelEncoder , self ) . transform ( y ) return X , yt","title":"transform()"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/","text":"Documentation for FClassifSelectPercentile Feature Selection for classification data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_classif and parameter percentile. __init__ ( self , percentile = 10 ) special Initialize the object. Parameters: Name Type Description Default percentile float Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None","title":"FClassifSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/#documentation-for-fclassifselectpercentile","text":"","title":"Documentation for FClassifSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/#photonai.modelwrapper.feature_selection.FClassifSelectPercentile","text":"Feature Selection for classification data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_classif and parameter percentile.","title":"photonai.modelwrapper.feature_selection.FClassifSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FClassifSelectPercentile/#photonai.modelwrapper.feature_selection.FClassifSelectPercentile.__init__","text":"Initialize the object. Parameters: Name Type Description Default percentile float Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/","text":"Documentation for FRegressionFilterPValue Feature Selection for Regression - p-value based. Fit f_regression and select all columns when p_value of column < p_threshold. __init__ ( self , p_threshold = 0.05 ) special Initialize the object. Parameters: Name Type Description Default p_threshold float Upper bound for p_values. 0.05 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , p_threshold : float = . 05 ): \"\"\" Initialize the object. Parameters: p_threshold: Upper bound for p_values. \"\"\" self . p_threshold = p_threshold self . selected_indices = [] self . n_original_features = None fit ( self , X , y ) Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_original_features] required y ndarray The input targets of shape [n_samples, 1] required Source code in photonai/modelwrapper/feature_selection.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: X: The input samples of shape [n_samples, n_original_features] y: The input targets of shape [n_samples, 1] \"\"\" self . n_original_features = X . shape [ 1 ] _ , p_values = f_regression ( X , y ) self . selected_indices = np . where ( p_values < self . p_threshold )[ 0 ] return self inverse_transform ( self , X ) Reverse to original dimension. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_selected_features]. required Exceptions: Type Description ValueError If input X has a different shape than during fitting. Returns: Type Description ndarray Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. Source code in photonai/modelwrapper/feature_selection.py def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. Parameters: X: The input samples of shape [n_samples, n_selected_features]. Raises: ValueError: If input X has a different shape than during fitting. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt transform ( self , X ) Reduced input X to selected_columns. Returns: Type Description ndarray Column-filtered array of shape [n_samples, n_selected_features]. Source code in photonai/modelwrapper/feature_selection.py def transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reduced input X to selected_columns. Parameters: X The input samples of shape [n_samples, n_original_features] Returns: Column-filtered array of shape [n_samples, n_selected_features]. \"\"\" return X [:, self . selected_indices ]","title":"FRegressionFilterPValue"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#documentation-for-fregressionfilterpvalue","text":"","title":"Documentation for FRegressionFilterPValue"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue","text":"Feature Selection for Regression - p-value based. Fit f_regression and select all columns when p_value of column < p_threshold.","title":"photonai.modelwrapper.feature_selection.FRegressionFilterPValue"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.__init__","text":"Initialize the object. Parameters: Name Type Description Default p_threshold float Upper bound for p_values. 0.05 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , p_threshold : float = . 05 ): \"\"\" Initialize the object. Parameters: p_threshold: Upper bound for p_values. \"\"\" self . p_threshold = p_threshold self . selected_indices = [] self . n_original_features = None","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.fit","text":"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_original_features] required y ndarray The input targets of shape [n_samples, 1] required Source code in photonai/modelwrapper/feature_selection.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\"Calculation of the important columns. Apply f_regression on input X, y to generate p_values. selected_indices = all p_value(columns) < p_threshold. Parameters: X: The input samples of shape [n_samples, n_original_features] y: The input targets of shape [n_samples, 1] \"\"\" self . n_original_features = X . shape [ 1 ] _ , p_values = f_regression ( X , y ) self . selected_indices = np . where ( p_values < self . p_threshold )[ 0 ] return self","title":"fit()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.inverse_transform","text":"Reverse to original dimension. Parameters: Name Type Description Default X ndarray The input samples of shape [n_samples, n_selected_features]. required Exceptions: Type Description ValueError If input X has a different shape than during fitting. Returns: Type Description ndarray Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. Source code in photonai/modelwrapper/feature_selection.py def inverse_transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reverse to original dimension. Parameters: X: The input samples of shape [n_samples, n_selected_features]. Raises: ValueError: If input X has a different shape than during fitting. Returns: Array of shape [n_samples, n_original_features] with columns of zeros inserted where features would have been removed. \"\"\" if X . shape [ 1 ] != len ( self . selected_indices ): msg = \"X has a different shape than during fitting.\" logger . error ( msg ) raise ValueError ( msg ) Xt = np . zeros (( X . shape [ 0 ], self . n_original_features )) Xt [:, self . selected_indices ] = X return Xt","title":"inverse_transform()"},{"location":"api/modelwrapper/feature_selection/FRegressionFilterPValue/#photonai.modelwrapper.feature_selection.FRegressionFilterPValue.transform","text":"Reduced input X to selected_columns. Returns: Type Description ndarray Column-filtered array of shape [n_samples, n_selected_features]. Source code in photonai/modelwrapper/feature_selection.py def transform ( self , X : np . ndarray ) -> np . ndarray : \"\"\"Reduced input X to selected_columns. Parameters: X The input samples of shape [n_samples, n_original_features] Returns: Column-filtered array of shape [n_samples, n_selected_features]. \"\"\" return X [:, self . selected_indices ]","title":"transform()"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/","text":"Documentation for FRegressionSelectPercentile Feature Selection for regression data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_regression and parameter percentile. __init__ ( self , percentile = 10 ) special Initialize the object. Parameters: Name Type Description Default percentile float Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None","title":"FRegressionSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/#documentation-for-fregressionselectpercentile","text":"","title":"Documentation for FRegressionSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/#photonai.modelwrapper.feature_selection.FRegressionSelectPercentile","text":"Feature Selection for regression data - percentile based. Apply VarianceThreshold -> SelectPercentile to data. SelectPercentile based on f_regression and parameter percentile.","title":"photonai.modelwrapper.feature_selection.FRegressionSelectPercentile"},{"location":"api/modelwrapper/feature_selection/FRegressionSelectPercentile/#photonai.modelwrapper.feature_selection.FRegressionSelectPercentile.__init__","text":"Initialize the object. Parameters: Name Type Description Default percentile float Percent of features to keep. 10 Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 10 ): \"\"\" Initialize the object. Parameters: percentile: Percent of features to keep. \"\"\" self . var_thres = VarianceThreshold () self . percentile = percentile self . my_fs = None","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/","text":"Documentation for LassoFeatureSelection Lasso based feature selection - based on feature_importance. Apply Lasso to ModelSelection. __init__ ( self , percentile = 0.3 , alpha = 1.0 , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default percentile float bool, default=False Percent of features to keep. 0.3 alpha float float, default=1. Weighting parameter for Lasso. 1.0 **kwargs Passed to Lasso object. {} Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 0.3 , alpha : float = 1. , ** kwargs ): \"\"\" Initialize the object. Parameters: percentile: bool, default=False Percent of features to keep. alpha: float, default=1. Weighting parameter for Lasso. **kwargs: Passed to Lasso object. \"\"\" self . percentile = percentile self . alpha = alpha self . model_selector = None self . Lasso_kwargs = kwargs self . needs_covariates = False self . needs_y = False","title":"LassoFeatureSelection"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#documentation-for-lassofeatureselection","text":"","title":"Documentation for LassoFeatureSelection"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#photonai.modelwrapper.feature_selection.LassoFeatureSelection","text":"Lasso based feature selection - based on feature_importance. Apply Lasso to ModelSelection.","title":"photonai.modelwrapper.feature_selection.LassoFeatureSelection"},{"location":"api/modelwrapper/feature_selection/LassoFeatureSelection/#photonai.modelwrapper.feature_selection.LassoFeatureSelection.__init__","text":"Initialize the object. Parameters: Name Type Description Default percentile float bool, default=False Percent of features to keep. 0.3 alpha float float, default=1. Weighting parameter for Lasso. 1.0 **kwargs Passed to Lasso object. {} Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , percentile : float = 0.3 , alpha : float = 1. , ** kwargs ): \"\"\" Initialize the object. Parameters: percentile: bool, default=False Percent of features to keep. alpha: float, default=1. Weighting parameter for Lasso. **kwargs: Passed to Lasso object. \"\"\" self . percentile = percentile self . alpha = alpha self . model_selector = None self . Lasso_kwargs = kwargs self . needs_covariates = False self . needs_y = False","title":"__init__()"},{"location":"api/modelwrapper/feature_selection/ModelSelector/","text":"Documentation for ModelSelector Model Selector - based on feature_importance. Apply feature selection on specific estimator and its importance scores. __init__ ( self , estimator_obj , threshold = 1e-05 , percentile = False ) special Initialize the object. Parameters: Name Type Description Default estimator_obj BaseEstimator Estimator with fit/tranform and possibility of feature_importance. required threshold float If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) 1e-05 percentile bool Percent of features to keep. False Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , estimator_obj : BaseEstimator , threshold : float = 1e-5 , percentile : bool = False ): \"\"\" Initialize the object. Parameters: estimator_obj: Estimator with fit/tranform and possibility of feature_importance. threshold: If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) percentile: Percent of features to keep. \"\"\" self . threshold = threshold self . estimator_obj = estimator_obj self . selected_indices = [] self . percentile = percentile self . importance_scores = [] self . n_original_features = None","title":"ModelSelector"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#documentation-for-modelselector","text":"","title":"Documentation for ModelSelector"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector","text":"Model Selector - based on feature_importance. Apply feature selection on specific estimator and its importance scores.","title":"photonai.modelwrapper.feature_selection.ModelSelector"},{"location":"api/modelwrapper/feature_selection/ModelSelector/#photonai.modelwrapper.feature_selection.ModelSelector.__init__","text":"Initialize the object. Parameters: Name Type Description Default estimator_obj BaseEstimator Estimator with fit/tranform and possibility of feature_importance. required threshold float If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) 1e-05 percentile bool Percent of features to keep. False Source code in photonai/modelwrapper/feature_selection.py def __init__ ( self , estimator_obj : BaseEstimator , threshold : float = 1e-5 , percentile : bool = False ): \"\"\" Initialize the object. Parameters: estimator_obj: Estimator with fit/tranform and possibility of feature_importance. threshold: If percentile == True: Lower Bound for required importance score to keep. If percentile == True: percentage to keep (ordered features by feature_importance) percentile: Percent of features to keep. \"\"\" self . threshold = threshold self . estimator_obj = estimator_obj self . selected_indices = [] self . percentile = percentile self . importance_scores = [] self . n_original_features = None","title":"__init__()"},{"location":"api/modelwrapper/keras/dnn_classifier/","text":"Documentation for KerasDnnClassifier Wrapper class for a classification-based Keras model. See Keras API . Examples: 1 2 3 4 5 6 7 PipelineElement ( 'KerasDnnClassifier' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 15 , 5 ]]), 'dropout_rate' : Categorical ([ 0.5 , [ 0.5 , 0.2 , 0.1 ]])}, activations = 'relu' , nn_batch_size = 32 , multi_class = True , verbosity = 1 ) __init__ ( self , multi_class = True , hidden_layer_sizes = None , learning_rate = 0.01 , loss = '' , epochs = 100 , nn_batch_size = 64 , metrics = None , callbacks = None , validation_split = 0.1 , verbosity = 1 , dropout_rate = 0.2 , activations = 'relu' , optimizer = 'adam' ) special Initialize the object. Parameters: Name Type Description Default multi_class bool Enables multi_target learning. True hidden_layer_sizes list Number of perceptrons per layer. None learning_rate float Step size of the learning adjustment. 0.01 loss str Loss function. '' epochs int Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. 100 nn_batch_size int Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. 64 metrics list List of evaluate metrics. None callbacks list Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). None validation_split float Split size of validation set. 0.1 verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 1 dropout_rate Union[float, list] A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size 0.2 activations Union[str, list] Activation function. 'relu' optimizer Union[tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2, str] Optimization algorithm. 'adam' Source code in photonai/modelwrapper/keras_dnn_classifier.py def __init__ ( self , multi_class : bool = True , hidden_layer_sizes : list = None , learning_rate : float = 0.01 , loss : str = \"\" , epochs : int = 100 , nn_batch_size : int = 64 , metrics : list = None , callbacks : list = None , validation_split : float = 0.1 , verbosity : int = 1 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: multi_class: Enables multi_target learning. hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . multi_class = multi_class self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'accuracy' ] super ( KerasDnnClassifier , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"softmax\" , learning_rate = learning_rate , loss = loss , metrics = metrics , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer , verbosity = verbosity ) fit ( self , X , y ) Starting the learning process of the neural network. Parameters: Name Type Description Default X ndarray The input samples with shape [n_samples, n_features]. required y ndarray The input targets with shape [n_samples, 1]. required Source code in photonai/modelwrapper/keras_dnn_classifier.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning process of the neural network. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . _calc_target_dimension ( y ) self . create_model ( X . shape [ 1 ]) super ( KerasDnnClassifier , self ) . fit ( X , y ) return self","title":"KerasDnnClassifier"},{"location":"api/modelwrapper/keras/dnn_classifier/#documentation-for-kerasdnnclassifier","text":"","title":"Documentation for KerasDnnClassifier"},{"location":"api/modelwrapper/keras/dnn_classifier/#photonai.modelwrapper.keras_dnn_classifier.KerasDnnClassifier","text":"Wrapper class for a classification-based Keras model. See Keras API . Examples: 1 2 3 4 5 6 7 PipelineElement ( 'KerasDnnClassifier' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 15 , 5 ]]), 'dropout_rate' : Categorical ([ 0.5 , [ 0.5 , 0.2 , 0.1 ]])}, activations = 'relu' , nn_batch_size = 32 , multi_class = True , verbosity = 1 )","title":"photonai.modelwrapper.keras_dnn_classifier.KerasDnnClassifier"},{"location":"api/modelwrapper/keras/dnn_classifier/#photonai.modelwrapper.keras_dnn_classifier.KerasDnnClassifier.__init__","text":"Initialize the object. Parameters: Name Type Description Default multi_class bool Enables multi_target learning. True hidden_layer_sizes list Number of perceptrons per layer. None learning_rate float Step size of the learning adjustment. 0.01 loss str Loss function. '' epochs int Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. 100 nn_batch_size int Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. 64 metrics list List of evaluate metrics. None callbacks list Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). None validation_split float Split size of validation set. 0.1 verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 1 dropout_rate Union[float, list] A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size 0.2 activations Union[str, list] Activation function. 'relu' optimizer Union[tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2, str] Optimization algorithm. 'adam' Source code in photonai/modelwrapper/keras_dnn_classifier.py def __init__ ( self , multi_class : bool = True , hidden_layer_sizes : list = None , learning_rate : float = 0.01 , loss : str = \"\" , epochs : int = 100 , nn_batch_size : int = 64 , metrics : list = None , callbacks : list = None , validation_split : float = 0.1 , verbosity : int = 1 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: multi_class: Enables multi_target learning. hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . multi_class = multi_class self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'accuracy' ] super ( KerasDnnClassifier , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"softmax\" , learning_rate = learning_rate , loss = loss , metrics = metrics , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer , verbosity = verbosity )","title":"__init__()"},{"location":"api/modelwrapper/keras/dnn_classifier/#photonai.modelwrapper.keras_dnn_classifier.KerasDnnClassifier.fit","text":"Starting the learning process of the neural network. Parameters: Name Type Description Default X ndarray The input samples with shape [n_samples, n_features]. required y ndarray The input targets with shape [n_samples, 1]. required Source code in photonai/modelwrapper/keras_dnn_classifier.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning process of the neural network. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . _calc_target_dimension ( y ) self . create_model ( X . shape [ 1 ]) super ( KerasDnnClassifier , self ) . fit ( X , y ) return self","title":"fit()"},{"location":"api/modelwrapper/keras/dnn_regressor/","text":"Documentation for KerasDnnRegressor Wrapper class for a regression-based Keras model. See Keras API . Examples: 1 2 3 4 5 6 7 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 18 , 14 ], [ 30 , 5 ]]), 'dropout_rate' : Categorical ([ 0.01 , 0.2 ])}, activations = 'relu' , epochs = 50 , nn_batch_size = 64 , verbosity = 1 ) __init__ ( self , hidden_layer_sizes = None , learning_rate = 0.01 , loss = 'mean_squared_error' , epochs = 10 , nn_batch_size = 64 , metrics = None , validation_split = 0.1 , callbacks = None , batch_normalization = True , verbosity = 0 , dropout_rate = 0.2 , activations = 'relu' , optimizer = 'adam' ) special Initialize the object. Parameters: Name Type Description Default hidden_layer_sizes int Number of perceptrons per layer. None learning_rate float Step size of the learning adjustment. 0.01 loss str Loss function. 'mean_squared_error' epochs int Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. 10 nn_batch_size int Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. 64 metrics list List of evaluate metrics. None callbacks list Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). None validation_split float Split size of validation set. 0.1 batch_normalization bool Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. True verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 0 dropout_rate Union[float, list] A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size 0.2 activations Union[str, list] Activation function. 'relu' optimizer Union[tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2, str] Optimization algorithm. 'adam' Source code in photonai/modelwrapper/keras_dnn_regressor.py def __init__ ( self , hidden_layer_sizes : int = None , learning_rate : float = 0.01 , loss : str = \"mean_squared_error\" , epochs : int = 10 , nn_batch_size : int = 64 , metrics : list = None , validation_split : float = 0.1 , callbacks : list = None , batch_normalization : bool = True , verbosity : int = 0 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. batch_normalization: Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'mean_squared_error' ] super ( KerasDnnRegressor , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"linear\" , target_dimension = 1 , learning_rate = learning_rate , loss = loss , metrics = metrics , batch_normalization = batch_normalization , verbosity = verbosity , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer ) fit ( self , X , y ) Starting the learning. Parameters: Name Type Description Default X ndarray The input samples with shape [n_samples, n_features]. required y ndarray The input targets with shape [n_samples, 1]. required Source code in photonai/modelwrapper/keras_dnn_regressor.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . create_model ( X . shape [ 1 ]) super ( KerasDnnBaseModel , self ) . fit ( X , y ) return self","title":"KerasDnnRegressor"},{"location":"api/modelwrapper/keras/dnn_regressor/#documentation-for-kerasdnnregressor","text":"","title":"Documentation for KerasDnnRegressor"},{"location":"api/modelwrapper/keras/dnn_regressor/#photonai.modelwrapper.keras_dnn_regressor.KerasDnnRegressor","text":"Wrapper class for a regression-based Keras model. See Keras API . Examples: 1 2 3 4 5 6 7 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 18 , 14 ], [ 30 , 5 ]]), 'dropout_rate' : Categorical ([ 0.01 , 0.2 ])}, activations = 'relu' , epochs = 50 , nn_batch_size = 64 , verbosity = 1 )","title":"photonai.modelwrapper.keras_dnn_regressor.KerasDnnRegressor"},{"location":"api/modelwrapper/keras/dnn_regressor/#photonai.modelwrapper.keras_dnn_regressor.KerasDnnRegressor.__init__","text":"Initialize the object. Parameters: Name Type Description Default hidden_layer_sizes int Number of perceptrons per layer. None learning_rate float Step size of the learning adjustment. 0.01 loss str Loss function. 'mean_squared_error' epochs int Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. 10 nn_batch_size int Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. 64 metrics list List of evaluate metrics. None callbacks list Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). None validation_split float Split size of validation set. 0.1 batch_normalization bool Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. True verbosity int The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. 0 dropout_rate Union[float, list] A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size 0.2 activations Union[str, list] Activation function. 'relu' optimizer Union[tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2, str] Optimization algorithm. 'adam' Source code in photonai/modelwrapper/keras_dnn_regressor.py def __init__ ( self , hidden_layer_sizes : int = None , learning_rate : float = 0.01 , loss : str = \"mean_squared_error\" , epochs : int = 10 , nn_batch_size : int = 64 , metrics : list = None , validation_split : float = 0.1 , callbacks : list = None , batch_normalization : bool = True , verbosity : int = 0 , dropout_rate : Union [ float , list ] = 0.2 , activations : Union [ str , list ] = 'relu' , optimizer : Union [ Optimizer , str ] = \"adam\" ): \"\"\" Initialize the object. Parameters: hidden_layer_sizes: Number of perceptrons per layer. learning_rate: Step size of the learning adjustment. loss: Loss function. epochs: Number of arbitrary cutoffs, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. nn_batch_size: Typically the batch_size. A batch is a set of nn_batch_size samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model. metrics: List of evaluate metrics. callbacks: Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving). validation_split: Split size of validation set. batch_normalization: Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. verbosity: The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug. dropout_rate: A Dropout layer applies random dropout and rescales the output. In inference mode, the same layer does nothing. Float -> added behind each layer List -> Same size as hidden_layer_size activations: Activation function. optimizer: Optimization algorithm. \"\"\" self . _loss = \"\" self . _multi_class = None self . loss = loss self . epochs = epochs self . nn_batch_size = nn_batch_size self . validation_split = validation_split if callbacks : self . callbacks = callbacks else : self . callbacks = [] if not metrics : metrics = [ 'mean_squared_error' ] super ( KerasDnnRegressor , self ) . __init__ ( hidden_layer_sizes = hidden_layer_sizes , target_activation = \"linear\" , target_dimension = 1 , learning_rate = learning_rate , loss = loss , metrics = metrics , batch_normalization = batch_normalization , verbosity = verbosity , dropout_rate = dropout_rate , activations = activations , optimizer = optimizer )","title":"__init__()"},{"location":"api/modelwrapper/keras/dnn_regressor/#photonai.modelwrapper.keras_dnn_regressor.KerasDnnRegressor.fit","text":"Starting the learning. Parameters: Name Type Description Default X ndarray The input samples with shape [n_samples, n_features]. required y ndarray The input targets with shape [n_samples, 1]. required Source code in photonai/modelwrapper/keras_dnn_regressor.py def fit ( self , X : np . ndarray , y : np . ndarray ): \"\"\" Starting the learning. Parameters: X: The input samples with shape [n_samples, n_features]. y: The input targets with shape [n_samples, 1]. \"\"\" self . create_model ( X . shape [ 1 ]) super ( KerasDnnBaseModel , self ) . fit ( X , y ) return self","title":"fit()"},{"location":"api/optimization/grid_search/","text":"Documentation for GridSearchOptimizer Grid search optimizer. Searches for the best configuration by iteratively testing a grid of possible hyperparameter combinations. Examples: 1 2 3 4 5 my_pipe = Hyperpipe ( name = 'grid_based_pipe' , optimizer = 'grid_search' , ... ) my_pipe . fit ( X , y ) __init__ ( self ) special Initialize the object. Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . param_grid = [] self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () next_config_generator ( self ) Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" for parameters in self . param_grid : yield parameters prepare ( self , pipeline_elements , maximize_metric ) Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () self . param_grid = create_global_config_grid ( self . pipeline_elements ) logger . info ( \"Grid Search generated \" + str ( len ( self . param_grid )) + \" configurations\" )","title":"GridSearch"},{"location":"api/optimization/grid_search/#documentation-for-gridsearchoptimizer","text":"","title":"Documentation for GridSearchOptimizer"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer","text":"Grid search optimizer. Searches for the best configuration by iteratively testing a grid of possible hyperparameter combinations. Examples: 1 2 3 4 5 my_pipe = Hyperpipe ( name = 'grid_based_pipe' , optimizer = 'grid_search' , ... ) my_pipe . fit ( X , y )","title":"photonai.optimization.grid_search.grid_search.GridSearchOptimizer"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.__init__","text":"Initialize the object. Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" self . param_grid = [] self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator ()","title":"__init__()"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.next_config_generator","text":"Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" for parameters in self . param_grid : yield parameters","title":"next_config_generator()"},{"location":"api/optimization/grid_search/#photonai.optimization.grid_search.grid_search.GridSearchOptimizer.prepare","text":"Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Creates a grid from a list of PipelineElements. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator () self . param_grid = create_global_config_grid ( self . pipeline_elements ) logger . info ( \"Grid Search generated \" + str ( len ( self . param_grid )) + \" configurations\" )","title":"prepare()"},{"location":"api/optimization/nevergrad/","text":"Documentation for NevergradOptimizer Nevergrad Wrapper for PHOTONAI. Nevergrad is a gradient-free optimization platform. Nevergrad usage and implementation details . Examples: 1 2 3 4 5 6 7 8 9 import nevergrad as ng # list of all available nevergrad optimizer print ( list ( ng . optimizers . registry . values ())) my_pipe = Hyperpipe ( 'nevergrad_example' , optimizer = 'nevergrad' , optimizer_params = { 'facade' : 'NGO' , 'n_configurations' : 30 }, ... ) __init__ ( self , facade = 'NGO' , n_configurations = 100 , rng = 42 ) special Initialize the object. Parameters: Name Type Description Default facade Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. 'NGO' n_configurations int Number of runs. 100 rng int Random Seed. 42 Source code in photonai/optimization/nevergrad/nevergrad.py def __init__ ( self , facade = 'NGO' , n_configurations : int = 100 , rng : int = 42 ): \"\"\" Initialize the object. Parameters: facade: Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. n_configurations: Number of runs. rng: Random Seed. \"\"\" if not __found__ : msg = \"Module nevergrad not found or not installed as expected. \" \\ \"Please install the nevergrad/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) if facade in list ( ng . optimizers . registry . values ()): self . facade = facade elif facade in list ( ng . optimizers . registry . keys ()): self . facade = ng . optimizers . registry [ facade ] else : msg = \"nevergrad.optimizer {} not known. Check out all available nevergrad optimizers \" \\ \"by nevergrad.optimizers.registry.keys()\" . format ( str ( facade )) logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . n_configurations = n_configurations self . space = None # Hyperparameter space for nevergrad self . switch_optiones = {} self . hyperparameters = [] self . rng = rng self . maximize_metric = False self . constant_dictionary = {} self . objective = None self . optimizer = None optimize ( self ) Start the optimization process based on the underlying objective function. Source code in photonai/optimization/nevergrad/nevergrad.py def optimize ( self ) -> None : self . optimizer . minimize ( self . objective ) prepare ( self , pipeline_elements , maximize_metric , objective_function ) Prepare Nevergrad Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/nevergrad/nevergrad.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ) -> None : \"\"\"Prepare Nevergrad Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . space = self . _build_nevergrad_space ( pipeline_elements ) self . space . random_state . seed ( self . rng ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric def nevergrad_objective_function ( ** current_config ): return objective_function ( current_config ) self . objective = nevergrad_objective_function self . optimizer = self . facade ( parametrization = self . space , budget = self . n_configurations )","title":"Nevergrad"},{"location":"api/optimization/nevergrad/#documentation-for-nevergradoptimizer","text":"","title":"Documentation for NevergradOptimizer"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer","text":"Nevergrad Wrapper for PHOTONAI. Nevergrad is a gradient-free optimization platform. Nevergrad usage and implementation details . Examples: 1 2 3 4 5 6 7 8 9 import nevergrad as ng # list of all available nevergrad optimizer print ( list ( ng . optimizers . registry . values ())) my_pipe = Hyperpipe ( 'nevergrad_example' , optimizer = 'nevergrad' , optimizer_params = { 'facade' : 'NGO' , 'n_configurations' : 30 }, ... )","title":"photonai.optimization.nevergrad.nevergrad.NevergradOptimizer"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default facade Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. 'NGO' n_configurations int Number of runs. 100 rng int Random Seed. 42 Source code in photonai/optimization/nevergrad/nevergrad.py def __init__ ( self , facade = 'NGO' , n_configurations : int = 100 , rng : int = 42 ): \"\"\" Initialize the object. Parameters: facade: Choice of the Nevergrad backend strategy, e.g. [NGO, ...]. n_configurations: Number of runs. rng: Random Seed. \"\"\" if not __found__ : msg = \"Module nevergrad not found or not installed as expected. \" \\ \"Please install the nevergrad/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) if facade in list ( ng . optimizers . registry . values ()): self . facade = facade elif facade in list ( ng . optimizers . registry . keys ()): self . facade = ng . optimizers . registry [ facade ] else : msg = \"nevergrad.optimizer {} not known. Check out all available nevergrad optimizers \" \\ \"by nevergrad.optimizers.registry.keys()\" . format ( str ( facade )) logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . n_configurations = n_configurations self . space = None # Hyperparameter space for nevergrad self . switch_optiones = {} self . hyperparameters = [] self . rng = rng self . maximize_metric = False self . constant_dictionary = {} self . objective = None self . optimizer = None","title":"__init__()"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.optimize","text":"Start the optimization process based on the underlying objective function. Source code in photonai/optimization/nevergrad/nevergrad.py def optimize ( self ) -> None : self . optimizer . minimize ( self . objective )","title":"optimize()"},{"location":"api/optimization/nevergrad/#photonai.optimization.nevergrad.nevergrad.NevergradOptimizer.prepare","text":"Prepare Nevergrad Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/nevergrad/nevergrad.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ) -> None : \"\"\"Prepare Nevergrad Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . space = self . _build_nevergrad_space ( pipeline_elements ) self . space . random_state . seed ( self . rng ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric def nevergrad_objective_function ( ** current_config ): return objective_function ( current_config ) self . objective = nevergrad_objective_function self . optimizer = self . facade ( parametrization = self . space , budget = self . n_configurations )","title":"prepare()"},{"location":"api/optimization/random_grid_search/","text":"Documentation for RandomGridSearchOptimizer Random grid search optimizer. Searches for the best configuration by randomly testing n points of a grid of possible hyperparameters. Examples: 1 2 3 4 5 6 7 my_pipe = Hyperpipe ( name = 'rgrid_based_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 50 , 'limit_in_minutes' : 10 }, ... ) my_pipe . fit ( X , y ) __init__ ( self , limit_in_minutes = None , n_configurations = 25 ) special Initialize the object. Parameters: Name Type Description Default limit_in_minutes Optional[float] Total time in minutes. None n_configurations Optional[int] Number of configurations to be calculated. 25 Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 25 ): \"\"\" Initialize the object. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" super ( RandomGridSearchOptimizer , self ) . __init__ () self . _k = n_configurations self . n_configurations = self . _k self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None next_config_generator ( self ) Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" if self . start_time is None and self . limit_in_minutes is not None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) for parameters in super ( RandomGridSearchOptimizer , self ) . next_config_generator (): if self . limit_in_minutes is None or datetime . datetime . now () < self . end_time : yield parameters prepare ( self , pipeline_elements , maximize_metric ) Prepare hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Prepare hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" super ( RandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . start_time = None self . n_configurations = self . _k self . param_grid = list ( self . param_grid ) # create random order in list np . random . shuffle ( self . param_grid ) if self . n_configurations is not None : # k is maximal all grid items if self . n_configurations > len ( self . param_grid ): self . n_configurations = len ( self . param_grid ) self . param_grid = self . param_grid [ 0 : self . n_configurations ]","title":"RandomGridSearch"},{"location":"api/optimization/random_grid_search/#documentation-for-randomgridsearchoptimizer","text":"","title":"Documentation for RandomGridSearchOptimizer"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer","text":"Random grid search optimizer. Searches for the best configuration by randomly testing n points of a grid of possible hyperparameters. Examples: 1 2 3 4 5 6 7 my_pipe = Hyperpipe ( name = 'rgrid_based_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 50 , 'limit_in_minutes' : 10 }, ... ) my_pipe . fit ( X , y )","title":"photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default limit_in_minutes Optional[float] Total time in minutes. None n_configurations Optional[int] Number of configurations to be calculated. 25 Source code in photonai/optimization/grid_search/grid_search.py def __init__ ( self , limit_in_minutes : Union [ float , None ] = None , n_configurations : Union [ int , None ] = 25 ): \"\"\" Initialize the object. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" super ( RandomGridSearchOptimizer , self ) . __init__ () self . _k = n_configurations self . n_configurations = self . _k self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None","title":"__init__()"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer.next_config_generator","text":"Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/grid_search/grid_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" if self . start_time is None and self . limit_in_minutes is not None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) for parameters in super ( RandomGridSearchOptimizer , self ) . next_config_generator (): if self . limit_in_minutes is None or datetime . datetime . now () < self . end_time : yield parameters","title":"next_config_generator()"},{"location":"api/optimization/random_grid_search/#photonai.optimization.grid_search.grid_search.RandomGridSearchOptimizer.prepare","text":"Prepare hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/grid_search/grid_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Prepare hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" super ( RandomGridSearchOptimizer , self ) . prepare ( pipeline_elements , maximize_metric ) self . start_time = None self . n_configurations = self . _k self . param_grid = list ( self . param_grid ) # create random order in list np . random . shuffle ( self . param_grid ) if self . n_configurations is not None : # k is maximal all grid items if self . n_configurations > len ( self . param_grid ): self . n_configurations = len ( self . param_grid ) self . param_grid = self . param_grid [ 0 : self . n_configurations ]","title":"prepare()"},{"location":"api/optimization/random_search/","text":"Documentation for RandomSearchOptimizer Random search optimizer. Searches for the best configuration by randomly testing hyperparameter combinations without any grid. __init__ ( self , limit_in_minutes = 60 , n_configurations = None ) special Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: Name Type Description Default limit_in_minutes Optional[float] Total time in minutes. 60 n_configurations Optional[int] Number of configurations to be calculated. None Source code in photonai/optimization/random_search/random_search.py def __init__ ( self , limit_in_minutes : Union [ float , None ] = 60 , n_configurations : Union [ int , None ] = None ): \"\"\" Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () self . n_configurations = None if not limit_in_minutes or limit_in_minutes <= 0 : self . limit_in_minutes = None else : self . limit_in_minutes = limit_in_minutes self . start_time = None self . end_time = None if not n_configurations or n_configurations <= 0 : self . n_configurations = None else : self . n_configurations = n_configurations self . k_configutration = 0 # use k++ until k==n: break if self . n_configurations is None and self . limit_in_minutes is None : msg = \"No stopping criteria for RandomSearchOptimizer.\" logger . error ( msg ) raise ValueError ( msg ) next_config_generator ( self ) Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/random_search/random_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" while True : _ = ( yield self . _generate_config ()) self . k_configutration += 1 if self . limit_in_minutes : if self . start_time is None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) if datetime . datetime . now () >= self . end_time : return if self . n_configurations : if self . k_configutration >= self . n_configurations : return prepare ( self , pipeline_elements , maximize_metric ) Initializes grid free random hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/random_search/random_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes grid free random hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator ()","title":"RandomSearch"},{"location":"api/optimization/random_search/#documentation-for-randomsearchoptimizer","text":"","title":"Documentation for RandomSearchOptimizer"},{"location":"api/optimization/random_search/#photonai.optimization.random_search.random_search.RandomSearchOptimizer","text":"Random search optimizer. Searches for the best configuration by randomly testing hyperparameter combinations without any grid.","title":"photonai.optimization.random_search.random_search.RandomSearchOptimizer"},{"location":"api/optimization/random_search/#photonai.optimization.random_search.random_search.RandomSearchOptimizer.__init__","text":"Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: Name Type Description Default limit_in_minutes Optional[float] Total time in minutes. 60 n_configurations Optional[int] Number of configurations to be calculated. None Source code in photonai/optimization/random_search/random_search.py def __init__ ( self , limit_in_minutes : Union [ float , None ] = 60 , n_configurations : Union [ int , None ] = None ): \"\"\" Initialize the object. One of limit_in_minutes or n_configurations must differ from None. Parameters: limit_in_minutes: Total time in minutes. n_configurations: Number of configurations to be calculated. \"\"\" self . pipeline_elements = None self . parameter_iterable = None self . ask = self . next_config_generator () self . n_configurations = None if not limit_in_minutes or limit_in_minutes <= 0 : self . limit_in_minutes = None else : self . limit_in_minutes = limit_in_minutes self . start_time = None self . end_time = None if not n_configurations or n_configurations <= 0 : self . n_configurations = None else : self . n_configurations = n_configurations self . k_configutration = 0 # use k++ until k==n: break if self . n_configurations is None and self . limit_in_minutes is None : msg = \"No stopping criteria for RandomSearchOptimizer.\" logger . error ( msg ) raise ValueError ( msg )","title":"__init__()"},{"location":"api/optimization/random_search/#photonai.optimization.random_search.random_search.RandomSearchOptimizer.next_config_generator","text":"Generator for new configs - ask method. Returns: Type Description Generator Yields the next config. Source code in photonai/optimization/random_search/random_search.py def next_config_generator ( self ) -> Generator : \"\"\" Generator for new configs - ask method. Returns: Yields the next config. \"\"\" while True : _ = ( yield self . _generate_config ()) self . k_configutration += 1 if self . limit_in_minutes : if self . start_time is None : self . start_time = datetime . datetime . now () self . end_time = self . start_time + datetime . timedelta ( minutes = self . limit_in_minutes ) if datetime . datetime . now () >= self . end_time : return if self . n_configurations : if self . k_configutration >= self . n_configurations : return","title":"next_config_generator()"},{"location":"api/optimization/random_search/#photonai.optimization.random_search.random_search.RandomSearchOptimizer.prepare","text":"Initializes grid free random hyperparameter search. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/random_search/random_search.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes grid free random hyperparameter search. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . pipeline_elements = pipeline_elements self . ask = self . next_config_generator ()","title":"prepare()"},{"location":"api/optimization/skopt/","text":"Documentation for SkOptOptimizer Wrapper for Scikit-Optimize with PHOTONAI. Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details A detailed parameter documentation here. Examples: 1 2 3 4 5 6 my_pipe = Hyperpipe ( 'skopt_example' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 , 'acq_func' : 'LCB' , 'acq_func_kwargs' : { 'kappa' : 1.96 }}, ... ) __init__ ( self , n_configurations = 20 , n_initial_points = 10 , limit_in_minutes = None , base_estimator = 'ET' , initial_point_generator = 'random' , acq_func = 'gp_hedge' , acq_func_kwargs = None ) special Initialize the object. Parameters: Name Type Description Default n_configurations int Number of configurations to be calculated. 20 n_initial_points int Number of evaluations with initialization points before approximating it with base_estimator . 10 limit_in_minutes Optional[float] Total time in minutes. None base_estimator Union[str, sklearn.base.RegressorMixin] Estimator for returning std(Y | x) along with E[Y | x]. 'ET' initial_point_generator str Generator for initial points. 'random' acq_func str Function to minimize over the posterior distribution. 'gp_hedge' acq_func_kwargs dict Additional arguments to be passed to the acquisition function. None Source code in photonai/optimization/scikit_optimize/sk_opt.py def __init__ ( self , n_configurations : int = 20 , n_initial_points : int = 10 , limit_in_minutes : Union [ float , None ] = None , base_estimator : Union [ str , sklearn . base . RegressorMixin ] = \"ET\" , initial_point_generator : str = \"random\" , acq_func : str = 'gp_hedge' , acq_func_kwargs : dict = None ): \"\"\" Initialize the object. Parameters: n_configurations: Number of configurations to be calculated. n_initial_points: Number of evaluations with initialization points before approximating it with `base_estimator`. limit_in_minutes: Total time in minutes. base_estimator: Estimator for returning std(Y | x) along with E[Y | x]. initial_point_generator: Generator for initial points. acq_func: Function to minimize over the posterior distribution. acq_func_kwargs: Additional arguments to be passed to the acquisition function. \"\"\" self . metric_to_optimize = '' self . n_configurations = n_configurations self . n_initial_points = n_initial_points self . base_estimator = base_estimator self . initial_point_generator = initial_point_generator self . acq_func = acq_func self . acq_func_kwargs = acq_func_kwargs self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None self . optimizer = None self . maximize_metric = None self . hyperparameter_list = [] self . constant_dictionary = {} self . ask = self . ask_generator () prepare ( self , pipeline_elements , maximize_metric ) Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . optimizer = None self . hyperparameter_list = [] self . maximize_metric = maximize_metric # build skopt space space = [] for pipe_element in pipeline_elements : if pipe_element . __class__ . __name__ == 'Switch' : error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\ 'element. We recommend the use of SMAC.' logger . error ( error_msg ) raise ValueError ( error_msg ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue skopt_param = self . _convert_photonai_to_skopt_space ( value , name ) if skopt_param is not None : space . append ( skopt_param ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) if len ( space ) == 0 : msg = \"Did not find any hyperparameter to convert into skopt space.\" logger . warning ( msg ) warnings . warn ( msg ) else : self . optimizer = Optimizer ( space , base_estimator = self . base_estimator , n_initial_points = self . n_initial_points , initial_point_generator = self . initial_point_generator , acq_func = self . acq_func , acq_func_kwargs = self . acq_func_kwargs ) self . ask = self . ask_generator () tell ( self , config , performance ) Provide a config result to calculate new ones. Parameters: Name Type Description Default config dict The configuration that has been trained and tested. required performance float Metrics about the configuration's generalization capabilities. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def tell ( self , config : dict , performance : float ) -> None : \"\"\" Provide a config result to calculate new ones. Parameters: config: The configuration that has been trained and tested. performance: Metrics about the configuration's generalization capabilities. \"\"\" # convert dictionary to list in correct order if self . optimizer is not None : config_values = [ config [ name ] for name in self . hyperparameter_list ] best_config_metric_performance = performance if self . maximize_metric : best_config_metric_performance = - best_config_metric_performance self . optimizer . tell ( config_values , best_config_metric_performance )","title":"Scikit-Optimize"},{"location":"api/optimization/skopt/#documentation-for-skoptoptimizer","text":"","title":"Documentation for SkOptOptimizer"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer","text":"Wrapper for Scikit-Optimize with PHOTONAI. Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details A detailed parameter documentation here. Examples: 1 2 3 4 5 6 my_pipe = Hyperpipe ( 'skopt_example' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 , 'acq_func' : 'LCB' , 'acq_func_kwargs' : { 'kappa' : 1.96 }}, ... )","title":"photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default n_configurations int Number of configurations to be calculated. 20 n_initial_points int Number of evaluations with initialization points before approximating it with base_estimator . 10 limit_in_minutes Optional[float] Total time in minutes. None base_estimator Union[str, sklearn.base.RegressorMixin] Estimator for returning std(Y | x) along with E[Y | x]. 'ET' initial_point_generator str Generator for initial points. 'random' acq_func str Function to minimize over the posterior distribution. 'gp_hedge' acq_func_kwargs dict Additional arguments to be passed to the acquisition function. None Source code in photonai/optimization/scikit_optimize/sk_opt.py def __init__ ( self , n_configurations : int = 20 , n_initial_points : int = 10 , limit_in_minutes : Union [ float , None ] = None , base_estimator : Union [ str , sklearn . base . RegressorMixin ] = \"ET\" , initial_point_generator : str = \"random\" , acq_func : str = 'gp_hedge' , acq_func_kwargs : dict = None ): \"\"\" Initialize the object. Parameters: n_configurations: Number of configurations to be calculated. n_initial_points: Number of evaluations with initialization points before approximating it with `base_estimator`. limit_in_minutes: Total time in minutes. base_estimator: Estimator for returning std(Y | x) along with E[Y | x]. initial_point_generator: Generator for initial points. acq_func: Function to minimize over the posterior distribution. acq_func_kwargs: Additional arguments to be passed to the acquisition function. \"\"\" self . metric_to_optimize = '' self . n_configurations = n_configurations self . n_initial_points = n_initial_points self . base_estimator = base_estimator self . initial_point_generator = initial_point_generator self . acq_func = acq_func self . acq_func_kwargs = acq_func_kwargs self . limit_in_minutes = limit_in_minutes self . start_time , self . end_time = None , None self . optimizer = None self . maximize_metric = None self . hyperparameter_list = [] self . constant_dictionary = {} self . ask = self . ask_generator ()","title":"__init__()"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.prepare","text":"Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create the hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def prepare ( self , pipeline_elements : list , maximize_metric : bool ) -> None : \"\"\" Initializes hyperparameter search with scikit-optimize. Assembles all hyperparameters of the list of PipelineElements in order to prepare the hyperparameter space. Hyperparameters can be accessed via pipe_element.hyperparameters. Parameters: pipeline_elements: List of all PipelineElements to create the hyperparameter space. maximize_metric: Boolean to distinguish between score and error. \"\"\" self . start_time = None self . optimizer = None self . hyperparameter_list = [] self . maximize_metric = maximize_metric # build skopt space space = [] for pipe_element in pipeline_elements : if pipe_element . __class__ . __name__ == 'Switch' : error_msg = 'Scikit-Optimize cannot operate in the specified hyperparameter space with a Switch ' \\ 'element. We recommend the use of SMAC.' logger . error ( error_msg ) raise ValueError ( error_msg ) if hasattr ( pipe_element , 'hyperparameters' ): for name , value in pipe_element . hyperparameters . items (): # if we only have one value we do not need to optimize if isinstance ( value , list ) and len ( value ) < 2 : self . constant_dictionary [ name ] = value [ 0 ] continue if isinstance ( value , PhotonCategorical ) and len ( value . values ) < 2 : self . constant_dictionary [ name ] = value . values [ 0 ] continue skopt_param = self . _convert_photonai_to_skopt_space ( value , name ) if skopt_param is not None : space . append ( skopt_param ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) if len ( space ) == 0 : msg = \"Did not find any hyperparameter to convert into skopt space.\" logger . warning ( msg ) warnings . warn ( msg ) else : self . optimizer = Optimizer ( space , base_estimator = self . base_estimator , n_initial_points = self . n_initial_points , initial_point_generator = self . initial_point_generator , acq_func = self . acq_func , acq_func_kwargs = self . acq_func_kwargs ) self . ask = self . ask_generator ()","title":"prepare()"},{"location":"api/optimization/skopt/#photonai.optimization.scikit_optimize.sk_opt.SkOptOptimizer.tell","text":"Provide a config result to calculate new ones. Parameters: Name Type Description Default config dict The configuration that has been trained and tested. required performance float Metrics about the configuration's generalization capabilities. required Source code in photonai/optimization/scikit_optimize/sk_opt.py def tell ( self , config : dict , performance : float ) -> None : \"\"\" Provide a config result to calculate new ones. Parameters: config: The configuration that has been trained and tested. performance: Metrics about the configuration's generalization capabilities. \"\"\" # convert dictionary to list in correct order if self . optimizer is not None : config_values = [ config [ name ] for name in self . hyperparameter_list ] best_config_metric_performance = performance if self . maximize_metric : best_config_metric_performance = - best_config_metric_performance self . optimizer . tell ( config_values , best_config_metric_performance )","title":"tell()"},{"location":"api/optimization/smac/","text":"Documentation for SMACOptimizer SMAC Wrapper for PHOTONAI. SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details here . References: 1 2 3 Hutter, F. and Hoos, H. H. and Leyton-Brown, K. Sequential Model-Based Optimization for General Algorithm Configuration In: Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 5) Examples: 1 2 3 4 5 6 my_pipe = Hyperpipe ( 'smac_example' , optimizer = 'smac' , optimizer_params = { \"facade\" : \"SMAC4BO\" , \"wallclock_limit\" : 60.0 * 10 , # seconds \"ta_run_limit\" : 100 }, # limit of configurations ... ) __init__ ( self , facade = 'SMAC4HPO' , run_obj = 'quality' , deterministic = 'true' , wallclock_limit = 60.0 , intensifier_kwargs = None , rng = 42 , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default facade Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. 'SMAC4HPO' run_obj str Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. 'quality' wallclock_limit float Maximum amount of wallclock-time used for optimization. 60.0 deterministic str If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. 'true' intensifier_kwargs dict Dict for intensifier settings. None rng int Random seed of SMAC.facade. 42 **kwargs All initial kwargs are passed to SMACs scenario. List of all a vailable parameters . {} Source code in photonai/optimization/smac/smac.py def __init__ ( self , facade = 'SMAC4HPO' , run_obj : str = \"quality\" , deterministic : str = \"true\" , wallclock_limit : float = 60.0 , intensifier_kwargs : dict = None , rng : int = 42 , ** kwargs ): \"\"\" Initialize the object. Parameters: facade: Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. run_obj: Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. wallclock_limit: Maximum amount of wallclock-time used for optimization. deterministic: If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. intensifier_kwargs: Dict for intensifier settings. rng: Random seed of SMAC.facade. **kwargs: All initial kwargs are passed to SMACs scenario. [List of all a vailable parameters]( https://automl.github.io/SMAC3/master/options.html#scenario). \"\"\" super ( SMACOptimizer , self ) . __init__ () if not __found__ : msg = \"Module smac not found or not installed as expected. \" \\ \"Please install the smac/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) self . run_obj = run_obj self . deterministic = deterministic self . wallclock_limit = wallclock_limit self . kwargs = kwargs if facade in [ \"SMAC4BO\" , SMAC4BO , \"SMAC4AC\" , SMAC4AC , \"SMAC4HPO\" , SMAC4HPO , \"BOHB4HPO\" , BOHB4HPO ]: if type ( facade ) == str : self . facade = eval ( facade ) else : self . facade = facade else : msg = \"SMAC.facade {} not known. Please use one of ['SMAC4BO', 'SMAC4AC', 'SMAC4HPO'].\" logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . rng = rng if not intensifier_kwargs : self . intensifier_kwargs = {} else : self . intensifier_kwargs = intensifier_kwargs self . cspace = ConfigurationSpace () # hyperparameter space for SMAC self . switch_optiones = {} self . hyperparameters = [] self . maximize_metric = False self . constant_dictionary = {} optimize ( self ) Start optimization process. Source code in photonai/optimization/smac/smac.py def optimize ( self ): \"\"\"Start optimization process.\"\"\" self . smac . optimize () prepare ( self , pipeline_elements , maximize_metric , objective_function ) Initializes the SMAC Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/smac/smac.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ): \"\"\" Initializes the SMAC Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . cspace = ConfigurationSpace () # build space self . _build_smac_space ( pipeline_elements ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric scenario_dict = self . kwargs scenario_dict . update ({ \"run_obj\" : self . run_obj , \"deterministic\" : self . deterministic , \"wallclock_limit\" : self . wallclock_limit , \"cs\" : self . cspace , \"limit_resources\" : False }) scenario = Scenario ( scenario_dict ) def smac_objective_function ( current_config ): current_config = { k : current_config [ k ] for k in current_config if ( current_config [ k ] and 'algos' not in k )} return objective_function ( current_config ) self . smac = self . facade ( scenario = scenario , intensifier_kwargs = self . intensifier_kwargs , rng = self . rng , tae_runner = smac_objective_function )","title":"SMAC"},{"location":"api/optimization/smac/#documentation-for-smacoptimizer","text":"","title":"Documentation for SMACOptimizer"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer","text":"SMAC Wrapper for PHOTONAI. SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details here . References: 1 2 3 Hutter, F. and Hoos, H. H. and Leyton-Brown, K. Sequential Model-Based Optimization for General Algorithm Configuration In: Proceedings of the conference on Learning and Intelligent OptimizatioN (LION 5) Examples: 1 2 3 4 5 6 my_pipe = Hyperpipe ( 'smac_example' , optimizer = 'smac' , optimizer_params = { \"facade\" : \"SMAC4BO\" , \"wallclock_limit\" : 60.0 * 10 , # seconds \"ta_run_limit\" : 100 }, # limit of configurations ... )","title":"photonai.optimization.smac.smac.SMACOptimizer"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.__init__","text":"Initialize the object. Parameters: Name Type Description Default facade Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. 'SMAC4HPO' run_obj str Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. 'quality' wallclock_limit float Maximum amount of wallclock-time used for optimization. 60.0 deterministic str If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. 'true' intensifier_kwargs dict Dict for intensifier settings. None rng int Random seed of SMAC.facade. 42 **kwargs All initial kwargs are passed to SMACs scenario. List of all a vailable parameters . {} Source code in photonai/optimization/smac/smac.py def __init__ ( self , facade = 'SMAC4HPO' , run_obj : str = \"quality\" , deterministic : str = \"true\" , wallclock_limit : float = 60.0 , intensifier_kwargs : dict = None , rng : int = 42 , ** kwargs ): \"\"\" Initialize the object. Parameters: facade: Choice of the SMAC backend strategy, [SMAC4BO, SMAC4HPO, SMAC4AC, BOHB4HPO]. run_obj: Defines the optimization metric. When optimizing runtime, cutoff_time is required as well. wallclock_limit: Maximum amount of wallclock-time used for optimization. deterministic: If true, SMAC assumes that the target function or algorithm is deterministic (the same static seed of 0 is always passed to the function/algorithm). If false, different random seeds are passed to the target function/algorithm. intensifier_kwargs: Dict for intensifier settings. rng: Random seed of SMAC.facade. **kwargs: All initial kwargs are passed to SMACs scenario. [List of all a vailable parameters]( https://automl.github.io/SMAC3/master/options.html#scenario). \"\"\" super ( SMACOptimizer , self ) . __init__ () if not __found__ : msg = \"Module smac not found or not installed as expected. \" \\ \"Please install the smac/requirements.txt PHOTONAI provides.\" logger . error ( msg ) raise ModuleNotFoundError ( msg ) self . run_obj = run_obj self . deterministic = deterministic self . wallclock_limit = wallclock_limit self . kwargs = kwargs if facade in [ \"SMAC4BO\" , SMAC4BO , \"SMAC4AC\" , SMAC4AC , \"SMAC4HPO\" , SMAC4HPO , \"BOHB4HPO\" , BOHB4HPO ]: if type ( facade ) == str : self . facade = eval ( facade ) else : self . facade = facade else : msg = \"SMAC.facade {} not known. Please use one of ['SMAC4BO', 'SMAC4AC', 'SMAC4HPO'].\" logger . error ( msg . format ( str ( facade ))) raise ValueError ( msg . format ( str ( facade ))) self . rng = rng if not intensifier_kwargs : self . intensifier_kwargs = {} else : self . intensifier_kwargs = intensifier_kwargs self . cspace = ConfigurationSpace () # hyperparameter space for SMAC self . switch_optiones = {} self . hyperparameters = [] self . maximize_metric = False self . constant_dictionary = {}","title":"__init__()"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.optimize","text":"Start optimization process. Source code in photonai/optimization/smac/smac.py def optimize ( self ): \"\"\"Start optimization process.\"\"\" self . smac . optimize ()","title":"optimize()"},{"location":"api/optimization/smac/#photonai.optimization.smac.smac.SMACOptimizer.prepare","text":"Initializes the SMAC Optimizer. Parameters: Name Type Description Default pipeline_elements list List of all PipelineElements to create hyperparameter space. required maximize_metric bool Boolean to distinguish between score and error. required objective_function Callable The cost or objective function. required Source code in photonai/optimization/smac/smac.py def prepare ( self , pipeline_elements : list , maximize_metric : bool , objective_function : Callable ): \"\"\" Initializes the SMAC Optimizer. Parameters: pipeline_elements: List of all PipelineElements to create hyperparameter space. maximize_metric: Boolean to distinguish between score and error. objective_function: The cost or objective function. \"\"\" self . cspace = ConfigurationSpace () # build space self . _build_smac_space ( pipeline_elements ) if self . constant_dictionary : msg = \"PHOTONAI has detected some one-valued params in your hyperparameters. Pleas use the kwargs for \" \\ \"constant values. This run ignores following settings: \" + str ( self . constant_dictionary . keys ()) logger . warning ( msg ) warnings . warn ( msg ) self . maximize_metric = maximize_metric scenario_dict = self . kwargs scenario_dict . update ({ \"run_obj\" : self . run_obj , \"deterministic\" : self . deterministic , \"wallclock_limit\" : self . wallclock_limit , \"cs\" : self . cspace , \"limit_resources\" : False }) scenario = Scenario ( scenario_dict ) def smac_objective_function ( current_config ): current_config = { k : current_config [ k ] for k in current_config if ( current_config [ k ] and 'algos' not in k )} return objective_function ( current_config ) self . smac = self . facade ( scenario = scenario , intensifier_kwargs = self . intensifier_kwargs , rng = self . rng , tae_runner = smac_objective_function )","title":"prepare()"},{"location":"api/optimization/hyperparameter/boolean_switch/","text":"Documentation for BooleanSwitch Boolean switch. Class for defining a boolean hyperparameter. Equivalent to Categorical([True, False]). __init__ ( self ) special Initialize the object. Source code in photonai/optimization/hyperparameters.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( BooleanSwitch , self ) . __init__ ([ True , False ])","title":"BooleanSwitch"},{"location":"api/optimization/hyperparameter/boolean_switch/#documentation-for-booleanswitch","text":"","title":"Documentation for BooleanSwitch"},{"location":"api/optimization/hyperparameter/boolean_switch/#photonai.optimization.hyperparameters.BooleanSwitch","text":"Boolean switch. Class for defining a boolean hyperparameter. Equivalent to Categorical([True, False]).","title":"photonai.optimization.hyperparameters.BooleanSwitch"},{"location":"api/optimization/hyperparameter/boolean_switch/#photonai.optimization.hyperparameters.BooleanSwitch.__init__","text":"Initialize the object. Source code in photonai/optimization/hyperparameters.py def __init__ ( self ): \"\"\"Initialize the object.\"\"\" super ( BooleanSwitch , self ) . __init__ ([ True , False ])","title":"__init__()"},{"location":"api/optimization/hyperparameter/categorical/","text":"Documentation for Categorical Class for defining a definite list of values. __init__ ( self , values ) special Initialize the object. Parameters: Name Type Description Default values list Definite list of hyperparameter values. required Source code in photonai/optimization/hyperparameters.py def __init__ ( self , values : list ): \"\"\" Initialize the object. Parameters: values: Definite list of hyperparameter values. \"\"\" super ( Categorical , self ) . __init__ ( values )","title":"Categorical"},{"location":"api/optimization/hyperparameter/categorical/#documentation-for-categorical","text":"","title":"Documentation for Categorical"},{"location":"api/optimization/hyperparameter/categorical/#photonai.optimization.hyperparameters.Categorical","text":"Class for defining a definite list of values.","title":"photonai.optimization.hyperparameters.Categorical"},{"location":"api/optimization/hyperparameter/categorical/#photonai.optimization.hyperparameters.Categorical.__init__","text":"Initialize the object. Parameters: Name Type Description Default values list Definite list of hyperparameter values. required Source code in photonai/optimization/hyperparameters.py def __init__ ( self , values : list ): \"\"\" Initialize the object. Parameters: values: Definite list of hyperparameter values. \"\"\" super ( Categorical , self ) . __init__ ( values )","title":"__init__()"},{"location":"api/optimization/hyperparameter/float_range/","text":"Documentation for FloatRange Float range. Class for easily creating an interval of numbers to be tested in the optimization process. __init__ ( self , start , stop , range_type = 'linspace' , step = None , num = None , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default start float The start value for generating the lower bound. The resulting interval includes the value. required stop float The stop value for generating the upper bound. if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options, \"range\": numpy.arange is used to generate a list of values separated by the same step width. \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). 'linspace' step float If range_type == 'range', the spacing between values. None num int If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. None kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'linspace' , step : float = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super ( FloatRange , self ) . __init__ ( start , stop , range_type , step , num , np . float64 , ** kwargs ) get_random_value ( self , definite_list = False ) Method for random search to get a random value based on the underlying domain. Parameters: Name Type Description Default definite_list bool Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . uniform ( self . start , self . stop )","title":"FloatRange"},{"location":"api/optimization/hyperparameter/float_range/#documentation-for-floatrange","text":"","title":"Documentation for FloatRange"},{"location":"api/optimization/hyperparameter/float_range/#photonai.optimization.hyperparameters.FloatRange","text":"Float range. Class for easily creating an interval of numbers to be tested in the optimization process.","title":"photonai.optimization.hyperparameters.FloatRange"},{"location":"api/optimization/hyperparameter/float_range/#photonai.optimization.hyperparameters.FloatRange.__init__","text":"Initialize the object. Parameters: Name Type Description Default start float The start value for generating the lower bound. The resulting interval includes the value. required stop float The stop value for generating the upper bound. if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options, \"range\": numpy.arange is used to generate a list of values separated by the same step width. \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). 'linspace' step float If range_type == 'range', the spacing between values. None num int If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. None kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'linspace' , step : float = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super ( FloatRange , self ) . __init__ ( start , stop , range_type , step , num , np . float64 , ** kwargs )","title":"__init__()"},{"location":"api/optimization/hyperparameter/float_range/#photonai.optimization.hyperparameters.FloatRange.get_random_value","text":"Method for random search to get a random value based on the underlying domain. Parameters: Name Type Description Default definite_list bool Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the num parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . uniform ( self . start , self . stop )","title":"get_random_value()"},{"location":"api/optimization/hyperparameter/integer_range/","text":"Documentation for IntegerRange Integer range. Class for easily creating a range of integers to be tested in optimization process. __init__ ( self , start , stop , range_type = 'range' , step = None , num = None , ** kwargs ) special Initialize the object. Parameters: Name Type Description Default start float The start value for generating the lower bound. The resulting interval includes the value. required stop float The stop value for generating the upper bound. if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options, \"range\": numpy.arange is used to generate a list of values separated by the same step width. \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). 'range' step int If range_type == 'range', the spacing between values. None num int If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. None **kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'range' , step : int = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. **kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super () . __init__ ( start , stop , range_type , step , num , np . int32 , ** kwargs ) get_random_value ( self , definite_list = False ) Method for random search to get a random value based on the underlying domain. Parameters: Name Type Description Default definite_list bool Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . randint ( self . start , self . stop - 1 )","title":"IntegerRange"},{"location":"api/optimization/hyperparameter/integer_range/#documentation-for-integerrange","text":"","title":"Documentation for IntegerRange"},{"location":"api/optimization/hyperparameter/integer_range/#photonai.optimization.hyperparameters.IntegerRange","text":"Integer range. Class for easily creating a range of integers to be tested in optimization process.","title":"photonai.optimization.hyperparameters.IntegerRange"},{"location":"api/optimization/hyperparameter/integer_range/#photonai.optimization.hyperparameters.IntegerRange.__init__","text":"Initialize the object. Parameters: Name Type Description Default start float The start value for generating the lower bound. The resulting interval includes the value. required stop float The stop value for generating the upper bound. if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). required range_type str Which method to use for generating the number interval. Possible options, \"range\": numpy.arange is used to generate a list of values separated by the same step width. \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). 'range' step int If range_type == 'range', the spacing between values. None num int If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. None **kwargs Further parameters that should be passed to the numpy function chosen with range_type. {} Source code in photonai/optimization/hyperparameters.py def __init__ ( self , start : float , stop : float , range_type : str = 'range' , step : int = None , num : int = None , ** kwargs ): \"\"\" Initialize the object. Parameters: start: The start value for generating the lower bound. The resulting interval includes the value. stop: The stop value for generating the upper bound. - if range_type == \"range\": The end value is not included in the interval (see documentation of numpy.arange). - if range_type == \"linspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.linspace). - if range_type == \"logspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). - if range_type == \"geomspace\" The end value is included in the interval, unless endpoint is set to False (see documentation of numpy.logspace). range_type: Which method to use for generating the number interval. Possible options, - \"range\": numpy.arange is used to generate a list of values separated by the same step width. - \"linspace\": numpy.linspace is used to generate a certain number of values between start and stop. - \"logspace\": numpy.logspace is used to generate a logarithmically distributed range of a certain length. - \"geomspace\": numpy.geomspace is used to generate numbers spaced evenly on a log scale (geometric progression). step: If range_type == 'range', the spacing between values. num: If range_type == 'linspace', range_type == 'logspace', or range_type == 'geomspace', the number of samples to generate. **kwargs: Further parameters that should be passed to the numpy function chosen with range_type. \"\"\" super () . __init__ ( start , stop , range_type , step , num , np . int32 , ** kwargs )","title":"__init__()"},{"location":"api/optimization/hyperparameter/integer_range/#photonai.optimization.hyperparameters.IntegerRange.get_random_value","text":"Method for random search to get a random value based on the underlying domain. Parameters: Name Type Description Default definite_list bool Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. False Source code in photonai/optimization/hyperparameters.py def get_random_value ( self , definite_list : bool = False ): \"\"\" Method for random search to get a random value based on the underlying domain. Parameters: definite_list: Choice between an element of a discrete list or a value within an interval. As example, the step parameter would vanishes when this parameter is set to False. \"\"\" if definite_list : if not self . values : msg = \"No values were set. Please use transform method.\" logger . error ( msg ) raise ValueError ( msg ) return random . choice ( self . values ) else : return random . randint ( self . start , self . stop - 1 )","title":"get_random_value()"},{"location":"api/processing/results_handler/","text":"Documentation for ResultsHandler Provides all functions that operate on calculated results. As IO for the results object the ResultsHandler is able to handle results on its own. __init__ ( self , results_object = None , output_settings = None ) special Initialize the object. Parameters: Name Type Description Default results_object MDBHyperpipe All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. None output_settings OutputSettings Setting for creation and storage of the results_object. None Source code in photonai/processing/results_handler.py def __init__ ( self , results_object : MDBHyperpipe = None , output_settings = None ): \"\"\" Initialize the object. Parameters: results_object: All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. output_settings (OutputSettings): Setting for creation and storage of the results_object. \"\"\" self . results = results_object self . output_settings = output_settings get_config_evaluations ( self ) Return the test performance of every tested configuration in every outer fold. Returns: Type Description dict Test performance of every configuration. Source code in photonai/processing/results_handler.py def get_config_evaluations ( self ) -> dict : \"\"\" Return the test performance of every tested configuration in every outer fold. Returns: Test performance of every configuration. \"\"\" config_performances = list () maximum_fold = None for outer_fold in self . results . outer_folds : if maximum_fold is None or len ( outer_fold . tested_config_list ) > maximum_fold : maximum_fold = len ( outer_fold . tested_config_list ) for outer_fold in self . results . outer_folds : performance = dict () for metric in self . results . hyperpipe_info . metrics : performance [ metric ] = list () for i in range ( maximum_fold ): # for config in outer_fold.tested_config_list: for metric in self . results . hyperpipe_info . metrics : if i >= len ( outer_fold . tested_config_list ): performance [ metric ] . append ( np . nan ) continue config = outer_fold . tested_config_list [ i ] if config . config_failed : performance [ metric ] . append ( np . nan ) else : for item in config . metrics_test : if ( item . operation == 'mean' ) and ( item . metric_name == metric ): performance [ metric ] . append ( item . value ) config_performances . append ( performance ) config_performances_dict = dict () for metric in self . results . hyperpipe_info . metrics : config_performances_dict [ metric ] = list () for fold in config_performances : config_performances_dict [ metric ] . append ( fold [ metric ]) return config_performances_dict get_methods () staticmethod This function returns a list of all methods available for ResultsHandler. Returns: Type Description list List of all available methods. Source code in photonai/processing/results_handler.py @staticmethod def get_methods () -> list : \"\"\" This function returns a list of all methods available for ResultsHandler. Returns: List of all available methods. \"\"\" methods_list = [ s for s in dir ( ResultsHandler ) if '__' not in s ] return methods_list get_performance_table ( self ) This function returns a summary table of the overall results. ToDo: add best_config information! Source code in photonai/processing/results_handler.py def get_performance_table ( self ): \"\"\"This function returns a summary table of the overall results. ToDo: add best_config information! \"\"\" res_tab = pd . DataFrame () for i , folds in enumerate ( self . results . outer_folds ): # add best config infos res_tab . loc [ i , 'best_config' ] = str ( folds . best_config . human_readable_config ) # add fold index res_tab . loc [ i , 'fold' ] = folds . fold_nr # add sample size infos res_tab . loc [ i , 'n_train' ] = folds . best_config . best_config_score . number_samples_training res_tab . loc [ i , 'n_validation' ] = folds . best_config . best_config_score . number_samples_validation # add performance metrics d = folds . best_config . best_config_score . validation . metrics for key , value in d . items (): res_tab . loc [ i , key ] = value # add row with overall info res_tab . loc [ i + 1 , 'n_validation' ] = np . sum ( res_tab [ 'n_validation' ]) for key , value in d . items (): m = res_tab . loc [:, key ] res_tab . loc [ i + 1 , key ] = np . mean ( m ) res_tab . loc [ i + 1 , key + '_sem' ] = sem ( m ) # standard error of the mean res_tab . loc [ i + 1 , 'best_config' ] = 'Overall' return res_tab load_from_file ( self , results_file ) Read results_file from json into MDBHyperpipe object self.results. Parameters: Name Type Description Default results_file str Full path to json file. required Source code in photonai/processing/results_handler.py def load_from_file ( self , results_file : str ): \"\"\" Read results_file from json into MDBHyperpipe object self.results. Parameters: results_file: Full path to json file. \"\"\" self . results = MDBHyperpipe . from_document ( json . load ( open ( results_file , 'r' ))) load_from_mongodb ( self , mongodb_connect_url , pipe_name ) Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: Name Type Description Default mongodb_connect_url str MongoDB connection string. required pipe_name str Name of the stored hyperpipe. required Source code in photonai/processing/results_handler.py def load_from_mongodb ( self , mongodb_connect_url : str , pipe_name : str ): \"\"\" Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: mongodb_connect_url: MongoDB connection string. pipe_name: Name of the stored hyperpipe. \"\"\" connect ( mongodb_connect_url , alias = \"photon_core\" ) results = list ( MDBHyperpipe . objects . raw ({ 'name' : pipe_name })) if len ( results ) == 1 : self . results = results [ 0 ] elif len ( results ) > 1 : self . results = MDBHyperpipe . objects . order_by ([( \"computation_start_time\" , DESCENDING )]) . raw ({ 'name' : pipe_name }) . first () warn_text = 'Found multiple hyperpipes with that name. Returning most recent one.' logger . warning ( warn_text ) warnings . warn ( warn_text ) else : raise FileNotFoundError ( 'Could not load hyperpipe from MongoDB.' )","title":"Results Handler"},{"location":"api/processing/results_handler/#documentation-for-resultshandler","text":"","title":"Documentation for ResultsHandler"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler","text":"Provides all functions that operate on calculated results. As IO for the results object the ResultsHandler is able to handle results on its own.","title":"photonai.processing.results_handler.ResultsHandler"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.__init__","text":"Initialize the object. Parameters: Name Type Description Default results_object MDBHyperpipe All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. None output_settings OutputSettings Setting for creation and storage of the results_object. None Source code in photonai/processing/results_handler.py def __init__ ( self , results_object : MDBHyperpipe = None , output_settings = None ): \"\"\" Initialize the object. Parameters: results_object: All results are stored here. An initial setting is not necessary, because a later loading via file or MongoDB is possible. output_settings (OutputSettings): Setting for creation and storage of the results_object. \"\"\" self . results = results_object self . output_settings = output_settings","title":"__init__()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_config_evaluations","text":"Return the test performance of every tested configuration in every outer fold. Returns: Type Description dict Test performance of every configuration. Source code in photonai/processing/results_handler.py def get_config_evaluations ( self ) -> dict : \"\"\" Return the test performance of every tested configuration in every outer fold. Returns: Test performance of every configuration. \"\"\" config_performances = list () maximum_fold = None for outer_fold in self . results . outer_folds : if maximum_fold is None or len ( outer_fold . tested_config_list ) > maximum_fold : maximum_fold = len ( outer_fold . tested_config_list ) for outer_fold in self . results . outer_folds : performance = dict () for metric in self . results . hyperpipe_info . metrics : performance [ metric ] = list () for i in range ( maximum_fold ): # for config in outer_fold.tested_config_list: for metric in self . results . hyperpipe_info . metrics : if i >= len ( outer_fold . tested_config_list ): performance [ metric ] . append ( np . nan ) continue config = outer_fold . tested_config_list [ i ] if config . config_failed : performance [ metric ] . append ( np . nan ) else : for item in config . metrics_test : if ( item . operation == 'mean' ) and ( item . metric_name == metric ): performance [ metric ] . append ( item . value ) config_performances . append ( performance ) config_performances_dict = dict () for metric in self . results . hyperpipe_info . metrics : config_performances_dict [ metric ] = list () for fold in config_performances : config_performances_dict [ metric ] . append ( fold [ metric ]) return config_performances_dict","title":"get_config_evaluations()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_methods","text":"This function returns a list of all methods available for ResultsHandler. Returns: Type Description list List of all available methods. Source code in photonai/processing/results_handler.py @staticmethod def get_methods () -> list : \"\"\" This function returns a list of all methods available for ResultsHandler. Returns: List of all available methods. \"\"\" methods_list = [ s for s in dir ( ResultsHandler ) if '__' not in s ] return methods_list","title":"get_methods()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.get_performance_table","text":"This function returns a summary table of the overall results. ToDo: add best_config information! Source code in photonai/processing/results_handler.py def get_performance_table ( self ): \"\"\"This function returns a summary table of the overall results. ToDo: add best_config information! \"\"\" res_tab = pd . DataFrame () for i , folds in enumerate ( self . results . outer_folds ): # add best config infos res_tab . loc [ i , 'best_config' ] = str ( folds . best_config . human_readable_config ) # add fold index res_tab . loc [ i , 'fold' ] = folds . fold_nr # add sample size infos res_tab . loc [ i , 'n_train' ] = folds . best_config . best_config_score . number_samples_training res_tab . loc [ i , 'n_validation' ] = folds . best_config . best_config_score . number_samples_validation # add performance metrics d = folds . best_config . best_config_score . validation . metrics for key , value in d . items (): res_tab . loc [ i , key ] = value # add row with overall info res_tab . loc [ i + 1 , 'n_validation' ] = np . sum ( res_tab [ 'n_validation' ]) for key , value in d . items (): m = res_tab . loc [:, key ] res_tab . loc [ i + 1 , key ] = np . mean ( m ) res_tab . loc [ i + 1 , key + '_sem' ] = sem ( m ) # standard error of the mean res_tab . loc [ i + 1 , 'best_config' ] = 'Overall' return res_tab","title":"get_performance_table()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.load_from_file","text":"Read results_file from json into MDBHyperpipe object self.results. Parameters: Name Type Description Default results_file str Full path to json file. required Source code in photonai/processing/results_handler.py def load_from_file ( self , results_file : str ): \"\"\" Read results_file from json into MDBHyperpipe object self.results. Parameters: results_file: Full path to json file. \"\"\" self . results = MDBHyperpipe . from_document ( json . load ( open ( results_file , 'r' )))","title":"load_from_file()"},{"location":"api/processing/results_handler/#photonai.processing.results_handler.ResultsHandler.load_from_mongodb","text":"Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: Name Type Description Default mongodb_connect_url str MongoDB connection string. required pipe_name str Name of the stored hyperpipe. required Source code in photonai/processing/results_handler.py def load_from_mongodb ( self , mongodb_connect_url : str , pipe_name : str ): \"\"\" Read results_file from MongoDB into MDBHyperpipe object self.results. Parameters: mongodb_connect_url: MongoDB connection string. pipe_name: Name of the stored hyperpipe. \"\"\" connect ( mongodb_connect_url , alias = \"photon_core\" ) results = list ( MDBHyperpipe . objects . raw ({ 'name' : pipe_name })) if len ( results ) == 1 : self . results = results [ 0 ] elif len ( results ) > 1 : self . results = MDBHyperpipe . objects . order_by ([( \"computation_start_time\" , DESCENDING )]) . raw ({ 'name' : pipe_name }) . first () warn_text = 'Found multiple hyperpipes with that name. Returning most recent one.' logger . warning ( warn_text ) warnings . warn ( warn_text ) else : raise FileNotFoundError ( 'Could not load hyperpipe from MongoDB.' )","title":"load_from_mongodb()"},{"location":"examples/classification/","text":"Classification Classification is one of the central machine learning tasks. With PHOTONAI, classification pipelines can be created and designed easily. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange , Categorical , IntegerRange my_pipe = Hyperpipe ( 'basic_svm_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 1 , 6 )}, gamma = 'scale' ) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Simple Classification"},{"location":"examples/compare_estimators/","text":"Comparing Estimators With the specialized switch optimizer the user can allocate the same computational resource to hyperparameter optimize the pipeline for each learning algorithm in a final switch element , respectively. The user chooses a hyperparameter optimization strategy, to be applied to optimize the pipeline for each learning algorithm in a distinct hyperparameter space. Thereby each algorithm is optimized with the pipeline with the same settings, so that comparability between the learning algorithms is given. Another strategy would be to optimize estimator selection within a unified hyperparameter space, e.g. by applying the smac3 optimizer . Within a unified hyperparameter space there is an exploration phase, after which only the most promising algorithms receive further computational time and thus, some learning algorithms receive more computational resources than others. This strategy is capable to auto- matically select the best algorithm, however it is due to the given reasons less suitable for algorithm comparisons. With the last line of code in this example, the user requests a comparative performance metrics table, that shows the mean validation performances for the best configurations found in each outer fold for each estimator, respectively. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import FloatRange , IntegerRange my_pipe = Hyperpipe ( 'hp_switch_optimizer' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'switch' , optimizer_params = { 'name' : 'sk_opt' , 'n_configurations' : 50 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' , verbosity = 1 ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) # set up two learning algorithms in an ensemble estimator_selection = Switch ( 'estimators' ) estimator_selection += PipelineElement ( 'RandomForestClassifier' , criterion = 'gini' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 ), 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'bootstrap' : [ True , False ]}) estimator_selection += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 ), 'kernel' : [ 'linear' , 'rbf' ]}) my_pipe += estimator_selection X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y ) my_pipe . results_handler . get_mean_of_best_validation_configs_per_estimator ()","title":"Compare estimators"},{"location":"examples/compare_estimators/#comparing-estimators","text":"With the specialized switch optimizer the user can allocate the same computational resource to hyperparameter optimize the pipeline for each learning algorithm in a final switch element , respectively. The user chooses a hyperparameter optimization strategy, to be applied to optimize the pipeline for each learning algorithm in a distinct hyperparameter space. Thereby each algorithm is optimized with the pipeline with the same settings, so that comparability between the learning algorithms is given. Another strategy would be to optimize estimator selection within a unified hyperparameter space, e.g. by applying the smac3 optimizer . Within a unified hyperparameter space there is an exploration phase, after which only the most promising algorithms receive further computational time and thus, some learning algorithms receive more computational resources than others. This strategy is capable to auto- matically select the best algorithm, however it is due to the given reasons less suitable for algorithm comparisons. With the last line of code in this example, the user requests a comparative performance metrics table, that shows the mean validation performances for the best configurations found in each outer fold for each estimator, respectively. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import FloatRange , IntegerRange my_pipe = Hyperpipe ( 'hp_switch_optimizer' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'switch' , optimizer_params = { 'name' : 'sk_opt' , 'n_configurations' : 50 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' , verbosity = 1 ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) # set up two learning algorithms in an ensemble estimator_selection = Switch ( 'estimators' ) estimator_selection += PipelineElement ( 'RandomForestClassifier' , criterion = 'gini' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 ), 'max_features' : [ 'auto' , 'sqrt' , 'log2' ], 'bootstrap' : [ True , False ]}) estimator_selection += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 ), 'kernel' : [ 'linear' , 'rbf' ]}) my_pipe += estimator_selection X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y ) my_pipe . results_handler . get_mean_of_best_validation_configs_per_estimator ()","title":"Comparing Estimators"},{"location":"examples/confounder_removal/","text":"In some situations, and especially in the life sciences, we are interested in the predictive value of certain features but would therefore like to exclude the contribution of confounding variables. In order to do that, simple linear models can be used to regress out the effect of a confounder from all of the features. However, to ensure the independence of training and test set, this has to be done within the cross-validation framework. Adding a ConfoundRemoval PipelineElement to a PHOTONAI pipeline will ensure exactly that when regressing out confounding effects. The confounder variables can be passed to the Hyperpipe in the .fit() method (see example below). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement # WE USE THE BREAST CANCER SET FROM SKLEARN data = load_breast_cancer () y = data . target # now let's assume we want to regress out the effect of mean_radius and mean_texture X = data . data [:, 2 :] mean_radius = data . data [:, 0 ] mean_texture = data . data [:, 1 ] # BUILD HYPERPIPE pipe = Hyperpipe ( 'confounder_removal_example' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 5 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # # there are two ways of specifying multiple confounders # # first, you can simply pass a dictionary with \"confounder\" as key and a data matrix or list as value # pipe += PipelineElement('ConfounderRemoval', {}, standardize_covariates=True, test_disabled=False) # pipe.fit(X, y, confounder=[mean_radius, mean_texture]) # pipe += PipelineElement('SVC') # second, you can also specify the names of the variables that should be used in the confounder removal step pipe += PipelineElement ( 'ConfounderRemoval' , {}, standardize_covariates = True , test_disabled = True , confounder_names = [ 'mean_radius' , 'mean_texture' ]) pipe += PipelineElement ( 'SVC' ) # those names must be keys in the kwargs dictionary pipe . fit ( X , y , mean_radius = mean_radius , mean_texture = mean_texture )","title":"Remove confounders"},{"location":"examples/dnn_multiclass_prediction/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from sklearn.datasets import load_digits from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_digits ( n_class = 5 , return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'basic_keras_multiclass_pipe' , optimizer = 'grid_search' , optimizer_params = {}, metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe . add ( PipelineElement ( 'StandardScaler' )) # attention: shape of hidden_layer_sizes == shape of activations. If you want to choose a function in every layer, # grid_search eliminates combinations with len(hidden_layer_size) != len(activations). # Check out: hidden_layer_sizes=[25, 10], activations=['tanh', 'relu'] my_pipe += PipelineElement ( 'KerasDnnClassifier' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 20 , 10 , 5 ], [ 10 , 8 , 4 ]]), 'dropout_rate' : Categorical ([ 0.5 , [ 0.5 , 0.5 , 0.5 ]])}, activations = 'relu' , nn_batch_size = 32 , epochs = 50 , multi_class = True , verbosity = 0 ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Use a DNN with multiclass prediction"},{"location":"examples/group_driven_cv_split/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import GroupKFold , GroupShuffleSplit from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange , Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'group_split_pipe' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 4 ), inner_cv = GroupShuffleSplit ( n_splits = 10 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) # then do feature selection using a PCA, specify which values to try in the hyperparameter search my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : [ 5 , 10 , None ]}, test_disabled = True ) # engage and optimize the good old SVM for Classification my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 0.5 , 2 , \"linspace\" , num = 5 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y , groups = groups )","title":"Use site-specific validation"},{"location":"examples/imbalanced_data/","text":"Imbalanced Data Transform We have a simple solution for imbalanced classes in a classification problem. Based on the imbalanced-learn package , you can choose between over-, under- and combinesampling. Have a look at the Developer Website for details about the balancing data algorithms. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import warnings from sklearn.model_selection import StratifiedKFold , StratifiedShuffleSplit from sklearn.exceptions import UndefinedMetricWarning from imblearn.datasets import fetch_datasets from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import Categorical # Since we test very imbalanced data, we want to ignore some metric based zero-divisions. warnings . filterwarnings ( \"ignore\" , category = UndefinedMetricWarning ) # example of imbalanced dataset dataset = fetch_datasets ()[ 'coil_2000' ] X , y = dataset . data , dataset . target # ratio class 0: 6%, class 1: 94% my_pipe = Hyperpipe ( 'balancing_pipe' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' , 'f1_score' ], best_config_metric = 'f1_score' , outer_cv = StratifiedKFold ( n_splits = 3 ), inner_cv = StratifiedShuffleSplit ( n_splits = 5 , test_size = 0.2 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'StandardScaler' ) tested_methods = Categorical ([ 'RandomOverSampler' , 'SMOTEENN' , 'SVMSMOTE' , 'BorderlineSMOTE' , 'SMOTE' , 'ClusterCentroids' ]) my_pipe += PipelineElement ( 'ImbalancedDataTransformer' , hyperparameters = { 'method_name' : tested_methods }, test_disabled = True ) my_pipe += PipelineElement ( \"RandomForestClassifier\" , n_estimators = 200 ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y ) # Possible values for method_name: # imbalance_type = OVERSAMPLING: # - ADASYN # - BorderlineSMOTE # - KMeansSMOTE # - RandomOverSampler # - SMOTE # - SMOTENC # - SVMSMOTE # # imbalance_type = UNDERSAMPLING: # - ClusterCentroids, # - RandomUnderSampler, # - NearMiss, # - InstanceHardnessThreshold, # - CondensedNearestNeighbour, # - EditedNearestNeighbours, # - RepeatedEditedNearestNeighbours, # - AllKNN, # - NeighbourhoodCleaningRule, # - OneSidedSelection # # imbalance_type = COMBINE: # - SMOTEENN, # - SMOTETomek","title":"Over- /Undersampling"},{"location":"examples/no_outer_cv/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import numpy as np from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement X , y = load_boston ( return_X_y = True ) my_pipe = Hyperpipe ( name = 'single_outer_pipe' , metrics = [ 'mean_absolute_error' , 'mean_squared_error' , 'pearson_correlation' ], best_config_metric = 'mean_absolute_error' , use_test_set = False , inner_cv = KFold ( n_splits = 10 , shuffle = True , random_state = 42 ), verbosity = 0 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'SimpleImputer' , missing_values = np . nan , strategy = 'median' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'GaussianProcessRegressor' ) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y ) # find mean and std of all metrics here test_metrics = my_pipe . results . best_config . metrics_test train_metrics = my_pipe . results . best_config . metrics_train","title":"No Hyperparameter Optimization"},{"location":"examples/permutation_importances/","text":"Permutation Importance PHOTONAI conveniently integrates scikit-learns permutation importance function to get the permutation feature importances for the optimum pipe after optimization is finished. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from sklearn.datasets import load_diabetes from sklearn.model_selection import KFold , train_test_split from photonai.base import Hyperpipe , PipelineElement diabetes = load_diabetes () X_train , X_val , y_train , y_val = train_test_split ( diabetes . data , diabetes . target , random_state = 0 ) my_pipe = Hyperpipe ( 'basic_ridge_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'grid_search' , metrics = [ 'mean_absolute_error' ], best_config_metric = 'mean_absolute_error' , project_folder = './tmp' ) my_pipe += PipelineElement ( \"StandardScaler\" ) my_pipe += PipelineElement ( 'Ridge' , alpha = 1e-2 ) my_pipe . fit ( X_train , y_train ) r = my_pipe . get_permutation_feature_importances ( X_val , y_val , n_repeats = 50 , random_state = 0 ) for i in r . importances_mean . argsort ()[:: - 1 ]: if r . importances_mean [ i ] - 2 * r . importances_std [ i ] > 0 : print ( f \" { diabetes . feature_names [ i ] : <8 } \" f \" { r . importances_mean [ i ] : .3f } \" f \" +/- { r . importances_std [ i ] : .3f } \" )","title":"Permutation Importances"},{"location":"examples/permutation_importances/#permutation-importance","text":"PHOTONAI conveniently integrates scikit-learns permutation importance function to get the permutation feature importances for the optimum pipe after optimization is finished. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from sklearn.datasets import load_diabetes from sklearn.model_selection import KFold , train_test_split from photonai.base import Hyperpipe , PipelineElement diabetes = load_diabetes () X_train , X_val , y_train , y_val = train_test_split ( diabetes . data , diabetes . target , random_state = 0 ) my_pipe = Hyperpipe ( 'basic_ridge_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'grid_search' , metrics = [ 'mean_absolute_error' ], best_config_metric = 'mean_absolute_error' , project_folder = './tmp' ) my_pipe += PipelineElement ( \"StandardScaler\" ) my_pipe += PipelineElement ( 'Ridge' , alpha = 1e-2 ) my_pipe . fit ( X_train , y_train ) r = my_pipe . get_permutation_feature_importances ( X_val , y_val , n_repeats = 50 , random_state = 0 ) for i in r . importances_mean . argsort ()[:: - 1 ]: if r . importances_mean [ i ] - 2 * r . importances_std [ i ] > 0 : print ( f \" { diabetes . feature_names [ i ] : <8 } \" f \" { r . importances_mean [ i ] : .3f } \" f \" +/- { r . importances_std [ i ] : .3f } \" )","title":"Permutation Importance"},{"location":"examples/permutation_test/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import uuid import numpy as np from sklearn.datasets import load_breast_cancer from photonai.processing.permutation_test import PermutationTest def create_hyperpipe (): # this is needed here for the parallelisation from photonai.base import Hyperpipe , PipelineElement , OutputSettings from sklearn.model_selection import GroupKFold from sklearn.model_selection import KFold settings = OutputSettings ( mongodb_connect_url = 'mongodb://localhost:27017/photon_results' ) my_pipe = Hyperpipe ( 'permutation_test_1' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), calculate_metrics_across_folds = True , use_test_set = True , verbosity = 1 , project_folder = './tmp/' , output_settings = settings ) # Add transformer elements my_pipe += PipelineElement ( \"StandardScaler\" , hyperparameters = {}, test_disabled = True , with_mean = True , with_std = True ) my_pipe += PipelineElement ( \"PCA\" , test_disabled = False ) # Add estimator my_pipe += PipelineElement ( \"SVC\" , hyperparameters = { 'kernel' : [ 'linear' , 'rbf' ]}, gamma = 'scale' , max_iter = 1000000 ) return my_pipe X , y = load_breast_cancer ( return_X_y = True ) my_perm_id = str ( uuid . uuid4 ()) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # in case the permutation test for this specific hyperpipe has already been calculated, PHOTON will skip the permutation # runs and load existing results perm_tester = PermutationTest ( create_hyperpipe , n_perms = 2 , n_processes = 1 , random_state = 11 , permutation_id = my_perm_id ) perm_tester . fit ( X , y , groups = groups ) results = PermutationTest . _calculate_results ( my_perm_id , mongodb_path = 'mongodb://localhost:27017/photon_results' ) print ( results . p_values )","title":"Permutation test"},{"location":"examples/regression/","text":"Regression In contrast to a classification task, a regression model is based on continuous target values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import IntegerRange , FloatRange my_pipe = Hyperpipe ( 'basic_regression_pipe' , optimizer = 'random_search' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'mean_squared_error' , 'mean_absolute_error' , 'explained_variance' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 3 , shuffle = True ), verbosity = 1 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'SimpleImputer' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'LassoFeatureSelection' , hyperparameters = { 'percentile' : [ 0.1 , 0.2 , 0.3 ], 'alpha' : FloatRange ( 0.5 , 5 )}) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 10 , 50 )}) # load data and train X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Simple Regression"},{"location":"examples/sample_pairing/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import Categorical # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'sample_pairing_example_classification' , optimizer = 'grid_search' , metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp' , random_seed = 42123 ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SamplePairingClassification' , hyperparameters = { 'draw_limit' : [ 500 , 1000 , 10000 ], 'generator' : Categorical ([ 'nearest_pair' ])}, distance_metric = 'euclidean' , test_disabled = True ) my_pipe += PipelineElement ( 'RandomForestClassifier' , hyperparameters = { 'n_estimators' : [ 10 , 100 ]}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Sample Pairing"},{"location":"examples/scikit_learn_mlp/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import IntegerRange # WE USE THE BREAST CANCER SET FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) # DESIGN YOUR PIPELINE my_pipe = Hyperpipe ( 'multi_perceptron_pipe' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) # ADD ELEMENTS TO YOUR PIPELINE my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'PhotonMLPClassifier' , hyperparameters = { 'layer_1' : IntegerRange ( 1 , 5 ), 'layer_2' : IntegerRange ( 0 , 5 ), 'layer_3' : IntegerRange ( 0 , 5 )}) # NOW TRAIN YOUR PIPELINE my_pipe . fit ( X , y )","title":"Optimize a MLP"},{"location":"features/additional_data/","text":"Stream and Access Additional Data Numerous use-cases rely on data not contained in the feature matrix at runtime, e.g. when aiming to control for the effect of covariates. In PHOTONAI, additional data can be streamed through the pipeline and is accessible for all pipeline steps while - importantly - being matched to the (nested) cross-validation splits. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from sklearn.base import BaseEstimator , ClassifierMixin from photonai.base import Hyperpipe , PipelineElement class AdditionalDataWrapper ( BaseEstimator , ClassifierMixin ): def __init__ ( self ): self . needs_covariates = True def fit ( self , X , y , ** kwargs ): if \"true_predictions\" in kwargs : print ( \"Found additional data\" ) return self def predict ( self , X , ** kwargs ): y_true = kwargs [ \"true_predictions\" ] assert X . shape [ 0 ] == len ( y_true ) return y_true def save ( self ): return None my_pipe = Hyperpipe ( 'additional_data_pipe' , metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement . create ( \"CustomWrapper\" , AdditionalDataWrapper (), hyperparameters = {}) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y , true_predictions = np . array ( y ))","title":"Stream additional data"},{"location":"features/additional_data/#stream-and-access-additional-data","text":"Numerous use-cases rely on data not contained in the feature matrix at runtime, e.g. when aiming to control for the effect of covariates. In PHOTONAI, additional data can be streamed through the pipeline and is accessible for all pipeline steps while - importantly - being matched to the (nested) cross-validation splits. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from sklearn.base import BaseEstimator , ClassifierMixin from photonai.base import Hyperpipe , PipelineElement class AdditionalDataWrapper ( BaseEstimator , ClassifierMixin ): def __init__ ( self ): self . needs_covariates = True def fit ( self , X , y , ** kwargs ): if \"true_predictions\" in kwargs : print ( \"Found additional data\" ) return self def predict ( self , X , ** kwargs ): y_true = kwargs [ \"true_predictions\" ] assert X . shape [ 0 ] == len ( y_true ) return y_true def save ( self ): return None my_pipe = Hyperpipe ( 'additional_data_pipe' , metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement . create ( \"CustomWrapper\" , AdditionalDataWrapper (), hyperparameters = {}) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y , true_predictions = np . array ( y ))","title":"Stream and Access Additional Data"},{"location":"features/batching/","text":"Batch Processing PHOTONAI offers batch processing of elements. This comes in handy for working memory sensitive tasks. An example is handling large medical data modalities, such as resampling gray matter 3D brain scan niftis. However, be aware that this only makes sense for algorithms that independently transform each item. Batching is easily accessed by adding the batch_size parameter to the PipelineElement . 1 PipelineElement ( \"LabelEncoder\" , batch_size = 10 )","title":"Transform in Batches"},{"location":"features/caching/","text":"Caching PHOTONAI offers a specialized caching that handles partially overlapping hyperparameter configurations for nested cross-validation splits. This is particularly useful for reusing results from expensive computations. More generally, caching is useful whenever re-computation needs more time than loading data. It is easily enabled by adding the cache_folder parameter to the Hyperpipe . 1 2 pipe = Hyperpipe ( \"...\" , cache_folder = \"./cache\" )","title":"Caching"},{"location":"features/callbacks/","text":"Callback Elements PHOTONAI implements pipeline callbacks which allow for live inspection of the data flowing through the pipeline at runtime. Callbacks act as pipeline elements and can be inserted at any point within the pipeline. They must define a function delegate which is called with the exact same data that the next pipeline step will receive. Thereby, a developer may inspect e.g. the shape and values of the feature matrix after a sequence of transformations have been applied. Return values from the delegate functions are ignored, so that after returning from the delegate call, the original data is directly passed to the next processing step. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , CallbackElement # DEFINE CALLBACK ELEMENT def my_monitor ( X , y = None , ** kwargs ): print ( X . shape ) # here is a useless statement where you can easily set a breakpoint # and do fancy developer stuff debug = True my_pipe = Hyperpipe ( 'monitoring_pipe' , optimizer = 'grid_search' , metrics = [ 'mean_squared_error' , 'pearson_correlation' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 1 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SamplePairingClassification' , hyperparameters = { 'draw_limit' : [ 500 , 1000 , 10000 ]}, distance_metric = 'euclidean' , generator = 'nearest_pair' , test_disabled = True ) # here we inspect the data after augmentation my_pipe += CallbackElement ( \"monitor\" , my_monitor ) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : [ 10 , 100 ]}) X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Inspect the dataflow at runtime"},{"location":"features/custom_metrics/","text":"How to use custom metrics 1) You can give PHOTONAI a tuple consisting of a metric name and a function delegate that takes true and predicted values and returns a custom metric 2) You can also use a (custom or existing) class that inherits from keras.metrics.Metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np from sklearn.metrics import f1_score from sklearn.datasets import fetch_openml from sklearn.model_selection import KFold from keras.metrics import Accuracy from photonai.base import Hyperpipe , PipelineElement # you can have a simple delegate def custom_metric ( y_true , y_pred ): def hot_encoding ( targets , nclasses ): \"\"\"Convert indices to one-hot encoded labels.\"\"\" targets = np . array ( targets ) . reshape ( - 1 ) return np . eye ( nclasses )[ targets ] return f1_score ( hot_encoding ( y_true , 3 ), hot_encoding ( y_pred , 3 ), average = 'macro' ) my_pipe = Hyperpipe ( 'custom_metric_project' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, # and here is how to register it in photonai metrics = [( 'custom_metric' , custom_metric ), Accuracy , 'accuracy' ], best_config_metric = 'custom_metric' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), allow_multidim_targets = True , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SVC' , kernel = 'rbf' ) X , y = fetch_openml ( \"cars1\" , return_X_y = True ) my_pipe . fit ( X . values , y . values . astype ( int ))","title":"Custom Metrics"},{"location":"features/custom_metrics/#how-to-use-custom-metrics","text":"1) You can give PHOTONAI a tuple consisting of a metric name and a function delegate that takes true and predicted values and returns a custom metric 2) You can also use a (custom or existing) class that inherits from keras.metrics.Metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np from sklearn.metrics import f1_score from sklearn.datasets import fetch_openml from sklearn.model_selection import KFold from keras.metrics import Accuracy from photonai.base import Hyperpipe , PipelineElement # you can have a simple delegate def custom_metric ( y_true , y_pred ): def hot_encoding ( targets , nclasses ): \"\"\"Convert indices to one-hot encoded labels.\"\"\" targets = np . array ( targets ) . reshape ( - 1 ) return np . eye ( nclasses )[ targets ] return f1_score ( hot_encoding ( y_true , 3 ), hot_encoding ( y_pred , 3 ), average = 'macro' ) my_pipe = Hyperpipe ( 'custom_metric_project' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, # and here is how to register it in photonai metrics = [( 'custom_metric' , custom_metric ), Accuracy , 'accuracy' ], best_config_metric = 'custom_metric' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), allow_multidim_targets = True , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SVC' , kernel = 'rbf' ) X , y = fetch_openml ( \"cars1\" , return_X_y = True ) my_pipe . fit ( X . values , y . values . astype ( int ))","title":"How to use custom metrics"},{"location":"features/learning_curves/","text":"Learning Curves 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange my_pipe = Hyperpipe ( 'basic_forest_pipe' , inner_cv = KFold ( n_splits = 2 ), outer_cv = KFold ( n_splits = 2 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' , # this is how to make photonai calculate learning curves # output and figures for this can be found in the project folder learning_curves = True , learning_curves_cut = FloatRange ( 0.1 , 1 , step = 0.1 )) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'RandomForestClassifier' ) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Learning Curves"},{"location":"features/performance_constraints/","text":"Performance Constraints Integrating performance baselines and performance expectations in the hyperparameter optimization process is furthermore helpful to increase the overall speed and efficiency. Further testing of a specific hyperparameter configuration in further inner-cross-validation folds can be skipped if the given configuration performs worse than a given static or dynamic threshold. There are three types of contraints implemented in PHOTONAI: MinimumPerformanceConstraint : the lower bound is the given threshold (e.g. accuracy of at least 0.8) BestPerformanceConstraint : the lower bound (+- margin) is the so far best metric value DummyPerformanceConstraint : the lower bound (+-margin) is the dummy performance of the specific metric The threshold is applied in three strategies: any : Computation is skipped if any of the folds is worse than the threshold first : Computation is skipped if the first fold performs worse than the threshold mean : Computation is skipped if the mean of all folds computed so far is worse than the threshold 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , OutputSettings from photonai.optimization import MinimumPerformanceConstraint , DummyPerformanceConstraint , BestPerformanceConstraint , IntegerRange my_pipe = Hyperpipe ( name = 'constrained_forest_pipe' , optimizer = 'grid_search' , metrics = [ 'mean_squared_error' , 'mean_absolute_error' , 'pearson_correlation' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 10 ), use_test_set = True , verbosity = 1 , project_folder = './tmp' , output_settings = OutputSettings ( mongodb_connect_url = \"mongodb://localhost:27017/photon_results\" , save_output = True ), performance_constraints = [ DummyPerformanceConstraint ( 'mean_absolute_error' ), MinimumPerformanceConstraint ( 'pearson_correlation' , 0.65 , 'any' ), BestPerformanceConstraint ( 'mean_squared_error' , 3 , 'mean' )]) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 5 , 50 )}) X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Hyperparameter Optimization Shortcuts"},{"location":"features/performance_constraints/#performance-constraints","text":"Integrating performance baselines and performance expectations in the hyperparameter optimization process is furthermore helpful to increase the overall speed and efficiency. Further testing of a specific hyperparameter configuration in further inner-cross-validation folds can be skipped if the given configuration performs worse than a given static or dynamic threshold. There are three types of contraints implemented in PHOTONAI: MinimumPerformanceConstraint : the lower bound is the given threshold (e.g. accuracy of at least 0.8) BestPerformanceConstraint : the lower bound (+- margin) is the so far best metric value DummyPerformanceConstraint : the lower bound (+-margin) is the dummy performance of the specific metric The threshold is applied in three strategies: any : Computation is skipped if any of the folds is worse than the threshold first : Computation is skipped if the first fold performs worse than the threshold mean : Computation is skipped if the mean of all folds computed so far is worse than the threshold 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , OutputSettings from photonai.optimization import MinimumPerformanceConstraint , DummyPerformanceConstraint , BestPerformanceConstraint , IntegerRange my_pipe = Hyperpipe ( name = 'constrained_forest_pipe' , optimizer = 'grid_search' , metrics = [ 'mean_squared_error' , 'mean_absolute_error' , 'pearson_correlation' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 10 ), use_test_set = True , verbosity = 1 , project_folder = './tmp' , output_settings = OutputSettings ( mongodb_connect_url = \"mongodb://localhost:27017/photon_results\" , save_output = True ), performance_constraints = [ DummyPerformanceConstraint ( 'mean_absolute_error' ), MinimumPerformanceConstraint ( 'pearson_correlation' , 0.65 , 'any' ), BestPerformanceConstraint ( 'mean_squared_error' , 3 , 'mean' )]) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 5 , 50 )}) X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Performance Constraints"},{"location":"features/permutation_test/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import uuid import numpy as np from sklearn.datasets import load_breast_cancer from photonai.processing.permutation_test import PermutationTest def create_hyperpipe (): # this is needed here for the parallelisation from photonai.base import Hyperpipe , PipelineElement , OutputSettings from sklearn.model_selection import GroupKFold from sklearn.model_selection import KFold settings = OutputSettings ( mongodb_connect_url = 'mongodb://localhost:27017/photon_results' ) my_pipe = Hyperpipe ( 'permutation_test_1' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = GroupKFold ( n_splits = 2 ), inner_cv = KFold ( n_splits = 2 ), calculate_metrics_across_folds = True , use_test_set = True , verbosity = 1 , project_folder = './tmp/' , output_settings = settings ) # Add transformer elements my_pipe += PipelineElement ( \"StandardScaler\" , hyperparameters = {}, test_disabled = True , with_mean = True , with_std = True ) my_pipe += PipelineElement ( \"PCA\" , test_disabled = False ) # Add estimator my_pipe += PipelineElement ( \"SVC\" , hyperparameters = { 'kernel' : [ 'linear' , 'rbf' ]}, gamma = 'scale' , max_iter = 1000000 ) return my_pipe X , y = load_breast_cancer ( return_X_y = True ) my_perm_id = str ( uuid . uuid4 ()) groups = np . random . random_integers ( 0 , 3 , ( len ( y ), )) # in case the permutation test for this specific hyperpipe has already been calculated, PHOTON will skip the permutation # runs and load existing results perm_tester = PermutationTest ( create_hyperpipe , n_perms = 2 , n_processes = 1 , random_state = 11 , permutation_id = my_perm_id ) perm_tester . fit ( X , y , groups = groups ) results = PermutationTest . _calculate_results ( my_perm_id , mongodb_path = 'mongodb://localhost:27017/photon_results' ) print ( results . p_values )","title":"Permutation test"},{"location":"features/preprocessing/","text":"How and why to apply preprocessing There are transformations that can be applied to the data as a whole BEFORE it is split into different training, validation, and testing subsets. Thereby, computational resources are saved as these operations are not repeated for each of the cross-validation folds. Importantly, this does only make sense for transformations that do not fit a model to the dataset as a group (such as e.g. a PCA) but rather apply transformations on a single- subject level (such as resampling a nifti image). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from sklearn.model_selection import ShuffleSplit from sklearn.datasets import fetch_openml from photonai.base import Hyperpipe , PipelineElement , Preprocessing from photonai.optimization import FloatRange audiology = fetch_openml ( name = 'audiology' ) X = audiology . data . values y = audiology . target . values my_pipe = Hyperpipe ( 'hot_encoder_pipeline' , inner_cv = ShuffleSplit ( n_splits = 5 , test_size = 0.2 ), outer_cv = ShuffleSplit ( n_splits = 3 , test_size = 0.2 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 20 }, metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , allow_multidim_targets = True , project_folder = './tmp' ) pre_proc = Preprocessing () pre_proc += PipelineElement ( 'OneHotEncoder' , sparse = False ) pre_proc += PipelineElement ( 'LabelEncoder' ) my_pipe += pre_proc my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : FloatRange ( 0.2 , 0.7 )}) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 1 , 150 )}, kernel = 'rbf' ) my_pipe . fit ( X , y )","title":"Preprocessing"},{"location":"features/preprocessing/#how-and-why-to-apply-preprocessing","text":"There are transformations that can be applied to the data as a whole BEFORE it is split into different training, validation, and testing subsets. Thereby, computational resources are saved as these operations are not repeated for each of the cross-validation folds. Importantly, this does only make sense for transformations that do not fit a model to the dataset as a group (such as e.g. a PCA) but rather apply transformations on a single- subject level (such as resampling a nifti image). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from sklearn.model_selection import ShuffleSplit from sklearn.datasets import fetch_openml from photonai.base import Hyperpipe , PipelineElement , Preprocessing from photonai.optimization import FloatRange audiology = fetch_openml ( name = 'audiology' ) X = audiology . data . values y = audiology . target . values my_pipe = Hyperpipe ( 'hot_encoder_pipeline' , inner_cv = ShuffleSplit ( n_splits = 5 , test_size = 0.2 ), outer_cv = ShuffleSplit ( n_splits = 3 , test_size = 0.2 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 20 }, metrics = [ 'accuracy' ], best_config_metric = 'accuracy' , allow_multidim_targets = True , project_folder = './tmp' ) pre_proc = Preprocessing () pre_proc += PipelineElement ( 'OneHotEncoder' , sparse = False ) pre_proc += PipelineElement ( 'LabelEncoder' ) my_pipe += pre_proc my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : FloatRange ( 0.2 , 0.7 )}) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 1 , 150 )}, kernel = 'rbf' ) my_pipe . fit ( X , y )","title":"How and why to apply preprocessing"},{"location":"features/result_handler/","text":"How to query PHOTONAI's result logging There is a whole bunch of information stored in PHOTONAI's result logging tree. Here we showcase some convenience function to easily retrieve useful result details. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 import os import pandas as pd import numpy as np from sklearn.model_selection import StratifiedShuffleSplit from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.datasets import fetch_openml # blood-transfusion-service-center blood_transfusion = fetch_openml ( name = 'blood-transfusion-service-center' ) X = blood_transfusion . data . values y = blood_transfusion . target . values y = ( y == '2' ) . astype ( int ) my_pipe = Hyperpipe ( 'results_example' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 , 'acq_func_kwargs' : { 'kappa' : 1 }}, metrics = [ 'accuracy' , 'f1_score' ], best_config_metric = 'f1_score' , outer_cv = StratifiedShuffleSplit ( n_splits = 3 , test_size = 0.2 ), inner_cv = StratifiedShuffleSplit ( n_splits = 4 , test_size = 0.2 ), verbosity = 0 , project_folder = './tmp' ) # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.1 , 150 )}, probability = True ) my_pipe . fit ( X , y ) # Either, we continue working with the results directly now handler = my_pipe . results_handler #, or we load them again later. # from photonai.processing import ResultsHandler # handler = ResultsHandler().load_from_file(os.path.join(my_pipe.results.output_folder, \"photon_results_file.json\")) # A table with properties and performance of each outer # fold (and the overall run) is created with the following command. performance_table = handler . get_performance_table () with pd . option_context ( 'display.max_rows' , None , 'display.max_columns' , None ): print ( performance_table ) print ( \" \" ) # We now analyze the optimization influence on the result. config_evals = handler . get_config_evaluations () for i , j in enumerate ( config_evals [ 'f1_score' ]): print ( \"Standard deviation for fold {} : {} .\" . format ( str ( i ), str ( np . std ( j )))) print ( \" \" ) # To get an impression of the results, # it is possible to take a closer look at the test_predictions. best_config_preds = handler . get_test_predictions () y_pred = best_config_preds [ 'y_pred' ] y_pred_probabilities = best_config_preds [ 'probabilities' ] y_true = best_config_preds [ 'y_true' ] # While some elements have been misclassified, # we have a closer look to the elementwise probability. for i in range ( 2 , 6 ): attribute = \"correct\" if y_true [ i ] == y_pred [ i ] else \"incorrect\" print ( \"Test-element {} was {} predicted \" \"with an assignment probability of {} .\" . format ( str ( i ), attribute , str ( y_pred_probabilities [ i ])))","title":"Result Queries"},{"location":"features/result_handler/#how-to-query-photonais-result-logging","text":"There is a whole bunch of information stored in PHOTONAI's result logging tree. Here we showcase some convenience function to easily retrieve useful result details. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 import os import pandas as pd import numpy as np from sklearn.model_selection import StratifiedShuffleSplit from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange from sklearn.datasets import fetch_openml # blood-transfusion-service-center blood_transfusion = fetch_openml ( name = 'blood-transfusion-service-center' ) X = blood_transfusion . data . values y = blood_transfusion . target . values y = ( y == '2' ) . astype ( int ) my_pipe = Hyperpipe ( 'results_example' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 , 'acq_func_kwargs' : { 'kappa' : 1 }}, metrics = [ 'accuracy' , 'f1_score' ], best_config_metric = 'f1_score' , outer_cv = StratifiedShuffleSplit ( n_splits = 3 , test_size = 0.2 ), inner_cv = StratifiedShuffleSplit ( n_splits = 4 , test_size = 0.2 ), verbosity = 0 , project_folder = './tmp' ) # first normalize all features my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'C' : FloatRange ( 0.1 , 150 )}, probability = True ) my_pipe . fit ( X , y ) # Either, we continue working with the results directly now handler = my_pipe . results_handler #, or we load them again later. # from photonai.processing import ResultsHandler # handler = ResultsHandler().load_from_file(os.path.join(my_pipe.results.output_folder, \"photon_results_file.json\")) # A table with properties and performance of each outer # fold (and the overall run) is created with the following command. performance_table = handler . get_performance_table () with pd . option_context ( 'display.max_rows' , None , 'display.max_columns' , None ): print ( performance_table ) print ( \" \" ) # We now analyze the optimization influence on the result. config_evals = handler . get_config_evaluations () for i , j in enumerate ( config_evals [ 'f1_score' ]): print ( \"Standard deviation for fold {} : {} .\" . format ( str ( i ), str ( np . std ( j )))) print ( \" \" ) # To get an impression of the results, # it is possible to take a closer look at the test_predictions. best_config_preds = handler . get_test_predictions () y_pred = best_config_preds [ 'y_pred' ] y_pred_probabilities = best_config_preds [ 'probabilities' ] y_true = best_config_preds [ 'y_true' ] # While some elements have been misclassified, # we have a closer look to the elementwise probability. for i in range ( 2 , 6 ): attribute = \"correct\" if y_true [ i ] == y_pred [ i ] else \"incorrect\" print ( \"Test-element {} was {} predicted \" \"with an assignment probability of {} .\" . format ( str ( i ), attribute , str ( y_pred_probabilities [ i ])))","title":"How to query PHOTONAI's result logging"},{"location":"features/save_load/","text":"Reload and use a model Every successful trained pipeline saves the best model in the result folder as photon_best_model.photon. To collaborate with other people share your trained model saved in this file. Have a look at the following example script if you want to reload the trained Hyperpipe : 1 2 3 4 5 6 7 8 9 10 11 12 13 from photonai.base import Hyperpipe from sklearn.datasets import load_breast_cancer X , _ = load_breast_cancer ( True ) # After optimization is finished, PHOTONAI saves the user's pipeline # fitted with the best hyperparameter configuration found # as \"photon_best_model.photon\" in the project's result folder. # this is done automatically, however the use may do so manually by calling # my_pipe.save_optimum_pipe('/home/photon_user/photon_test/optimum_pipe.photon') my_pipe = Hyperpipe . load_optimum_pipe ( \"full_path/to/photon_best_model.photon\" ) predictions = my_pipe . predict ( X )","title":"Load and use a .photon model"},{"location":"getting_started/algorithm_index/","text":"Algorithms PHOTONAI offers easy access to established machine learning algorithms. The algorithms can be imported by adding a PipelineElement with a specific name, such as \"SVC\" for importing the SupportVectorClassifier from scikit-learn , as shown in the following examples. You can set all parameters of the imported class as usual: e.g. add gamma='auto' to the PipelineElement to set the support vector machine's gamma parameter to 'auto'. In addition, you can specify each parameter as a hyperparameter and define a value range or value list to find the optimal value, such as 'kernel': ['linear', 'rbf'] . To build a custom pipeline, have a look at PHOTONAIs pre-registered processing- and learning algorithms . You can access algorithms for all purposes from several open-source packages. In addition, PHOTONAI offers several utility classes as well, such as linear statistical feature selection or sample pairing algorithms. In addition you can specify hyperparameters as well as their value range in order to be optimized by the hyperparameter optimization strategy. Currently, PHOTONAI offers Grid-Search , Random Search and two frameworks for bayesian optimization. PCA 1 2 3 4 5 6 from photonai.base import PipelineElement PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) # to test if disabling the PipelineElement improves performance, # simply add the test_disabled=True parameter SVC 1 2 3 4 PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'poly' ]), 'C' : FloatRange ( 0.5 , 2 )}, gamma = 'auto' ) Keras Neural Net 1 2 3 4 5 6 7 8 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 , 3 ]]), 'dropout_rate' : Categorical ([[ 0.5 , 0.2 , 0.1 ], 0.1 ])}, activations = 'relu' , epochs = 5 , batch_size = 32 )","title":"Access established ml-packages"},{"location":"getting_started/algorithm_index/#algorithms","text":"PHOTONAI offers easy access to established machine learning algorithms. The algorithms can be imported by adding a PipelineElement with a specific name, such as \"SVC\" for importing the SupportVectorClassifier from scikit-learn , as shown in the following examples. You can set all parameters of the imported class as usual: e.g. add gamma='auto' to the PipelineElement to set the support vector machine's gamma parameter to 'auto'. In addition, you can specify each parameter as a hyperparameter and define a value range or value list to find the optimal value, such as 'kernel': ['linear', 'rbf'] . To build a custom pipeline, have a look at PHOTONAIs pre-registered processing- and learning algorithms . You can access algorithms for all purposes from several open-source packages. In addition, PHOTONAI offers several utility classes as well, such as linear statistical feature selection or sample pairing algorithms. In addition you can specify hyperparameters as well as their value range in order to be optimized by the hyperparameter optimization strategy. Currently, PHOTONAI offers Grid-Search , Random Search and two frameworks for bayesian optimization.","title":"Algorithms"},{"location":"getting_started/algorithm_index/#pca","text":"1 2 3 4 5 6 from photonai.base import PipelineElement PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 5 , 20 )}, test_disabled = True ) # to test if disabling the PipelineElement improves performance, # simply add the test_disabled=True parameter","title":"PCA"},{"location":"getting_started/algorithm_index/#svc","text":"1 2 3 4 PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'poly' ]), 'C' : FloatRange ( 0.5 , 2 )}, gamma = 'auto' )","title":"SVC"},{"location":"getting_started/algorithm_index/#keras-neural-net","text":"1 2 3 4 5 6 7 8 PipelineElement ( 'KerasDnnRegressor' , hyperparameters = { 'hidden_layer_sizes' : Categorical ([[ 10 , 8 , 4 ], [ 20 , 5 , 3 ]]), 'dropout_rate' : Categorical ([[ 0.5 , 0.2 , 0.1 ], 0.1 ])}, activations = 'relu' , epochs = 5 , batch_size = 32 )","title":"Keras Neural Net"},{"location":"getting_started/classification/","text":"Classification Classification is one of the central machine learning tasks. With PHOTONAI, classification pipelines can be created and designed easily. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import FloatRange , Categorical , IntegerRange my_pipe = Hyperpipe ( 'basic_svm_pipe' , inner_cv = KFold ( n_splits = 5 ), outer_cv = KFold ( n_splits = 3 ), optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' , 'balanced_accuracy' ], best_config_metric = 'accuracy' , project_folder = './tmp' ) my_pipe . add ( PipelineElement ( 'StandardScaler' )) my_pipe += PipelineElement ( 'PCA' , hyperparameters = { 'n_components' : IntegerRange ( 10 , 30 )}, test_disabled = True ) my_pipe += PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 1 , 6 )}, gamma = 'scale' ) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Classification"},{"location":"getting_started/custom_algorithm/","text":"Add a custom algorithm In order to integrate a custom algorithm in PHOTONAI, all you need to do is provide a class adhering to the popular scikit-learn object API . In the following we will demonstrate an example to integrate a custom transformer to the Hyperpipe . First, implement your data processing logic like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # we use BaseEstimator as to prepare the transformer for hyperparameter optimization # we inherit the get_params and set_params methods from sklearn.base import BaseEstimator class CustomTransformer ( BaseEstimator ): def __init__ ( self , param1 = 0 , param2 = None ): # it is important that you name your params the same in the constructor # stub as well as in your class variables! self . param1 = param1 self . param2 = param2 def fit ( self , data , targets = None , ** kwargs ): \"\"\" Adjust the underlying model or method to the data. Returns ------- IMPORTANT: must return self! \"\"\" return self def transform ( self , data , targets = None , ** kwargs ): \"\"\" Apply the method's logic to the data. \"\"\" return data Afterwards, register your element with the photon registry like this. Custom elements must only be registered once. 1 2 3 4 5 6 7 8 9 10 11 from photonai.base import PhotonRegistry custom_element_root_folder = \"./\" registry = PhotonRegistry ( custom_elements_folder = custom_element_root_folder ) registry . register ( photon_name = 'MyCustomTransformer' , class_str = 'custom_transformer.CustomTransformer' , element_type = 'Transformer' ) # show information about the element registry . info ( \"MyCustomTransformer\" ) Afterwards, you can use your custom element in the pipeline like this. Importantly, the custom_elements_folder must be activated for each use as the folder's content, and therefore the custom class implementation might otherwise not be accessible by the python script. 1 2 3 4 5 6 7 8 9 10 11 from photonai.base import PhotonRegistry , Hyperpipe , PipelineElement custom_element_root_folder = \"./\" registry = PhotonRegistry ( custom_elements_folder = custom_element_root_folder ) # This add the custom algorithm folder to the python path in order to import and instantiate the algorithm registry . activate () # then use it my_pipe = Hyperpipe ( \"...\" ) my_pipe += PipelineElement ( 'MyCustomTransformer' , hyperparameters = { 'param1' : [ 1 , 2 , 3 ]})","title":"Include custom algorithm"},{"location":"getting_started/hpos/","text":"Hyperparameter Optimization PHOTONAI offers easy access to several established hyperparameter optimization strategies. Grid Search An exhaustive searching through a manually specified subset of the hyperparameter space. The grid is defined by a finite list for each hyperparameter. 1 2 pipe = Hyperpipe ( \"...\" , optimizer = 'grid_search' ) Random Grid Search Random sampling of a manually specified subset of the hyperparameter space. The grid is defined by a finite list for each hyperparameter. Then, a specified number of random configurations from this grid is tested 1 2 3 4 pipe = Hyperpipe ( \"...\" , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 30 , 'limit_in_minutes' : 10 }) Random Search A grid-free selection of configurations based on the hyperparameter space. In the case of numerical parameters, decisions are made only on the basis of the interval limits. The creation of configurations is limited by time or a maximum number of runs. 1 2 3 4 pipe = Hyperpipe ( \"...\" , optimizer = 'random_search' , optimizer_params = { 'n_configurations' : 30 , 'limit_in_minutes' : 20 }) Scikit-Optimize Scikit-Optimize, or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-optimize usage and implementation details available here . A detailed parameter documentation here. 1 2 3 4 5 6 7 pipe = Hyperpipe ( \"...\" , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 55 , 'n_initial_points' : 15 , 'initial_point_generator' : \"sobol\" , 'acq_func' : 'LCB' , 'acq_func_kwargs' : { 'kappa' : 1.96 }}) Nevergrad Nevergrad is a gradient-free optimization platform. Thus, this package is suitable for optimizing over the hyperparamter space. As a great advantage, evolutionary algorithms are implemented here in addition to Bayesian techniques. Nevergrad usage and implementation details available here . 1 2 3 4 5 6 import nevergrad as ng # list of all available nevergrad optimizer print ( list ( ng . optimizers . registry . values ())) my_pipe = Hyperpipe ( \"...\" , optimizer = 'nevergrad' , optimizer_params = { 'facade' : 'NGO' , 'n_configurations' : 30 }) Smac SMAC (sequential model-based algorithm configuration) is a versatile tool for optimizing algorithm parameters. The main core consists of Bayesian Optimization in combination with an aggressive racing mechanism to efficiently decide which of two configurations performs better. SMAC usage and implementation details available here . 1 2 3 4 5 6 my_pipe = Hyperpipe ( \"...\" , optimizer = 'smac' , optimizer_params = { \"facade\" : \"SMAC4BO\" , \"wallclock_limit\" : 60.0 * 10 , # seconds \"ta_run_limit\" : 100 } # limit of configurations ) Switch Optimizer This optimizer is special, as it uses the strategies above to optimizes the same dataflow for different learning algorithms in a switch (\"OR\") element at the end of the pipeline. For example you can use bayesian optimization for each learning algorithm and select that each of the algorithms gets 25 configurations to be tested. This is different to a global optimization, in which, after an initial exploration phase, computational resources are dedicated to the best performing learning algorithm only. By equally distributing computational ressources to each learning algorithms, better comparability is achieved in-between the algorithms. This can according to the use case be desirable. 1 2 3 pipe = Hyperpipe ( \"...\" , optimizer = \"switch\" , optimizer_params = { 'name' : 'sk_opt' , 'n_configurations' : 25 })","title":"Hyperparameter Optimization"},{"location":"getting_started/output/","text":"PHOTONAI Output After executing the script a result folder is created. In there you find six files with different information about your pipeline and the results. photon_summary.txt A text file including a summary of the results. best_config_predictions.csv This file saves the test set predictions for the best configuration of each outer fold. photon_result_file.json You can visualize this file with our Explorer . Visualized information: Best Hyperparameter Configuration Performance Fold information Tested Configuration Optimization Progress photon_best_model.photon This file stores the best model. You can share or reload it later. photon_output.log Saves the console output from every fold, including the time, the current testing configurations and the results. hyperpipe_config.json Here is the initial setup for your analysis, so you can recreate it later.","title":"Inspect and visualize results"},{"location":"getting_started/photonai/","text":"Reasons to Use PHOTONAI It allows researchers to build, optimize and evaluate machine learning pipelines with few lines of code. It automates the training, optimization and test workflow according to user-defined parameters. It offers convenient access to established machine learning toolboxes such as sklearn. It facilitates machine learning applications for researchers with little programming experience. Users can select and change hyperparameter optimization strategies with simple keywords. It is built on clean interfaces and therefore fully customizable. It is easily extendable with custom algorithms, e.g. for handling biomedical data modalities. It acts as a unifying framework to help researchers share and reuse code across projects. It offers both simple and parallel pipeline streams for comparing algorithms, combining features and building ensembles. It extends existing pipeline implementations to enable the developer, e.g. to change the dataset (data augmentation within the training and testing cross validation splits at runtime. It enables rapid prototyping in contexts which require iterative evaluation of novel machine learning models. and many others... Class diagram Basic structure The PHOTONAI framework is built to accelerate and simplify the design of machine learning pipelines and automatize the training, testing and hyperparameter optimization process. The most important class is the Hyperpipe , as it is used to parametrize and control both the pipeline and the training and testing workflow. The Pipeline streams data through a sequence of PipelineElements , the latter of which represent either established or custom algorithm implementations ( BaseElement ). PipelineElements can share a position within the data stream via an And-Operation ( Stack ), an Or-Operation ( Switch ) or represent a parallel sub-pipeline ( Branch ).","title":"PHOTONAI Framework"},{"location":"getting_started/photonai/#reasons-to-use-photonai","text":"It allows researchers to build, optimize and evaluate machine learning pipelines with few lines of code. It automates the training, optimization and test workflow according to user-defined parameters. It offers convenient access to established machine learning toolboxes such as sklearn. It facilitates machine learning applications for researchers with little programming experience. Users can select and change hyperparameter optimization strategies with simple keywords. It is built on clean interfaces and therefore fully customizable. It is easily extendable with custom algorithms, e.g. for handling biomedical data modalities. It acts as a unifying framework to help researchers share and reuse code across projects. It offers both simple and parallel pipeline streams for comparing algorithms, combining features and building ensembles. It extends existing pipeline implementations to enable the developer, e.g. to change the dataset (data augmentation within the training and testing cross validation splits at runtime. It enables rapid prototyping in contexts which require iterative evaluation of novel machine learning models. and many others...","title":"Reasons to Use PHOTONAI"},{"location":"getting_started/photonai/#class-diagram","text":"","title":"Class diagram"},{"location":"getting_started/photonai/#basic-structure","text":"The PHOTONAI framework is built to accelerate and simplify the design of machine learning pipelines and automatize the training, testing and hyperparameter optimization process. The most important class is the Hyperpipe , as it is used to parametrize and control both the pipeline and the training and testing workflow. The Pipeline streams data through a sequence of PipelineElements , the latter of which represent either established or custom algorithm implementations ( BaseElement ). PipelineElements can share a position within the data stream via an And-Operation ( Stack ), an Or-Operation ( Switch ) or represent a parallel sub-pipeline ( Branch ).","title":"Basic structure"},{"location":"getting_started/regression/","text":"Regression In contrast to a classification task, a regression model is based on continuous target values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_boston from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement from photonai.optimization import IntegerRange , FloatRange my_pipe = Hyperpipe ( 'basic_regression_pipe' , optimizer = 'random_search' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'mean_squared_error' , 'mean_absolute_error' , 'explained_variance' ], best_config_metric = 'mean_squared_error' , outer_cv = KFold ( n_splits = 3 , shuffle = True ), inner_cv = KFold ( n_splits = 3 , shuffle = True ), verbosity = 1 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'SimpleImputer' ) my_pipe += PipelineElement ( 'StandardScaler' ) my_pipe += PipelineElement ( 'LassoFeatureSelection' , hyperparameters = { 'percentile' : [ 0.1 , 0.2 , 0.3 ], 'alpha' : FloatRange ( 0.5 , 5 )}) my_pipe += PipelineElement ( 'RandomForestRegressor' , hyperparameters = { 'n_estimators' : IntegerRange ( 10 , 50 )}) # load data and train X , y = load_boston ( return_X_y = True ) my_pipe . fit ( X , y )","title":"A first regression example"},{"location":"photon_elements/classifier_ensemble/","text":"An ensemble is a combination of multiple base estimators. For a short introduction to ensemble methods, see Sklearn Ensemble Methods . In PHOTONAI, an estimator ensemble can be easily created by adding any number of estimators to a Stack . Afterwards, simply add a meta estimator that receives the predictions of your stack. This can be any estimator or simply a averaging or voting strategy. In this example, we used the PhotonVotingClassifier to create a final prediction. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import StratifiedKFold from photonai.base import Hyperpipe , PipelineElement , Stack my_pipe = Hyperpipe ( name = 'ensemble_pipe' , optimizer = 'random_grid_search' , metrics = [ 'balanced_accuracy' ], best_config_metric = 'balanced_accuracy' , outer_cv = StratifiedKFold ( n_splits = 2 , shuffle = True , random_state = 42 ), inner_cv = StratifiedKFold ( n_splits = 2 , shuffle = True , random_state = 42 ), project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) # setup estimator stack est_stack = Stack ( name = 'classifier_stack' ) clf_list = [ 'RandomForestClassifier' , 'LinearSVC' , 'NuSVC' , \"SVC\" , \"MLPClassifier\" , \"KNeighborsClassifier\" , \"Lasso\" , \"PassiveAggressiveClassifier\" , \"LogisticRegression\" , \"Perceptron\" , \"RidgeClassifier\" , \"SGDClassifier\" , \"GaussianProcessClassifier\" , \"AdaBoostClassifier\" , \"BaggingClassifier\" , \"GradientBoostingClassifier\" ] for clf in clf_list : est_stack += PipelineElement ( clf ) my_pipe += est_stack my_pipe += PipelineElement ( 'PhotonVotingClassifier' ) X , y = load_breast_cancer ( return_X_y = True ) my_pipe . fit ( X , y )","title":"Classifier Ensemble"},{"location":"photon_elements/feature_subset_pipelines/","text":"In PHOTONAI, you can create individual data streams very easily. If, for example, you like to apply different preprocessing steps to distinct subsets of your features , you can create multiple branches within your ML pipeline that will hold any kind of preprocessing. Similarly, you could train different classifiers on different feature subsets. To add a branch to your pipeline, you can simply create a PHOTONAI Branch and then add any number of elements to it. If you only add transformer elements to your branch, the transformed data will be passed to the next element after your branch (or stacked in case of a PHOTONAI Stack). If, however, you add a final estimator to your branch, the prediction of this estimator will be passed to the next element. You could now add your created branch to a Hyperpipe, however, creating branches only really makes sense when having multiple ones and adding those to either a Stack or Switch . Otherwise, why create a branch in the first place? Importantly, a branch will always receive all of your features if you don't add a PHOTONAI DataFilter . A DataFilter can be added as first element of a branch to make sure only a specific subset of the features will be passed to the remaining elements of the branch. It only takes a parameter called indices that specifies the data columns that are ultimately passed to the next element. In this example, we create three branches to process three feature subsets of the breast cancer dataset separately. For all three branches, we add an SVC to predict the classification label. This way, PHOTONAI can find the optimal SVC hyperparameter for the three data modalities. All predictions are then stacked and passed to a final Switch that will decide between a Random Forest or another SVC. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack , Branch , Switch , DataFilter from photonai.optimization import FloatRange , IntegerRange # LOAD DATA FROM SKLEARN X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'data_integration' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 20 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'f1_score' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 0 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'SimpleImputer' ) my_pipe += PipelineElement ( 'StandardScaler' , {}, with_mean = True ) # Use only \"mean\" features: [mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness, mean_compactness, # mean_concavity, mean_concave_points, mean_symmetry, mean_fractal_dimension mean_branch = Branch ( 'MeanFeature' ) mean_branch += DataFilter ( indices = range ( 0 , 10 )) mean_branch += PipelineElement ( 'SVC' , { 'C' : FloatRange ( 0.1 , 200 )}, kernel = 'linear' ) # Use only \"error\" features error_branch = Branch ( 'ErrorFeature' ) error_branch += DataFilter ( indices = range ( 10 , 20 )) error_branch += PipelineElement ( 'SVC' , { 'C' : FloatRange ( 0.1 , 200 )}, kernel = 'linear' ) # use only \"worst\" features: [worst_radius, worst_texture, ..., worst_fractal_dimension] worst_branch = Branch ( 'WorstFeature' ) worst_branch += DataFilter ( indices = range ( 20 , 30 )) worst_branch += PipelineElement ( 'SVC' , { 'C' : FloatRange ( 0.1 , 200 )}, kernel = 'linear' ) my_pipe += Stack ( 'SourceStack' , [ mean_branch , error_branch , worst_branch ]) my_pipe += Switch ( 'EstimatorSwitch' , [ PipelineElement ( 'RandomForestClassifier' , { 'n_estimators' : IntegerRange ( 2 , 5 )}), PipelineElement ( 'SVC' )]) my_pipe . fit ( X , y )","title":"Feature Subset Pipelines"},{"location":"photon_elements/stack/","text":"Stack You want to do stacking if more than one algorithm shall be applied, which equals to an AND-Operation. The PHOTONAI Stack delivers the data to all of the entailed PipelineElements and the transformations or predictions are afterwards horizontally concatenated. In this way you can preprocess data in different ways and collect the resulting information to create a new feature matrix. Additionally, you can train several learning algorithms with the same data in an ensemble-like fashion and concatenate their predictions to a prediction matrix on which you can apply further processing like voting strategies. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack from photonai.optimization import FloatRange , IntegerRange X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'basic_stack_pipe' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 0 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) svc = PipelineElement ( 'LinearSVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 )}) # for a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators # in case only some implement predict_proba, predict is called for the remaining estimators my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True ) my_pipe += PipelineElement ( 'LinearSVC' ) my_pipe . fit ( X , y )","title":"Stack (AND)"},{"location":"photon_elements/stack/#stack","text":"You want to do stacking if more than one algorithm shall be applied, which equals to an AND-Operation. The PHOTONAI Stack delivers the data to all of the entailed PipelineElements and the transformations or predictions are afterwards horizontally concatenated. In this way you can preprocess data in different ways and collect the resulting information to create a new feature matrix. Additionally, you can train several learning algorithms with the same data in an ensemble-like fashion and concatenate their predictions to a prediction matrix on which you can apply further processing like voting strategies. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack from photonai.optimization import FloatRange , IntegerRange X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'basic_stack_pipe' , optimizer = 'sk_opt' , optimizer_params = { 'n_configurations' : 25 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 3 ), verbosity = 0 , project_folder = './tmp/' ) my_pipe += PipelineElement ( 'StandardScaler' ) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) svc = PipelineElement ( 'LinearSVC' , hyperparameters = { 'C' : FloatRange ( 0.5 , 25 )}) # for a stack that includes estimators you can choose whether predict or predict_proba is called for all estimators # in case only some implement predict_proba, predict is called for the remaining estimators my_pipe += Stack ( 'final_stack' , [ tree , svc ], use_probabilities = True ) my_pipe += PipelineElement ( 'LinearSVC' ) my_pipe . fit ( X , y )","title":"Stack"},{"location":"photon_elements/subpipelines/","text":"Subpipelines If the user wants to parallelize a complete sequence of transformations, that is not only singular PipelineElements but an ordered number of PipelineElements, the class PHOTONAI Branch offers a way to create parallel subpipelines. The branch in turn, can be used in combination with the AND- and OR- Elements in order to design complex pipeline architectures. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Stack , Branch from photonai.optimization import IntegerRange , Categorical , FloatRange X , y = load_breast_cancer ( return_X_y = True ) my_pipe = Hyperpipe ( 'basic_stacking' , optimizer = 'grid_search' , metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'f1_score' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 10 ), verbosity = 1 , project_folder = './tmp/' ) # BRANCH WITH QUANTILTRANSFORMER AND DECISIONTREECLASSIFIER tree_qua_branch = Branch ( 'tree_branch' ) tree_qua_branch += PipelineElement ( 'QuantileTransformer' , n_quantiles = 100 ) tree_qua_branch += PipelineElement ( 'DecisionTreeClassifier' , { 'min_samples_split' : IntegerRange ( 2 , 4 )}, criterion = 'gini' ) # BRANCH WITH MinMaxScaler AND DecisionTreeClassifier svm_mima_branch = Branch ( 'svm_branch' ) svm_mima_branch += PipelineElement ( 'MinMaxScaler' ) svm_mima_branch += PipelineElement ( 'SVC' , { 'kernel' : Categorical ([ 'rbf' , 'linear' ]), 'C' : FloatRange ( 0.01 , 2.0 , num = 10 )}, gamma = 'auto' ) # BRANCH WITH StandardScaler AND KNeighborsClassifier knn_sta_branch = Branch ( 'neighbour_branch' ) knn_sta_branch += PipelineElement ( 'StandardScaler' ) knn_sta_branch += PipelineElement ( 'KNeighborsClassifier' ) # voting = True to mean the result of every branch my_pipe += Stack ( 'final_stack' , [ tree_qua_branch , svm_mima_branch , knn_sta_branch ]) my_pipe += PipelineElement ( 'LogisticRegression' , solver = 'lbfgs' ) my_pipe . fit ( X , y )","title":"Subpipelines"},{"location":"photon_elements/switch/","text":"Switch The PipelineSwitch element acts like an OR-Operator and decides which element performs best. Currently, you can only optimize the PipelineSwitch using Grid Search , Random Grid Search and Smac3 . In this example, we add two different transformer elements and two different estimators, and PHOTONAI will evaluate the best choices including the respective hyperparameters. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import IntegerRange # GET DATA X , y = load_breast_cancer ( return_X_y = True ) # CREATE HYPERPIPE my_pipe = Hyperpipe ( 'basic_switch_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 5 ), verbosity = 1 , project_folder = './tmp/' ) # Transformer Switch my_pipe += Switch ( 'StandardizationSwitch' , [ PipelineElement ( 'StandardScaler' ), PipelineElement ( 'MinMaxScaler' )]) # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ]) my_pipe . fit ( X , y )","title":"Switch (OR)"},{"location":"photon_elements/switch/#switch","text":"The PipelineSwitch element acts like an OR-Operator and decides which element performs best. Currently, you can only optimize the PipelineSwitch using Grid Search , Random Grid Search and Smac3 . In this example, we add two different transformer elements and two different estimators, and PHOTONAI will evaluate the best choices including the respective hyperparameters. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from sklearn.datasets import load_breast_cancer from sklearn.model_selection import KFold from photonai.base import Hyperpipe , PipelineElement , Switch from photonai.optimization import IntegerRange # GET DATA X , y = load_breast_cancer ( return_X_y = True ) # CREATE HYPERPIPE my_pipe = Hyperpipe ( 'basic_switch_pipe' , optimizer = 'random_grid_search' , optimizer_params = { 'n_configurations' : 15 }, metrics = [ 'accuracy' , 'precision' , 'recall' ], best_config_metric = 'accuracy' , outer_cv = KFold ( n_splits = 3 ), inner_cv = KFold ( n_splits = 5 ), verbosity = 1 , project_folder = './tmp/' ) # Transformer Switch my_pipe += Switch ( 'StandardizationSwitch' , [ PipelineElement ( 'StandardScaler' ), PipelineElement ( 'MinMaxScaler' )]) # Estimator Switch svm = PipelineElement ( 'SVC' , hyperparameters = { 'kernel' : [ 'rbf' , 'linear' ]}) tree = PipelineElement ( 'DecisionTreeClassifier' , hyperparameters = { 'min_samples_split' : IntegerRange ( 2 , 5 ), 'min_samples_leaf' : IntegerRange ( 1 , 5 ), 'criterion' : [ 'gini' , 'entropy' ]}) my_pipe += Switch ( 'EstimatorSwitch' , [ svm , tree ]) my_pipe . fit ( X , y )","title":"Switch"}]}